[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISG 5312 - Genomic Data Analysis in Practice II",
    "section": "",
    "text": "Preface\nThis course is the second in a two part series on practical analysis of genomic data. The first course heavily emphasized Linux and HPC skills, the statistical computing language R, and the basics of high throughput sequencing data. It used a model workflow, bulk RNA-seq analysis, as a framework for developing basic competencies.\nThis course will broaden the focus to include higher level bioinformatics skills, such as using Git for version controlling code, an introduction to workflow languages through Nextflow, and introduces several more workflows (variant detection, genome assembly, and single-cell RNA-seq). The course comprises five modules, one for each of these topics.\nIn this semester students will also do independent projects where they pick a workflow and reanalyze a relevant public dataset.\nThere are five modules:\n\nIntroduction to version control with Git/GitHub.\nVariant Detection.\nGenome Assembly.\nscRNAseq\nPipeline development with Nextflow",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html",
    "href": "01_1_git-ingStarted.html",
    "title": "1¬† First steps with Git",
    "section": "",
    "text": "1.1 Learning Objectives\nIn the first chapter of this module, you will learn the very basics of Git and learn how to set up a remote repository on GitHub.\nThere are GUI tools for using Git, but we‚Äôre going to focus on the command-line here (except when we get to GitHub).\nNote that explanation will be sparse here, as you will be expected to do some assigned reading and watch some videos.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#learning-objectives",
    "href": "01_1_git-ingStarted.html#learning-objectives",
    "title": "1¬† First steps with Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nCreate a code repository with git.\n\n\nSet up a documented remote copy of the repository on GitHub.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#installing-git",
    "href": "01_1_git-ingStarted.html#installing-git",
    "title": "1¬† First steps with Git",
    "section": "1.2 Installing Git",
    "text": "1.2 Installing Git\nYou‚Äôre going to want git to be installed on your local machine. There are instructions here\nIf you‚Äôre using a Mac, you‚Äôve probably already got it. If not, you can install Xcode developer tools, which will include it.\nIf you‚Äôre using Windows (and thus probably also WSL2) you may want it installed for both systems. See the Linux and Windows instructions here.\nNote that on Xanadu, the base installation of Git is waaay out of date. In most cases things will still work fine. Do git --version to see what version is currently running (üíÄ). You can module load git/2.30.0, however, if you run into problems.\nIf you did the chapter on customizing your shell in ISG5311, you can add module load git/2.30.0 to your .bashrc file in your home directory so that the module will be loaded every time you log in.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#a-brief-pause",
    "href": "01_1_git-ingStarted.html#a-brief-pause",
    "title": "1¬† First steps with Git",
    "section": "1.3 A brief pause‚Ä¶",
    "text": "1.3 A brief pause‚Ä¶\nBefore going further here, please do the readings and watch the videos in HuskyCT.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#configuring-git",
    "href": "01_1_git-ingStarted.html#configuring-git",
    "title": "1¬† First steps with Git",
    "section": "1.4 Configuring Git",
    "text": "1.4 Configuring Git\nBefore we do much with Git, we want to make sure that your instances are configured correctly. Anywhere you‚Äôve got it installed, you want to set it up to say who you are. Run these commands everywhere you might use it (Mac, Windows, WSL, and don‚Äôt forget Xanadu!), but obviously, please edit them first.\ngit config --global user.name \"Bob Loblaw\"\ngit config --global user.mail \"Bob.Loblaw@BobLoblawsLawBlog.com\"\nThe config subcommand edits git‚Äôs configuration file. --global applies settings to all the current user‚Äôs repositories on this computer (you can change these settings for individual repositories). You can see your configuration with\ngit config -l\n\n1.4.1 Git‚Äôs text editor\nGit is meant to hide in the background. You don‚Äôt use it to write or edit code, but to track it as it develops. Nevertheless, there are a times when Git will demand that you explain something, or reconcile conflicting edits. When that happens it will drop you into a text editor.\n\n1.4.1.1 vim\nBy default that text editor is vi, a powerful and widely used command line editor that comes with, shall we say, a steep learning curve.\nYou can change this. First let‚Äôs quickly cover the absolute barest of details for vi (more likely an updated version vim), as you will sooner or later run into it.\nLet‚Äôs create a dummy file with some text.\necho {a..z} | sed 's/ /\\n/g' &gt;letters.txt\nWe can open it with vi\nvi letters.txt\nAfter opening, you are in Normal mode. You can navigate around, but not edit.\nIf you press i you will go into Insert mode. Now you can edit. Press escape to go back to normal mode. Beware there are several other modes.\nQuitting vi is famously annoying to people who don‚Äôt know how to use it (or who have learned and forgotten many times).\nFirst, go to normal mode by pressing escape. After this:\n\nIf you have made no edits and wish to quit, you can type : then q then enter (:q then enter).\nIf you have made edits you wish to discard, you can type :q! then enter.\nIf you have made edits you wish to save you can type :w then enter THEN :q OR just :wq then enter.\n\n\nIf you want the bragging rights (and efficiency) that come with being proficient at using a powerful command-line editor, there are lots of ways to learn. One is through this game, vim adventures.\n\n\n1.4.1.2 Changing the text editor (if you want)\nIf you thought, ‚Äúvim is fine‚Äù, then fantastic, you can keep using it. If you thought, ‚Äúyuck‚Äù, and want to change the editor, you can use git config.\nIf you wanted to use nano for instance, you could do:\ngit config --global core.editor nano\nYou can also use VS Code (only on your local machine). First, you have to enable launching VS Code from the command line (see here)\nThen run\ngit config --global core.editor \"code --wait\"\nThe flag --wait will cause the terminal to wait until VS Code is closed before moving on.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#git-basics",
    "href": "01_1_git-ingStarted.html#git-basics",
    "title": "1¬† First steps with Git",
    "section": "1.5 Git Basics",
    "text": "1.5 Git Basics\nOk, hopefully git is all set up now. We can run through the basics. There are two ways you might get started with a Git repository.\n\nYou create a new one in a brand new directory as you would with a new project.\nYou can clone a copy of an existing repository to alter or contribute to it. Perhaps one from GitHub (or GitLab, or BitBucket).\n\n\n1.5.1 Starting a new repository:\nTo start a git repository:\nmkdir newproject\ncd newproject\ngit init\nWhich writes out:\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /home/FCAM/blah/blah/blah/newproject/.git/\nYou can see (with ls -a) that a hidden directory newproject/.git/ has been created. This .git directory is where Git will store the whole version control history. You should never muck around in .git. If for some reason you wanted to destroy the whole version history, you could delete .git and do git init to start anew.\n\n\n1.5.2 Cloning a repository\nWe saw this last semester with our RNAseq example, but you can clone a git repository like this:\ngit clone https://github.com/isg-certificate/rnaseq.git\ncd rnaseq\nWith stderr output:\nCloning into 'rnaseq'...\nremote: Enumerating objects: 85, done.\nremote: Counting objects: 100% (85/85), done.\nremote: Compressing objects: 100% (62/62), done.\nremote: Total 85 (delta 35), reused 66 (delta 19), pack-reused 0 (from 0)\nReceiving objects: 100% (85/85), 20.91 KiB | 509.00 KiB/s, done.\nResolving deltas: 100% (35/35), done.\nIf we do ls -la:\ndrwxr-xr-x 5 nreid cbc 2560 Jan  6 17:05 .\ndrwxr-xr-x 4 nreid cbc 1024 Jan  6 17:05 ..\ndrwxr-xr-x 8 nreid cbc 5632 Jan  6 17:05 .git\n-rw-r--r-- 1 nreid cbc   68 Jan  6 17:05 .gitignore\ndrwxr-xr-x 2 nreid cbc  512 Jan  6 17:05 metadata\n-rw-r--r-- 1 nreid cbc    8 Jan  6 17:05 README.md\ndrwxr-xr-x 7 nreid cbc 2560 Jan  6 17:05 scripts\nYou can see we grab all the contents of the repository, but also we have a .git directory.\n\n\n1.5.3 Adding files to the repository\nAdding a file (or a change to a file) to the repository happens in three steps.\n\nYou create (or alter) a file.\nYou add the file to the staging area.\nYou commit the file.\n\nLet‚Äôs consider our fresh, clean repository newproject. Let‚Äôs assume we‚Äôre going to start a new bioinformatics project. cd into newproject and type git status. You should see this:\n# On branch master\n#\n# Initial commit\n#\nnothing to commit (create/copy files and use \"git add\" to track)\ngit status gives us a summary of any changes that have been made to the repository since it was last ‚Äúcommitted‚Äù (more in a moment).\nLet‚Äôs create a script:\n# this syntax allows you to print a multi-line file and redirect it. \ncat &lt;&lt;EOF &gt;hw.sh\n#!/bin/bash\n\necho \"Hello World!\"\nEOF\n\nbash hw.sh\nNow git status again:\n# On branch master\n#\n# Initial commit\n#\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hw.sh\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit sees that we have a new file that is not being tracked as part of the repository. We can have lots of files that are not tracked as part of the repository. In fact, when doing bioinformatics, we probably want to keep all our results (and maybe even the data) in this working directory, but we do not want to keep them as part of the\nWe can add this file to the staging area with git add hw.sh. We could also add everything in the repository with git add --all.\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#\n#   new file:   hw.sh\n#\nNow git is tracking the file and its current contents have been staged, but it has NOT been committed to the repository. Remember, there are three conceptual spaces in Git, the working directory (where we actually work) the staging area (where we add changes temporarily) and the repository (where we more or less permanently record updates to the project).\nNow that we‚Äôve got our file in the staging area, we can commit it to the repository.\ngit commit -m \"first commit\"\nThat produces the message:\n[master (root-commit) fe2dafd] first commit added hw.sh\n 1 file changed, 3 insertions(+)\n create mode 100644 hw.sh \nNow we‚Äôve committed the first file to the repository.\nThere is lots more to learn, including branching, merging, inspecting the history‚Ä¶ we‚Äôll get to that in the next chapter. For the last part of this chapter we‚Äôll introduce GitHub,",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#remote-repositories",
    "href": "01_1_git-ingStarted.html#remote-repositories",
    "title": "1¬† First steps with Git",
    "section": "1.6 Remote repositories",
    "text": "1.6 Remote repositories\nGit is a distributed version control system, so it naturally allows us (and collaborators) to maintain and interact with remote copies of our repositories.\nAssuming you cloned rnaseq above, if you enter the working directory and type\ngit remote -v\nYou will see:\norigin  https://github.com/isg-certificate/rnaseq.git (fetch)\norigin  https://github.com/isg-certificate/rnaseq.git (push)\nThis indicates the cloned repository is using a repository at the URL https://github.com/isg-certificate/rnaseq.git as a remote. rnaseq.git is a Git repository in the same sense that the .git directory in newproject is a Git repository. The remote is named origin. This is arbitrary and the default choice in git. You can name remotes anything you want. (fetch) and (push) mean that pulling down changes from the remote and pushing them up uses the same URL. The repository is hosted at GitHub (more in the next section), which controls access, so you won‚Äôt have permission to push any changes you might make.\nIf you cd to our brand new directory newproject and do git remote -v You should see no output.\n\n1.6.1 Adding a remote repository\nMost people host remote repositories, well, remotely, but to make things clear (hopefully) we‚Äôll create a remote repository for newproject locally.\ncd into the directory containing newproject (not newproject itself) and do\ngit init --bare remotenewproject.git\nIf you do ls you should now see (at least) newproject  remotenewproject.git.\nremotenewproject.git is a git repository, like .git in newproject. There is no working directory (or working tree, as git sometimes calls it). You cannot directly edit the files in this remote (without changing some settings first). This is a result of the option --bare. You want this because you want the remote to serve as a common access point or a backup of your code.\nNow cd back into newproject. You can add remotenewproject.git like this:\ngit remote add origin ../remotenewproject.git\nThe git remote add is adding a remote (arbitrarily) named origin at the location ../remotenewproject.git.\nIf you do git remote -v you‚Äôll see\norigin  ../remotenewproject.git (fetch)\norigin  ../remotenewproject.git (push)\nThe remote repository currently does not have any of our local commits in it. We will cover branches in the next chapter, but to push our local commits to the remote, we need to set an ‚Äúupstream‚Äù branch in the remote repository to push to.\nWe can see that our current branch does not have an upstream remote branch to push to by doing:\ngit branch -vv\n* master bf15b43 initial commit\nWe can set the upstream branch and push in one command:\ngit push -u origin master\nCounting objects: 3, done.\nWriting objects: 100% (3/3), 226 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo ../remotenewproject.git\n * [new branch]      master -&gt; master\nBranch master set up to track remote branch master from origin.\n-u origin says we‚Äôre setting up an upstream branch in origin master is the (arbitrary) name of the local and remote branch we‚Äôre pushing.\nDo git branch -vv to see we have now set up a remote master branch at origin:\n* master bf15b43 [origin/master] initial commit\nIn the future we can just do git push to push changes to the remote.\n\n\n1.6.2 Pushing and pulling from remotes.\nThere are many cases, such as collaboration, or a single person using multiple workstations, where you will have a single central remote repository and clones in multiple locations. You may update the code in one place, push it to the remote, and then need to pull it down in another place. This can get complicated (more in the next chapter) but we‚Äôll do a simple version of that now.\nFirst, clone a new copy of the remote:\ngit clone remotenewproject.git newproject_copy\nCloning into 'newproject_copy'...\ndone.\nNow ls and see: newproject  newproject_copy  remotenewproject.git.\nWe have the original directory we created, the remote repository, and our new clone. cd into newproject_copy and create a new file:\ncat &lt;&lt;EOF &gt;hm.sh\n#!/bin/bash\n\necho \"Hola Mundo!\"\nEOF\n\nbash hm.sh\nNow: git status\n# On branch master\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hm.sh\nnothing added to commit but untracked files present (use \"git add\" to track)\nDo git add hm.sh and then git commit -m \"added hola mundo script\"\nNow git status:\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#   (use \"git push\" to publish your local commits)\n#\nnothing to commit, working directory clean\nOur branch is 1 commit ahead of the remote.\nNow git push to send the local repository changes to the remote:\nCounting objects: 4, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 312 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo /path/to/remotenewproject.git\n   bf15b43..7f84097  master -&gt; master\nNow go back to our original newproject directory and git status. You‚Äôll see it does not know that the remote repository has been updated. To pull down any changes do git pull\nremote: Counting objects: 4, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\nFrom ../remotenewproject\n   bf15b43..7f84097  master     -&gt; origin/master\nUpdating bf15b43..7f84097\nFast-forward\n hm.sh | 3 +++\n 1 file changed, 3 insertions(+)\n create mode 100644 hm.sh\nls should show we now have our new file.\nTo see our commit history try git log\ncommit 7f84097d7c3528e43023c66b5ddfc7d302567002\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Wed Dec 31 23:59:59 1999 -0500\n\n    added hola mundo script\n\ncommit bf15b4396cb1c7379c03bbca9222ef46f2384359\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Wed Dec 31 11:59:59 1999 -0500\n\n    initial commit\nThis process can get complicated, and conflicts can arise. We‚Äôll deal with that in the next chapter.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#github",
    "href": "01_1_git-ingStarted.html#github",
    "title": "1¬† First steps with Git",
    "section": "1.7 GitHub",
    "text": "1.7 GitHub\nGitHub is a web service (among others, such as GitLab and Bitbucket) that can host Git repositories. GitHub is a natural place to store a remote repository. It allows you to control who can access or contribute to the repository, and is a great way to share (and find) open code.\nWe are going to use GitHub in this course in several exercises and the final project. You may wish to keep your final project from this semester in a public repository as a sort of portfolio project.\n\n\n\n\n\n\nGitHub Copilot and privacy\n\n\n\n\n\nBe aware that GitHub has an AI assistant, ‚ÄúCopilot‚Äù which is trained at least in part on code stored in GitHub. It is unclear which code is used for training, but most likely individual account repositories are used (both public and private).\n\n\n\nBefore moving forward, please create an account.\nAfter you‚Äôve created an account, you will need to set up SSH key authentication so that Git can access your remote repositories hosted at GitHub. GitHub no longer allows password authentication with command-line Git. You may have some experience with SSH keys already if you set them up for access to Xanadu. GitHub‚Äôs documentation for this is here.\nYou should set up ssh key access from both your local machine and Xanadu. If you have trouble with this, please reach out for help.\n\n1.7.1 Setting up GitHub as a remote repository\nThe easiest way to approach setting up GitHub as a remote with a new project is to start a new empty repository on GitHub (from your home page) and then clone it locally.\nIf you‚Äôve already got a repository started, you can more or less follow the same approach we did above. First go to your GitHub profile and click on the Repositories tab (e.g.¬†user isg-certificate would go to the page https://github.com/isg-certificate?tab=repositories) and click the green New button.\nEnter the name newproject, select ‚Äúpublic‚Äù or ‚Äúprivate‚Äù as you prefer and then click Create repository.\nGitHub will then tell you how to push an existing repository (although it will use main as the default main branch name, whereas in this material we have been using master).\nBefore you do this, however, let‚Äôs get rid of our extra clone and local remote:\n# force remove git repos\nrm -rf newproject_copy\nrm -rf newproject.git\n\n# remove current remote\ncd newproject\ngit remote remove origin\nNow we can add a new remote (again named origin). cd into newproject and:\ngit remote add origin git@github.com:isg-certificate/newproject.git\ngit branch -M master\ngit push -u origin master\nIf you go to the repository page on GitHub you should see your files hm.sh and hw.sh have been pushed there.\n\n\n1.7.2 README.md\nIt‚Äôs always advisable to include documentation. GitHub encourages this through the inclusion of a (GitHub flavored) markdown formatted document titled README.md. GitHub will format this document into HTML and include its contents on the splash page for the GitHub hosted repository.\nLet‚Äôs create a file locally in newproject now and push it to GitHub:\necho \"This is a dummy project to demo Git/GitHub\" &gt;README.md\ngit add README.md\ngit commit -m \"added README.md\"\ngit push\nNow visit the GitHub repository page and you should see the contents of README.md.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#summing-up",
    "href": "01_1_git-ingStarted.html#summing-up",
    "title": "1¬† First steps with Git",
    "section": "1.8 Summing up",
    "text": "1.8 Summing up\nGit has many commands (type git to see them all). We have used the following:\n\ngit init to initialize a new repository.\ngit add to add a file to the staging area.\ngit commit to commit a staged snapshot of the working directory to the repository.\ngit status to summarize the current status of new, alterered, or staged files.\ngit remote to list and modify remote repositories.\ngit push to push changes to a remote repository.\ngit pull to pull changes from a remote repository. ## A few tips for thinking about Git/GitHub\nIn Bioinformatics, Git is used for code, documentation and sometimes figures or written reports. Don‚Äôt track analysis results unless they are very small and you think its important to version control them. fastqs, bams, etc should not be committed. Git is not meant for dealing with large data.\nGenerally think about Git as forward looking. If you want to delete a file, don‚Äôt think about deleting it from the history of the repository, think about deleting now and leaving it out in the future. If you want to restore a file (that was deleted or altered in a regrettable way) think about it as grabbing that file from a past commit and bringing it forward to replace your unwanted changes rather than ‚Äúundoing‚Äù them. We‚Äôll cover this in the next chapter.\nThere is a ton of stuff you can do with GitHub beyond hosting remote repositories (including hosting static web pages like this one!). We won‚Äôt get too far into it in this course.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html",
    "href": "01_2_moreGit.html",
    "title": "2¬† More Git",
    "section": "",
    "text": "2.1 Learning Objectives\nIn the second chapter, we‚Äôre going to cover a few more topics.\nAgain, this will all be pretty short on explanation, make sure to refer to readings and videos in HuskyCT to understand better how things work.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#learning-objectives",
    "href": "01_2_moreGit.html#learning-objectives",
    "title": "2¬† More Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nBranch the repository to edit, and then merge changes.\n\n\nWork collaboratively.\n\n\n\n\n\nRestoring past versions.\nTelling git to ignore files.\nBranching and merging.\nManaging remote branches.\nForking and pull requests on Github.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#restoring-past-versions",
    "href": "01_2_moreGit.html#restoring-past-versions",
    "title": "2¬† More Git",
    "section": "2.2 Restoring past versions",
    "text": "2.2 Restoring past versions\nConsider our newproject directory. It contains two files (and the hidden .git directory):\ntree\n.\n‚îú‚îÄ‚îÄ hm.sh\n‚îî‚îÄ‚îÄ hw.sh\n\n0 directories, 2 files\nThey should both be committed at this point.\nWhat happens if we make some modifications and we want to get rid of them? Like say, we overwrite the entire file.\necho 'oops!' &gt;hw.sh\ngit status will show us the file has been modified:\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   hw.sh\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n2.2.1 When the changes are unstaged\nSo we‚Äôve made an edit, but haven‚Äôt staged it. We can grab the most recent committed version with:\ngit restore hw.sh\nNow cat hw.sh\n#!/bin/bash\n\necho \"Hello World!\"\nReveals the file has been restored.\n\n\n2.2.2 When the changes are staged\nIf we have already staged the file however, this won‚Äôt work directly, as it will simply check the file out from the staging area, not the most recent commit.\nIf you make the same mistake again, echo 'oops!' &gt;hw.sh, but this time stage the change git add hw.sh, you will see with git status:\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   hw.sh\nPer the message, you can unstage with\ngit restore --staged hw.sh\nAnd then go back to the last commit like before with git restore hw.sh.\n\n\n2.2.3 When the changes have been committed\nIf we make an edit and commit it:\necho 'echo \"Good day, world!\"' &gt;&gt;hw.sh\ngit add hw.sh\ngit commit -m \"good day\"\nWe‚Äôve now added a change to our repository. If we want to go back to a previous version (i.e without the ‚Äúgood day world‚Äù echo), we first need to identify the commit containing the file version we want. We can run git log to see a list of commits.\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84 (HEAD -&gt; master)\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 1999 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 1998 -0500\n\n    added hola mundo script\n\ncommit 39fb28833d64c1176fc4fa8dc010d0dbdf8ff374\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 10:43:07 1997 -0500\n\n    first commit\nCommits are uniquely identified by those long strings of letters and numbers. They are hashes of the snapshot (you can read more elsewhere!).\nTo get a version of the file from one of these commits:\ngit restore --source=ed8b357e hw.sh\nWe only need to provide enough of the hash that it is unique among commits. Now git status\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   hw.sh\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nWe have restored our file, but not staged or committed it. We need to git add and then git commit. Now git log:\ncommit 3140e0d685f677c4ac2cc2123a8b53ef68eeee4a (HEAD -&gt; master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:40:11 2025 -0500\n\n    restoring hw.sh\n\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 2025 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 2025 -0500\n\n    added hola mundo script\n\ncommit 39fb28833d64c1176fc4fa8dc010d0dbdf8ff374\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:43:07 2025 -0500\n\n    first commit\nWe can see we have added a new commit. We didn‚Äôt go backwards in our history so much as reached back to a previous file version and pulled it forward into a new change.\n\n\n2.2.4 Examining differences between file versions\nWhat if we‚Äôre not sure which commit to restore from? We can use git diff to examine differences between files:\nWe can compare file versions with git diff.\ngit diff f8b8dcc hw.sh\nThis tells git to compare the file hw.sh between the current working space and the commit hash beginning with f8b8dcc. The output:\ndiff --git a/hw.sh b/hw.sh\nindex 22b4300..cd16289 100644\n--- a/hw.sh\n+++ b/hw.sh\n@@ -1,4 +1,3 @@\n #!/bin/bash\n \n echo \"Hello World!\"\n-echo \"Good day, world!\"\nThere‚Äôs a lot here (nicely colored in the terminal), but you can see, essentially, that the line echo \"Good day, world!\" has been removed (signified by -) from our current version, relative to the commit we‚Äôre examining.\nIf you‚Äôre working on a local machine, you can use something else to view the diff results. For Visual Studio Code, you can configure the difftool command like this:\ngit config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\nAs mentioned in the previous chapter, code will have to be in your PATH (linked instructions are there).\nNow\ngit difftool ed8b357e46329f4 hw.sh\nAnd after a prompt you should get a nicely formatted window with side-by-side files that makes it easy to see differences.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#branching-and-merging",
    "href": "01_2_moreGit.html#branching-and-merging",
    "title": "2¬† More Git",
    "section": "2.3 Branching and merging",
    "text": "2.3 Branching and merging\nThis is one of the pillars of git and can get a little confusing, especially when remotes are brought into the equation.\nWhen working with an established code repository, you may have an idea you want to test out, but you are unsure of whether it will be any good, or it may take some time to implement and you don‚Äôt want to unsettle the existing code.\n\n2.3.1 Branching\nIn these cases you can create a new branch (again, see materials in huskyCT for more conceptual details). A branch is an independent line of commits. Repositories can have dozens of branches. Check the GitHub repository for the NF-Core pipeline rnaseq. Click the button with the branching icon that says master.\nAfter you have worked out your idea, if you decide it should become a part of the repository, you can merge it back into the main branch.\nWith our newproject repository, we can see existing branches like this:\ngit branch -vv\n* master 3140e0d [origin/master: ahead 2] restoring hw.sh\nThere‚Äôs only one branch currently. The * means this branch is currently what we see in the working directory. master is the name of the branch (the arbitrary default). 3140e0d is the beginning of the commit hash. [origin/master: ahead 2] indicates this branch is tracking a branch master at our remote origin and that it is two commits ahead of origin/master as of the last time we communicated with the remote. restoring hw.sh is the commit message from the last commit.\nLet‚Äôs make a new branch.\ngit branch goodbye\nNow git branch -vv\n  goodbye 3140e0d restoring hw.sh\n* master  3140e0d [origin/master: ahead 2] restoring hw.sh\nWe have a new branch goodbye. We didn‚Äôt switch to that branch, however (* is still on master), and importantly this branch is not on our remote (or any remote).\nTo switch to the branch:\ngit checkout goodbye\nNow git branch -vv\n* goodbye 3140e0d restoring hw.sh\n  master  3140e0d [origin/master: ahead 2] restoring hw.sh\nLet‚Äôs add some new files:\ncat &lt;&lt;EOF &gt;gw.sh\n#!/bin/bash\n\necho \"Goodbye World!\"\nEOF\n\ncat &lt;&lt;EOF &gt;am.sh\n#!/bin/bash\n\necho \"Adios Mundo!\"\nEOF\nAdd git add --all and commit git commit -m \"goodbye\"\nNow we‚Äôve added two brand new files that should only be present on branch goodbye.\nSee ls\nam.sh  gw.sh  hm.sh  hw.sh\nNow switch branches git checkout master and ls\nhm.sh  hw.sh\nNow back to goodbye with git checkout goodbye.\nLet‚Äôs look at our history another way git log --decorate. The --decorate option will write the names of any references. References very simply point at commits. While conceptually a branch is much like the branch of a tree, when you create a branch, git creates one of these pointers with the name of your branch and attaches it to a commit. The last few commits with references added:\ngit log --decorate\ncommit 45f76a809cc3b03c53d7fe1e91ab23944438f5a2 (HEAD, goodbye)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 14:44:40 2001 -0500\n\n    goodbye\n\ncommit 3140e0d685f677c4ac2cc2123a8b53ef68eeee4a (master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:40:11 2000 -0500\n\n    restoring hw.sh\n\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 1999 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 1998 -0500\n\n    added hola mundo script\nWe have references HEAD, goodbye, master and origin/master.\nHEAD is a special pointer telling us which commit is currently checked out in our working directory (uncommitted changes notwithstanding). goodbye is the reference for our current branch. origin/master is a remote tracking branch (more shortly) telling us which commit the remote was one when we last checked in with it.\nWe can put HEAD on a commit without a branch, but this isn‚Äôt a great state of affairs:\ngit checkout f8b8dcc6e75b325d4976f27d10c74f7154452d84\nNote: checking out 'f8b8dcc6e75b325d4976f27d10c74f7154452d84'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at f8b8dcc... good day\ndetached HEAD state means the HEAD pointer is on a commit but has no branch. As the message says, you can create a new branch from this point, or just git checkout goodbye to get back to your original commit.\nSee git log --decorate to validate that HEAD is all by its lonesome.\nmaster is currently directly upstream of goodbye, but branches can diverge down independent paths. Lets make a couple changes to master. First git checkout master.\necho 'echo Hiya World!' &gt;&gt;hw.sh\n\ncat &lt;&lt;EOF &gt;bl.sh\n#!/bin/bash\n\necho 'Bonjour le Monde!'\nEOF\nWe‚Äôve added a file and altered one. Now git add --all and git commit -m \"more hi\"\nIf you do git log --decorate now, you‚Äôll see the pointer for goodbye has disappeared. It‚Äôs not gone, but it‚Äôs not part of the commit history for master at this point. The changes we made to goodbye and master are on diverging branches.\n\n\n2.3.2 Merging\nNow at this point we may be happy with changes we‚Äôve made to both branches and want to incorporate them. For this we use git merge. If we assume master is going to be our main branch, we probably want to incorporate our changes there.\nSo git checkout master to ensure we‚Äôre there, and git merge goodbye. This should drop you into whatever text editor you‚Äôve specified (if it‚Äôs vim, see last chapter) to write a message about the merge.\nGit is smart enough that it can figure out our changes don‚Äôt conflict with each other at the level of text, so it brings them all together. This doesn‚Äôt rule out a merge breaking functionality at some higher level.\nWhen changes cannot be trivially merged, git will ask you to edit the conflicting files.\nIf you‚Äôve merged a branch and you‚Äôre done with it, you can delete it (not the commit history, just the pointer).\ngit branch -d goodbye",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#branching-and-remotes",
    "href": "01_2_moreGit.html#branching-and-remotes",
    "title": "2¬† More Git",
    "section": "2.4 Branching and Remotes",
    "text": "2.4 Branching and Remotes\nWith remotes, things get a little more complicated. In the above example, we created a branch goodbye, made edits and then deleted the branch pointer without ever pushing the changes to the remote repository. goodbye never existed on the remote. If we push the changes now, the commit history will be recorded on the remote, but there hasn‚Äôt been and will not be a goodbye branch there.\nIf we want a branch to be represented on the remote (because we want collaborators to see it or work on it or we want to back it up), we need to add it there. When we do that, two things happen.\n\nWe create the branch on the remote (and push our commit history there).\nWe create another branch locally, called a remote tracking branch.\n\nLet‚Äôs see how this goes.\nCreate a new branch and check it out.\ngit branch congratulations\ngit checkout congratulations\nMake an edit.\ncat &lt;&lt;EOF &gt;cw.sh\n#!/bin/bash\n\necho 'Congratulations World!'\nEOF\nNow git add and git commit. If we simply try to push the branch, git will push our changes on master, but not those on congratulations. Try it and check out your remote (on GitHub, ideally, at this point).\ngit push\nTo create the branch on the remote and the remote tracking branch locally:\ngit push -u origin congratulations\nWith output:\nCounting objects: 4, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 306 bytes | 0 bytes/s, done.\nTotal 3 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote: \nremote: Create a pull request for 'congratulations' on GitHub by visiting:\nremote:      https://github.com/isg-certificate/newproject/pull/new/congratulations\nremote: \nTo git@github.com:isg-certificate/newproject.git\n * [new branch]      congratulations -&gt; congratulations\nBranch congratulations set up to track remote branch congratulations from origin.\nThe remote tracking branch keeps track of where in our commit history the remote was the last time we communicated with the remote. Above in the git log --decorate output we saw the branch (or reference, or pointer) origin/master indicating the commit associated with the remote tracking branch. We can see remote tracking branches with the -a flag in git branch\ngit branch -a -vv\n* congratulations                c6f675f [origin/congratulations] congratulations\n  master                         1c735ce [origin/master] Merge branch 'goodbye'\n  remotes/origin/congratulations c6f675f congratulations\n  remotes/origin/master          1c735ce Merge branch 'goodbye'\nNow that we‚Äôre tracking a branch that exists on our remote origin, it‚Äôs possible for the remote tracking branch and our local congratulations branch to diverge just like master and goodbye did above.\nThis can happen if work happens on the branches in two different places without pushing and pulling changes to and from the remote (such as when you work at two workstations, or a collaborator contributes). If the remote branch gets ahead of your tracking branch, git will not allow you to push new changes from your version of the branch before you get the changes from the remote and merge them.\nYou can update the tracking branch with git fetch and then you can do git merge to merge the tracking and local branches (fixing any conflicts as necessary) or you can do git pull, which updates the remote tracking branch and merges in one command (which will still require you to fix conflicts).",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#forking-and-pull-requests-on-github",
    "href": "01_2_moreGit.html#forking-and-pull-requests-on-github",
    "title": "2¬† More Git",
    "section": "2.5 Forking and pull requests on GitHub",
    "text": "2.5 Forking and pull requests on GitHub\nWe aren‚Äôt going to demo forking and pull requests here, because the key parts of that occur on the GitHub web site. But to put this in context: a single git repository can have branches that diverge and then merge again. When working with remotes, the complexity increases, especially when you have collaborators. Branches can exist in local repositories, but not the remote, and then be added to the remote (creating remote tracking branches in the local repository), and all these different branches can diverge and then be merged in various sequences.\nForking adds another layer of complexity. On GitHub you can fork someone else‚Äôs public repository. This is a bit like cloning it, except you are creating a mostly independent copy of it under your own account. You might create forks for a couple reasons:\n\nYou want to mess around with the repository and possibly adapt it to your own needs.\nYou found something you don‚Äôt like about the repository and you want to improve it, potentially sending your improvements back to the original developers for incorporation.\n\nIn both these cases, you probably don‚Äôt have permission to modify the original repository, or maybe your modifications would not be appropriate to its original purpose. You are likely not a close collaborator or co-worker if you are forking a repository, otherwise you would probably be able to simply clone the original and create branches to incorporate your work.\nOn your fork, you can do whatever you want. It‚Äôs your copy. You can‚Äôt accidentally push bad changes to the origina.\nIf you want to alter the original, however, you can create a pull request. This is a GitHub feature that allows you to notify the repository‚Äôs owner that you‚Äôve made a change you think should be incorporated into the original. The pull request creates a discussion thread that allows the owner to see what you‚Äôve proposed and talk about the changes, possibly asking you to tweak them.\nIn the end, the owner can merge the changes proposed from your fork in the original repository. Instead of doing this here, we‚Äôll have an assignment where students fork each other‚Äôs repositories and make pull requests.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#assorted-git-topics",
    "href": "01_2_moreGit.html#assorted-git-topics",
    "title": "2¬† More Git",
    "section": "2.6 Assorted Git Topics",
    "text": "2.6 Assorted Git Topics",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#visualizing-the-commit-history",
    "href": "01_2_moreGit.html#visualizing-the-commit-history",
    "title": "2¬† More Git",
    "section": "2.7 Visualizing the commit history",
    "text": "2.7 Visualizing the commit history\nThis has all been fairly abstract. If you think it might be helpful to be able to quickly visualize the history of a repository and the changes made to files, check out Sourcetree. You don‚Äôt have to connect it to your GitHub account, but you can if you want. To have a look at how it works, try cloning NF-Core‚Äôs rnaseq repository. This is a big repository with a lot of collaborators. If you want, you can make some arbitrary commits, branches, etc and see how they show up in the visualization of the history. Even though the GitHub repository is still the remote, you won‚Äôt have permission to push any changes.\n\n2.7.1 Ignoring files\nYou may want to git add --all sometimes when you have many changes to incorporate into a repository. However, it‚Äôs often the case in data analysis that as you develop your set of scripts and run them that you generate files you don‚Äôt want to commit, such as results and log files. You may also want to organize your project so that your data (or a symlink to your data) are inside the project directory to keep everything self-contained, and large raw data sets definitely don‚Äôt belong in a git repository.\nFor cases like these, you can create a file in the root of your working directory, .gitignore and write out file names and patterns there that you want git to ignore.\nCheck out the .gitignore file for this repository containing scripts for a bulk RNA-seq/differential expression tutorial.\nIt‚Äôs got a lot in it because these scripts all write results together, but briefly:\n-**/results matches any directory or file named results anywhere (and thus also ignores the contents of the directory) -*.fa matches any file ending in .fa\nThe documentation contains more details on pattern matching in .gitignore.\nYou can commit the .gitignore file to keep it as part of the repository.\n\n\n2.7.2 Removing and renaming\nYou can rm and mv just like you normally do, but it makes more work with Git. If you remove with git rm and rename (or move) with git mv Git will handle it better and stage the changes for you.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#getting-help",
    "href": "01_2_moreGit.html#getting-help",
    "title": "2¬† More Git",
    "section": "2.8 Getting help",
    "text": "2.8 Getting help\nPlease reach out through the teams channel if you need help. The Pro Git book is really nicely written as well. Re-reading sections of it as you get your feet under you will be helpful.\nGetting help with Git is also an ideal use case for LLMs like chatGPT. If you‚Äôve run into an issue that you don‚Äôt understand, an LLM can give you suggestions (with an explanation) about how to fix it that you can implement and then immediately check for correctness. It‚Äôs much less risky than relying on an LLM for something conceptually more complicated and harder to validate the output for, like choosing a parameter in a statistical model that is appropriate for your data.\n\n2.8.1 Git is kind of hard\nWhile the basic ideas behind the Git workflow are not that challenging, when getting more involved in collaborations, working with remotes, and actually navigating the complexity of the command-line tools, things can get difficult. Depending on how and how much you use git, you should expect there to be a somewhat long learning curve. Don‚Äôt get discouraged, it‚Äôs a useful and widespread tool and great to have some experience with.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html",
    "href": "02_1_qc.html",
    "title": "3¬† Variant calling - First Steps",
    "section": "",
    "text": "3.1 Learning Objectives\nIn this chapter, we‚Äôre going to introduce a basic outline of a variant detection workflow, and then work through the initial QC and mapping stages of the analysis. In the following chapters we‚Äôll cover actually calling variants and genotyping, filtering variants and evaluating quality, comparing variant call sets, and functional annotation.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#learning-objectives",
    "href": "02_1_qc.html#learning-objectives",
    "title": "3¬† Variant calling - First Steps",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw and mapped data specific to variant detection.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#the-basic-workflow",
    "href": "02_1_qc.html#the-basic-workflow",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.2 The basic workflow",
    "text": "3.2 The basic workflow\nOur workflow is going to be focused on calling short variants (SNPs/indels) for multiple samples against a single linear reference genome using short-read data. We‚Äôre not going to cover multiple whole genome alignment pan-genome graphs, or structural variant detection here, though those are really fun and interesting topics in bioinformatics.\nA typical reference-based variant detection workflow consists of the following steps:\n\nOrganizing your data resources and setting up your project structure (always the first step!).\nDoing basic QC on your input sequence data (and perhaps also your reference genome).\nIndexing your reference genome and aligning your sequence data to it.\nDoing QC on the alignments.\nCalling variants and genotyping.\nFiltering and evaluating the variant callset.\n\nIn this module, we will additionally look at ways to compare multiple variant callsets and functionally annotate them given a structural annotation of the genome.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#our-example-data",
    "href": "02_1_qc.html#our-example-data",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.3 Our example data",
    "text": "3.3 Our example data\nFor this module, we‚Äôre going to use data from the Genome-in-a-Bottle project which is created and housed in the US National Institute of Standards and Technology, a part of the Department of Commerce. NIST is devoted to the science of measurement and the development and use of standards. The GIAB data is created as part of an initiative to create benchmarking data to test methods of measuring genetic variation in humans.\nGIAB has a set of human samples they subject to every sequencing technology imaginable. They publish the data, along with products such as alignments and variant call sets for others to use. We‚Äôre going to use one set of three samples, a trio (mother, father, son) of Ashkenazi ancestry. For each sample, we‚Äôre going to download Illumina 2x250bp paired end sequence at about 50x coverage. In our scripts in this chapter, to make things run quickly, we‚Äôre going to cut the dataset down to look at only 5mb of chromosome 20. To make the QC steps interesting, this region overlaps a bit of messy sequence adjacent to the centromere.\nLinks to the data (and all the other sequencing data for these and other GIAB samples) can be found here.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#getting-set-up",
    "href": "02_1_qc.html#getting-set-up",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.4 Getting set up",
    "text": "3.4 Getting set up\nAssuming you‚Äôre working on UConn‚Äôs Xanadu (or Mantis) cluster, you should clone this github repository. It contains all the scripts that will be covered here. Each of them can be run in sequence to reproduce the results you‚Äôll see in this chapter. Please follow along, submitting them as you go. In the exercises for this module, you will be asked to modify and re-run the scripts and answer questions about what you find.\ngit clone git@github.com:isg-certificate/variants.git\nYou‚Äôll see initially that there is only a README.md file and a scripts directory containing scripts for the analysis. The scripts will build up more subdirectories as we go.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#getting-the-data",
    "href": "02_1_qc.html#getting-the-data",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.5 Getting the data",
    "text": "3.5 Getting the data\nIn the directory scripts/01_downloadData we have two scripts for downloading data. cd there now.\n\n3.5.1 The read data\nLet‚Äôs look at 01_getSequences.sh.\nTo make these examples run quickly, we‚Äôre going to grab only a subset of data from a small region of the genome. As you know at this point, fastq files straight off the sequencer are not ordered with respect to location in the genome, so doing this with truly raw data would be a challenge. GIAB provides genome-aligned BAM files, however, so we can do this relatively easily.\nLoad up our software modules:\nmodule load samtools/1.12 \nmodule load bedtools/2.29.0\nWe worked with samtools in ISG5311, so we won‚Äôt cover it again here. bedtools is new however. It‚Äôs a toolkit that allows the creation and manipulation of data in genomic windows. We‚Äôre going to elide the details in this section and cover it more depth shortly.\nTo grab the data (for one sample):\nSON='https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/NIST_Illumina_2x250bps/novoalign_bams/HG002.GRCh38.2x250.bam'\n\nsamtools view -uh $SON chr20:29400000-34400000 | \\\nsamtools sort -n - | \\\nbedtools bamtofastq -i /dev/stdin/ -fq $OUTDIR/son.1.fq -fq2 $OUTDIR/son.2.fq\nWe first set a variable pointing to the bam file for the son. Then we use samtools to extract region chr20:29400000-34400000 directly from the file on the server. This is a pretty nice feature of samtools. As along as the .bai index is also on the server, this approach will work great.\nNext we pipe the data to samtools sort -n to sort the reads by name instead of position.\nFinally, we‚Äôre sending the name-sorted output to bedtools bamtofastq, which will split the paired-end reads into separate files, reconstituting something like our raw data file, with a bunch of caveats. We could equally well have used samtools fastq to accomplish this.\nWe do this for all three samples (we could have done it in parallel, or used an array job as well). The last step of the script gzip compresses all the fastq files. So if we check out output directory:\n$ ll ../../data/\ntotal 765M\n-rw-r--r-- 1 nreid cbc 114M Feb  2 10:56 dad.1.fq.gz\n-rw-r--r-- 1 nreid cbc 125M Feb  2 10:56 dad.2.fq.gz\n-rw-r--r-- 1 nreid cbc 123M Feb  2 10:55 mom.1.fq.gz\n-rw-r--r-- 1 nreid cbc 134M Feb  2 10:55 mom.2.fq.gz\n-rw-r--r-- 1 nreid cbc 131M Feb  2 10:53 son.1.fq.gz\n-rw-r--r-- 1 nreid cbc 141M Feb  2 10:53 son.2.fq.gz\nYou should always check the .err. and .out files to make sure you script ran successfully. In this case you‚Äôre going to see a lot of warnings produced by bedtools that look like this:\n*****WARNING: Query D00360:94:H2YT5BCXX:1:1102:3248:71588 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping. \nCan you guess why these warnings occur?\n\n\n3.5.2 The reference genome\nNow let‚Äôs look at 02_getGenome.sh. This is very straightforward:\n# this is modified version of GRCh38 that corrects some errors. GIAB uses this for their benchmarking work. \n    # \"with masked false duplications and contaminations, as well as decoy sequences from CHM13, which we are now using for GIAB analyses\"\n    #  https://genomebiology.biomedcentral.com/articles/10.1186/s13059-023-02863-7\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz\ngunzip *gz\nThe human genome is 3.1 gigabases. That‚Äôs decently large (though it‚Äôs no axolotl). You could run something like QUAST on it to get some basic stats. When it‚Äôs your first time working with a given genome, it‚Äôs good to get to know it a bit. We won‚Äôt do that here though, we‚Äôll cover evaluating genome quality in the next module.\nThere are lots of human reference genome versions out there. GRCh38 was the state of the art until the telomere-to-telomere (CHM13) genome was released. Within GRCh38 there are multiple versions as well. We‚Äôre using GRCh38, and this very particular version, because GIAB has made some corrections to it (they do not alter the coordinate system, so GRCh38 annotations should work fine with it) and because it‚Äôs the one they use in their analyses. T2T is more complete and correct, however. Problems with a multiplicity of confusing genome versions are typically only a problem in model systems, and most of all in human.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#initial-qc",
    "href": "02_1_qc.html#initial-qc",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.6 Initial QC",
    "text": "3.6 Initial QC\nWe‚Äôve finished downloading our data, so let‚Äôs move to the directory scripts/02_qc.\n\n3.6.1 fastqc/multiqc on raw data\nWe have seen fastqc and multiqc used with RNA-seq data, so we won‚Äôt belabor this. But we‚Äôre going to run them on all our fastq files. Look at the script 01_fastqcRaw.sh:\n# run fastqc. \"*fq\" tells it to run on the illumina fastq files in directory \"data/\"\nfastqc -t 6 -o $OUTDIR ../../data/*1.fq.gz\nfastqc -t 6 -o $OUTDIR ../../data/*2.fq.gz\n\n# run on fastqc output\nmultiqc -f -o $OUTDIR/multiqc $OUTDIR\nCheck your outputs! There are some notable things here:\n\nThe sequence quality drops considerably at the end of these 250bp PE reads. It‚Äôs normal for sequence quality to drop. We‚Äôre going to trim a bit so it will increase.\nWe have a super-weird per-sequence GC content distribution! The bimodal shape is because we have deliberately pulled down reads from a region that is part run-of-the-mill genome sequence, and part really messy sequence next to the centromere. These two subregions of our 5mb window apparently have substantially different GC content.\nWe have a little bit of adapter contamination. It‚Äôs not so much that you would be super worried, but we‚Äôre going to trim it out anyway.\n\n\n\n3.6.2 Trimmomatic\nWe‚Äôve seen Trimmomatic before, but here‚Äôs the call, from a SLURM array script 02_trimmomatic.sh:\n# adapters to trim out\nADAPTERS=/isg/shared/apps/Trimmomatic/0.39/adapters/TruSeq3-PE-2.fa\n\n# sample bash array\nSAMPLELIST=(son mom dad)\n\n# run trimmomatic\n\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\njava -jar $Trimmomatic PE -threads 4 \\\n        ${INDIR}/${SAMPLE}.1.fq.gz \\\n        ${INDIR}/${SAMPLE}.2.fq.gz \\\n        ${TRIMDIR}/${SAMPLE}_trim.1.fq.gz ${TRIMDIR}/${SAMPLE}_trim_orphans.1.fq.gz \\\n        ${TRIMDIR}/${SAMPLE}_trim.2.fq.gz ${TRIMDIR}/${SAMPLE}_trim_orphans.2.fq.gz \\\n        ILLUMINACLIP:\"${ADAPTERS}\":2:30:10 \\\n        SLIDINGWINDOW:4:15 MINLEN:45\nShould be pretty standard at this point. In the .err files we should see trimming didn‚Äôt have a huge impact, e.g.:\nInput Read Pairs: 719326 Both Surviving: 708775 (98.53%) Forward Only Surviving: 9828 (1.37%) Reverse Only Surviving: 692 (0.10%) Dropped: 31 (0.00%)\nIf we look at our output directory:\nll ../../results/02_qc/trimmed_fastq/\ntotal 706M\n-rw-r--r-- 1 nreid cbc 108M Feb  2 11:19 dad_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 111M Feb  2 11:19 dad_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.3M Feb  2 11:19 dad_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 117K Feb  2 11:19 dad_trim_orphans.2.fq.gz\n-rw-r--r-- 1 nreid cbc 116M Feb  2 11:20 mom_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 120M Feb  2 11:20 mom_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.5M Feb  2 11:20 mom_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 122K Feb  2 11:20 mom_trim_orphans.2.fq.gz\n-rw-r--r-- 1 nreid cbc 123M Feb  2 11:20 son_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 126M Feb  2 11:20 son_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.5M Feb  2 11:20 son_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 131K Feb  2 11:20 son_trim_orphans.2.fq.gz\nNote that trimmomatic does produce ‚Äúorphans‚Äù files with singletons whose mate pairs did not survive trimming. We‚Äôre going to ignore those going forward.\n\n\n3.6.3 fastqc/multiqc on trimmed data\nThe script 03_fastqcTrimmed.sh is essentially the same as above, but pointed at the trimmmed fastq files. We see improved overall sequence quality, and our minor residual adapter contamination is gone. We still very much have the bimodal GC distribution though!",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#alignment",
    "href": "02_1_qc.html#alignment",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.7 Alignment",
    "text": "3.7 Alignment\nIn ISG5311 we used a spliced aligner, HISAT2 to align our RNA-seq reads to a reference genome. Here we‚Äôre going to use bwa-mem2, an extremely popular alignment program for aligning DNA. It is perfectly happy to do ‚Äúsplit-read‚Äù alignments, where chunks of a read are aligned non-contiguously. These are in principle quite similar to spliced alignments, but instead of having a large splice in the CIGAR string of the SAM record (e.g.¬†21M1552N80M with 552N indicating a 552bp intron), if bwa-mem2 determines a read should be split-aligned, it will output multiple alignment records, one for each aligned chunk. The first will be the primary alignment, and the rest will be flagged as secondary in the FLAG column of the SAM file.\nOur alignment scripts are located in scripts/03_Alignment. cd over there right now.\n\n3.7.1 Indexing\nAs always, when using a short-read aligner, the first step is to index the reference genome. We do this with the script 01_bwaIndex.sh. It‚Äôs very straightforward:\nbwa-mem2 index \\\n   -p $INDEXDIR/GRCh38 \\\n   $GENOME\nWe assign a prefix GRCh38 to be used for our index files, and that‚Äôs what we use to invoke the index later. The index files can be found in the results directory:\n$ ll ../../results/03_Alignment/bwa_index/\ntotal 16G\n-rw-r--r-- 1 nreid cbc 5.8G Feb  2 11:46 GRCh38.0123\n-rw-r--r-- 1 nreid cbc  18K Feb  2 11:44 GRCh38.amb\n-rw-r--r-- 1 nreid cbc 9.9K Feb  2 11:44 GRCh38.ann\n-rw-r--r-- 1 nreid cbc 9.4G Feb  2 12:34 GRCh38.bwt.2bit.64\n-rw-r--r-- 1 nreid cbc 740M Feb  2 11:44 GRCh38.pac\nNote that indexing can use significant amounts of memory (depending on the size of the genome) and take some time. For this job the SLURM seff output looks like this:\nJob ID: 8839080\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 8\nCPU Utilized: 00:50:12\nCPU Efficiency: 12.45% of 06:43:12 core-walltime\nJob Wall-clock time: 00:50:24\nMemory Utilized: 69.31 GB\nMemory Efficiency: 86.64% of 80.00 GB\nSo this human genome required around 70G of memory and 50 minutes to run. In the script it was allowed 8 cpus, but only used one of them, and apparently has no options to specify number of cpus/threads. So we over-requested resources on this. Not a big in this case.\n\n\n3.7.2 Alignment\nNow that we‚Äôve got our data all QC‚Äôed and our genome indexed, we‚Äôre ready to align. At this stage the workflow starts to diverge a little bit more from the RNA-seq of last semester. We have a couple new points to cover here.\nLet‚Äôs look at our script 02_bwaAlign.sh. It is another array script.\n\n3.7.2.1 Read group ID\nThe beginning is pretty standard stuff. The first weird thing we notice is the creation of the variable RG.\n# sample ID list\nSAMPLELIST=(son dad mom)\n\n# extract one sample ID\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\n# create read group string\nRG=$(echo \\@RG\\\\tID:$SAMPLE\\\\tSM:$SAMPLE)\nIn that line we populate the variable with, for the son, @RG\\tID:son\\tSM:son. This will ultimately be parsed as @RG     ID:son  SM:son.\nWe‚Äôre going to provide one of these strings to our aligner for each sample. It‚Äôs known as the read group string. This allows every read in a file to be tagged with some metadata. Different groups of reads can be kept in the same SAM/BAM file and keep a distinguishing identifier. There are a few bits of metadata that can be attached, most importantly for our use-case here is the sample tag SM, which is son in this case.\nRead group IDs must be attached for variant callers to correctly process the reads downstream. If you fail to attach them during the alignment step, you can add them later (e.g.¬†using PICARD), but you‚Äôll create a whole new BAM file and have to delete the old one. Very inefficient.\nThe line @RG     ID:son  SM:son will be added the the SAM header, and then for each alignment record (line in the SAM/BAM file), a tag is added: RG:Z:son. The ID tag is a short identifier for the whole read group.\n\n\n3.7.2.2 The alignment pipe\nNow we arrive at the actual alignment, which should be mostly familiar:\nbwa-mem2 mem -t 7 -R ${RG} ${INDEX} ${SAMPDIR}/${SAMPLE}_trim.1.fq.gz $SAMPDIR/${SAMPLE}_trim.2.fq.gz | \\\n    samblaster | \\\n    samtools view -S -h -u - | \\\n    samtools sort -T ${OUTDIR}/${SAMPLE}.temp -O BAM &gt;$OUTDIR/${SAMPLE}.bam \nWe provide our reads, the index, the read group string and sort and compress the output, just as we did with RNA-seq reads.\n\n\n3.7.2.3 Marking duplicates\nThere is one new step in there: samblaster. We are using this program to mark duplicate read pairs. In variant calling (and many other applications) we want to assume that each fragment of sequenced DNA derives from an independent original biological molecule. When we sequence reads covering a heterozygous site, our variant calling model assumes the alleles are sampled in proportion to their true frequency in the pool of DNA extracted from the organism.\nMany library preparation protocols, however, involve the use of the polymerase chain reaction, which creates copies of molecules. These copies can distort the apparent frequencies of alleles. It‚Äôs also possible for there to be ‚Äúoptical‚Äù duplicates, which are artifacts of the sequencer, not the library preparation.\nsamblaster identifies putative duplicates and marks them in the FLAG column of their SAM records. Downstream tools will (typically by default) ignore these duplicate reads.\nRead pairs are identified as duplicates when both members of the pair share the same alignment as another pair.\n\n\n\n3.7.3 Alignment indexing\nFinally, we‚Äôre going to align the sorted, compressed BAM file:\nsamtools index ${OUTDIR}/${SAMPLE}.bam\nThis index allows fast access to reads from any part of the reference genome.\nAnd Finally we‚Äôve got this in our results directory:\nll ../../results/03_Alignment/bwa_align/\ntotal 612M\n-rw-r--r-- 1 nreid cbc 190M Feb  2 12:49 dad.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:50 dad.bam.bai\n-rw-r--r-- 1 nreid cbc 202M Feb  2 12:49 mom.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:49 mom.bam.bai\n-rw-r--r-- 1 nreid cbc 216M Feb  2 12:49 son.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:49 son.bam.bai",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#alignment-qc",
    "href": "02_1_qc.html#alignment-qc",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.8 Alignment QC",
    "text": "3.8 Alignment QC\nAt this point it‚Äôs good practice to learn a bit about your alignment files. There are a few scripts in scripts/04_alignQC.\n\n3.8.1 samtools stats\nA straightforward first step is to summarize some aspects of the alignment. We can do that with samtools stats. It‚Äôs easy to run and can be summarized across samples using MultiQC. The script 01_samstats.sh does this. It also includes some bash code for pulling out the ‚Äúsummary numbers‚Äù table from the output for each sample and putting it into a table. That looks like this:\n\n\n\n\n\n\n\n\n\nMetric\ndad\nmom\nson\n\n\n\n\nRaw total sequences\n1308084\n1417550\n1505512\n\n\nFiltered sequences\n0\n0\n0\n\n\nSequences\n1308084\n1417550\n1505512\n\n\nIs sorted\n1\n1\n1\n\n\n1st fragments\n654042\n708775\n752756\n\n\nLast fragments\n654042\n708775\n752756\n\n\nReads mapped\n1307653\n1417146\n1505108\n\n\nReads mapped and paired\n1307222\n1416742\n1504704\n\n\nReads unmapped\n431\n404\n404\n\n\nReads properly paired\n1281272\n1392498\n1470964\n\n\nReads paired\n1308084\n1417550\n1505512\n\n\nReads duplicated\n19212\n39831\n39867\n\n\nReads MQ0\n18822\n19495\n22938\n\n\nReads QC failed\n0\n0\n0\n\n\nNon-primary alignments\n0\n0\n0\n\n\nSupplementary alignments\n11446\n10429\n14171\n\n\nTotal length\n304366705\n332326447\n353260305\n\n\nTotal first fragment length\n156799604\n170401850\n180976062\n\n\nTotal last fragment length\n147567101\n161924597\n172284243\n\n\nBases mapped\n304320288\n332284625\n353218188\n\n\nBases mapped (cigar)\n303309452\n331333643\n352041382\n\n\nBases trimmed\n0\n0\n0\n\n\nBases duplicated\n4487160\n9392133\n9387993\n\n\nMismatches\n2968410\n2783598\n3624412\n\n\nError rate\n9.79e-03\n8.40e-03\n1.03e-02\n\n\nAverage length\n232\n234\n234\n\n\nAverage first fragment length\n240\n240\n240\n\n\nAverage last fragment length\n226\n228\n229\n\n\nMaximum length\n250\n250\n250\n\n\nMaximum first fragment length\n250\n250\n250\n\n\nMaximum last fragment length\n250\n250\n250\n\n\nAverage quality\n36.7\n36.8\n36.8\n\n\nInsert size average\n404.2\n409.4\n419.2\n\n\nInsert size standard deviation\n117.1\n88.5\n187.1\n\n\nInward oriented pairs\n633663\n690440\n730889\n\n\nOutward oriented pairs\n14307\n11466\n14481\n\n\nPairs with other orientation\n659\n760\n754\n\n\nPairs on different chromosomes\n4982\n5705\n6228\n\n\nPercentage of properly paired reads (%)\n98.0\n98.2\n97.7\n\n\n\nThat‚Äôs a lot of data, especially when you‚Äôve got many samples, so it‚Äôs often a good idea to read this information into R and make some plots that can help you understand what‚Äôs going.\nThere are a few bits that are important to pay attention to:\n\nThe mapping rate ‚Äúreads mapped‚Äù / ‚ÄúRaw total sequences‚Äù: This tells you how many of your reads actually mapped to the reference genome! In this case the rate should be extremely high because we only grabbed mapped reads in the first place. Unmapped reads may be attributable to GIAB‚Äôs alignment having been produced by a different aligner.\nReads MQ0: These are reads that have been mapped, but assigned a mapping quality of 0, indicating the mapper has no confidence the alignment location is correct.\nReads duplicated: This is the number of duplicate reads mentioned above. Ideally this will be very low, but there is no ‚Äúcorrect‚Äù percentage. Observed duplication rates depend heavily on library prep, input DNA, and sequencing depth.\nPercentage of properly paired reads: This is the number of read pairs with the expected relative orientation and that are not too far apart / on separate reference sequences. This number tends to be larger when divergence between sample and reference is high and there is structural variation.\nError rate: this is the number of mismatches in mapped reads divided by the number of bases mapped. This encompasses both sequencing errors and true variation. When you have lots of samples you can get a sense of what this value should be in your study, and outliers with high error rates can indicate problematic samples.\n\n\n\n3.8.2 Looking at coverage\nAnother really important thing to do at this stage is to look at the distribution of depth of coverage across your genome. As you are learning in ISG5302, strong deviations from expected coverage can point to genomic regions with alignment issues. In those regions, variant calls are likely to be highly unreliable. You will want to know about those regions so that you can filter them out later (or even during variant calling) or at least treat them with a lot of caution.\nThere are lots of ways you can do this. We are going to write our own little pipe to calculate per-base coverage summed across all three samples in 1kb windows (see mosdepth for a fast alternative. This will be a helpful introduction to bedtools as well.\nTo give a brief overview: We will first use bedtools to create a BED file defining the genomic windows over which we want to calculate coverage. Then, in a single pipe, we‚Äôre going to merge our three BAM files into a single stream, filter it a little bit, and pass it to samtools depth, which will output the depth of coverage at each base in the genome. We will pipe that stream to awk to reformat it into BED format, and then pipe that stream to bedtools map, also passing in our 1kb window file. This tool will allow us to summarize (mean and median) the per-base coverage in 1kb windows. We use 1kb windows because the resulting file is a lot smaller and easier to manage (3.1 million lines) than the raw per-base coverage (3.1 billion lines).\nTo create the window file we do the following:\n# create faidx genome index file\nGENOME=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\nFAI=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.fai\nsamtools faidx ${GENOME}\n\n# make a \"genome\" file, required by bedtools makewindows command, set variable for location\nGFILE=${OUTDIR}/GRCh38.genome\ncut -f 1-2 $FAI &gt; $GFILE\n\n# make 1kb window bed file, set variable for location\nWIN1KB=${OUTDIR}/GRCh38_1kb.bed\nbedtools makewindows -g ${GFILE} -w 1000 &gt;${WIN1KB}\nbedtools makewindows wants a ‚Äúgenome‚Äù file, which is just a table with each sequence in the reference and its length, in order to create the window BED file.\nbedtools makewindows is then pretty straightforward -w is the window size. If you want the windows to be overlapping you can specify a ‚Äúslide‚Äù smaller than the window size with -s.\nTo refresh your memory about BED files: BED has only 3 required columns: the sequence identifier, the start position and the end position. The trick here is that whereas GTF is 1-based and fully closed, BED is 0-based and half-open. This means that in a GTF file, an interval of 1-1000 refers to the first 1000 bases, including the start and end point of the interval. In BED format, however, the first base in a sequence is numbered 0, and the end base in an interval is not included as part of the interval, making the comparable BED interval 0-1000. See this blog for a discussion about intervals in genomics.\nOk, so after creating our window file, we can now run our pipe:\nbamtools merge -list ${OUTDIR}/bam.list | \\\nbamtools filter -in - -mapQuality \"&gt;30\" -isDuplicate false -isProperPair true | \\\nsamtools depth -a /dev/stdin | \\\nawk '{OFS=\"\\t\"}{print $1,$2-1,$2,$3}' | \\\nbedtools map \\\n-a ${WIN1KB} \\\n-b stdin \\\n-c 4 -o mean,median,count \\\n-g ${GFILE} | \\\nbgzip &gt;${OUTDIR}/coverage_1kb.bed.gz\nWe use bamtools here to merge and filter our 3 samples (provided in a list). bamtools is older software at this point and no longer being updated, so this pipeline could use a refresh with samtools merge, but it will work for now.\nWe pipe the output to samtools depth -a. -a simply means output all sites, even if they are zero.\nThe output looks like this:\nchr20   29858643        83\nchr20   29858644        82\nchr20   29858645        83\nchr20   29858646        82\nchr20   29858647        84\nThe awk line reformats samtools depth output to BED format, putting the depth column in column 4:\nchr20   29858642   29858643        83\nchr20   29858643   29858644        82\nchr20   29858644   29858645        83\nchr20   29858645   29858646        82\nchr20   29858646   29858647        84\nFinally, we‚Äôve got bedtools map. This tool takes in intervals in BED format with argument -a, and maps the intervals provided using argument -b. You can do all kinds of operations on the mapped intervals. Here we are telling bedtools map to grab column 4 of argument -b (-c 4) and calculate the mean, median, and total number of records (-o mean,median,count) for column 4 in each -b interval overlapping a given interval from -a. The output looks like this:\nchr20   34387000        34388000        155.231 156     1000\nchr20   34388000        34389000        174.651 179     1000\nchr20   34389000        34390000        203.441 202     1000\nchr20   34390000        34391000        133.606 136     1000\nchr20   34391000        34392000        234.953 236     1000\nWhat can we do with this table? Let‚Äôs make some plots in R:\nFirst load up the tidyverse:\n\nlibrary(tidyverse)\n\nRead in the table and filter down to just our target region:\n\ncov &lt;- read.table(\"variants/results/04_alignQC/coverage/coverage_1kb.bed.gz\", header=FALSE) %&gt;%\n  filter(V1==\"chr20\", V2 &gt;= 29400000 & V3 &lt;= 34400000)\ncolnames(cov) &lt;- c(\"chromosome\", \"start\", \"end\", \"mean\", \"median\", \"count\")\n\n# some stats read in as character data, fix that:\ncov[, 4] &lt;- as.numeric(cov[,4])\ncov[, 5] &lt;- as.numeric(cov[,5])\n\nFirst, let‚Äôs ask what the median (of medians, of course) coverage is across windows:\n\nmedian(cov$median)\n\n[1] 185\n\n\nNow let‚Äôs plot coverage over the region:\n\nggplot(cov, aes(x=start, y=mean)) + \n  geom_point(size=0.2)\n\n\n\n\n\n\n\n\nWell, we‚Äôve got some serious outliers here. We expect in the neighborhood of 50x coverage for each sample, so around 150x for all three. This plot goes up past 6000x coverage for a few points. That extreme of a coverage spike is guarantee there is mismapping in that spot.\nLet‚Äôs truncate the y-axis to get a closer look at the rest of the points:\n\nggplot(cov, aes(x=start, y=mean)) + \n  geom_point(size=0.2) + \n  ylim(0,500)\n\n\n\n\n\n\n\n\nAt a smaller scale we can see a little better the problematic coverage in left 30-40% of this region. This region is centromeric (or at least centromere-adjacent), and there are problems with the assembly because of the repetitiveness and low complexity of the sequence.\nYou can see spots where the coverage drops to zero. Some of those are actually gaps, others may be locations where no reads map with MAPQ &gt; 30. You can also see other spikes in coverage. You can explore this region of genome with the UCSC genome browser.\nLater on we‚Äôre going to want to either exclude some of these regions from variant calling, or filter them out afterward.\nThere aren‚Äôt really great ways of setting coverage thresholds for filtering. We usually just eyeball a graph and set some thresholds. Let‚Äôs say 90 and 260.\n\nggplot(cov, aes(x=mean)) + \n  geom_histogram(binwidth=10) +\n  xlim(0,500) + \n  geom_vline(xintercept=c(90, 260), color=\"red\", linetype=\"dashed\", size=1)\n\n\n\n\n\n\n\n\nHow many windows would we lose out of our 5000?\n\nsum(cov$mean &lt; 90 | cov$mean &gt; 260)\n\n[1] 645\n\n\nOrdinarily that would seem like a lot, but we‚Äôve deliberately chosen a problematic genomic region here.\nLet‚Äôs replot:\n\ncov &lt;- mutate(cov, \n              exclude = cov$mean &lt; 90 | cov$mean &gt; 260)\n\nggplot(cov, aes(x=start, y=mean, color=exclude)) + \n  geom_point(size=0.2) + \n  ylim(0,500)\n\n\n\n\n\n\n\n\nIt‚Äôs pretty clear we will be able to exclude some of the most aberrant regions with these thresholds. We could either use these thresholds to filter individual variants after the fact, or use them to identify regions to filter out either during or after variant calling by creating a bed file that merged together windows to include or exclude. We will cover that in the next chapter.\nJust for fun, we‚Äôve got a script 03_bedtoolsNuc.sh that uses bedtools to calculate the base content in windows. It‚Äôs a simple script, so we won‚Äôt review it here, but have a look and run it. We can load up the output:\n\n# read in the table and filter down the output to focal region\nnuc &lt;- read.table(\"variants/results/04_alignQC/bedtoolsnuc/nuc.bed.gz\", header=FALSE) %&gt;%\n    filter(V1==\"chr20\", V2 &gt;= 29400000 & V3 &lt;= 34400000)\ncolnames(nuc) &lt;- c(\"chromosome\", \"start\", \"end\", \"ATpct\", \"GCpct\", \"A\", \"C\", \"G\", \"T\", \"N\", \"other\", \"length\")\n\nLet‚Äôs look at the GC percentage:\n\nggplot(nuc, aes(x=start, y=GCpct)) + \n  geom_point(size=0.2)\n\n\n\n\n\n\n\n\nWe certainly have some wacky things going on with the GC content, especially in the left-most part of the plot and they are associated with aberrations in coverage. The spots where GC content plunges to zero are gaps in the assembly (you will see this if you plot the number of N‚Äôs in each window‚Äìcolumn 10).\nWe will leave off here for now. In the next chapter we‚Äôll tackle variant calling itself.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html",
    "href": "02_2_variantCalling.html",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "4.1 Learning Objectives\nIn this section we‚Äôre going to take our qc‚Äôed and aligned data and call variants using a couple different methods. We‚Äôll demonstrate freebayes and bcftools, which do multi-sample variant calling, and GATK which can do basic multi-sample variant calling, but we‚Äôll walk through the step-wise procedure for calling variants on multiple samples because it‚Äôs a great way of doing variant calling when your set of samples could grow over time. We‚Äôll also see a basic split-apply-combine approach to variant calling. We‚Äôll also introduce the VCF format and some tools we can use to manipulate it.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#learning-objectives",
    "href": "02_2_variantCalling.html#learning-objectives",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "Learning Objectives:\n\n\nIdentify variants and genotype samples against a reference genome.\n\n\nManipulate the VCF format.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#multi-sample-variant-calling",
    "href": "02_2_variantCalling.html#multi-sample-variant-calling",
    "title": "4¬† Variant Calling",
    "section": "4.2 Multi-sample variant calling",
    "text": "4.2 Multi-sample variant calling\nWhen you have more than one sample (typical when variant calling) you want to combine the samples together so they can be called simultaneously rather than calling variants on a per-sample basis and then combining them. This has a couple benefits:\n\nCombining the data increases statistical power. When alleles are shared by more than one individual, pooling the data adds more evidence that they are real (rare alleles always remain a relative challenge).\nCalling variants across samples unifies representation of complex variants. Some variants have more than one possible representation and calling samples together ensures representation is consistent across samples. Representation issues arise most often with complex variants involving multiple indels or mixtures of SNPs and indels and/or when you have multiple haplotypes. If you called variants on each sample separately with the intent to combine them later, you could wind up with the same alleles being represented differently, which could be a hindrance.\nHomozygous reference genotypes (at variable sites) are accurately called The output of variant calling algorithms typically only outputs variant records at sites with alternate alleles (ones that differ from the reference genome) and does not distinguish sites with no data (or not enough evidence to make a call) from sites where there is good evidence for a genotype that is homozygous for the reference allele. If you do single sample calling and combine later, your site x genotype matrix will be splattered with missing data and you won‚Äôt really know if those missing genotypes are ref/ref genotypes or actually missing data and it won‚Äôt be safe to assume either way.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#freebayes",
    "href": "02_2_variantCalling.html#freebayes",
    "title": "4¬† Variant Calling",
    "section": "4.3 freebayes",
    "text": "4.3 freebayes\nFreebayes is a popular variant caller. It uses a Bayesian model that accounts for many different features of the mapped data in trying to distinguish errors from true variation. It has a notably simple in application in that it does not require pre-processing or filters to be applied to the read data and produces output in a single step. As the authors note in the github readme, freebayes adheres to the Unix philosophy of creating modular tools that can read from stdin and write to stdout and be chained together by users into pipelines that fit their specific needs. This is a really nice feature that makes it easy to work with.\n\n4.3.1 freebayes and short haplotypes\nA major distinguishing feature of freebayes is that it outputs short haplotype variants. Working with haplotypes improves the sensitivity and specificity of variant calling, but as output, they can be useful or annoying, depending on your downstream application.\nConsider the codon TTT, which codes for phenyalanine. Now imagine you have a reference sequence with a protein containing TTT at some position and you have sequenced a diploid sample that is heterozygous TTT/TAA. If your variant caller outputs short haplotypes, you will see the genotype TT/AA and easily recognize that one is the reference sequence and the other produces a premature stop codon. If, like most others, your variant caller outputs two heterozygous SNPs T/A and T/A and no phasing information, you will have no way of knowing (without going back to the data) if you have the haplotypes TT/AA, producing phenylalanine/stop codon or TA/AT, producing leucine/tyrosine. Note that this is a general problem when we lack phasing information.\nAn example where these haplotypes can be a headache is in population genetics, where you might wish to calculate a statistic like pi, the mean pairwise divergence between a collection of sequences (phase is unimportant here). The short haplotypes introduce complexity into the calculations that simple SNPs avoid. .\nWe will look more at haplotypes and how to manage them later in this chapter in the section on variant normalization.\n\n\n4.3.2 Running freebayes\nAfter all that introduction, running freebayes is pretty simple. We will continue in the variants GitHub repository assuming you have completed chapter 2. Go to the directory scripts/05_variantCalling and look at script 01_freebayes.sh.\nWe‚Äôre going to provide freebayes with a list of bam files (a useful feature when you‚Äôve got tons of samples), so we‚Äôll create that first:\n# make a list of bam files\nls ${INDIR}/*.bam &gt;${INDIR}/bam_list.txt\nThen we can run the program, using a variable $GEN to hold the genome location and clean up the command line:\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# run freebayes\nfreebayes \\\n-f ${GEN} \\\n--bam-list ${INDIR}/bam_list.txt |\nbgzip -c &gt;${OUTDIR}/freebayes.vcf.gz\n\ntabix -p vcf ${OUTDIR}/freebayes.vcf.gz\n\n4.3.2.1 bgzip and tabix\nBy default freebayes will write results to the standard output in uncompressed format. We‚Äôre going to pipe that to bgzip to compress it and then index the compressed file with tabix. These are both part of the htslib project, which also includes samtools and bcftools (more on bcftools in a moment).\nbgzip is a variant of gzip: ‚Äúblock gzip‚Äù. It does a slightly modified gzip compression, but can still be read by gzip, gunzip, zgrep etc. The modification allows for easy indexing of position sorted tabular genomic data for fast access to data from any genomic region (just as with sorted, indexed bam files).\nOnce the file is bgzipped, we index with tabix. tabix can be used to index any bgzipped position sorted tabular file, and once indexed, retrieve data from the file. It has a few preset modes for BED, GFF, and VCF, but you can specify which columns contain the positional information so that any tabular format can be effectively indexed. Here we specify -p vcf because our output is in VCF format.\n\n\n4.3.2.2 A few other freebayes options\nIt‚Äôs worth skimming the github readme for freebayes. It has lots of information and isn‚Äôt hard to read. We‚Äôll highlight a few more useful options here.\n\nCalling variants in a specific region: You can limit freebayes to calling variants in a single genomic interval with the following flag -r chr20:29400000-34400000. It probably would have been wise to do that in our case, as we only really have usable data from that region (mismapped reads notwithstanding). You can also provide a BED file containing multiple target regions with -t targets.bed. We could have limited freebayes to calling variants only in windows within our coverage threshold this way. This would have sped things up a little, particularly as it would not have needed to churn through all the useless data in that region with 6000x coverage.\nSpecifying populations: freebayes puts a prior probability distribution on genotype frequencies that assumes that all samples in a run are drawn from a single population. If this is incorrect (it often is) this prior may increase the error rate for genotype calls (most likely by favoring heterozygote calls too much). If you have high coverage (as we do in the test data) this probably doesn‚Äôt matter too much, as the data will overwhelm the prior, but if your coverage is on the low end, it can cause real problems (particularly for very low pass sequencing). You can specify which samples belong to which populations with --populations FILE (see docs for details). You can turn off this prior altogether with -w.\nSpecifying genetic diversity: freebayes also puts a prior on the expected genetic diversity. By default this is 0.001. This is appropriate for human data, but won‚Äôt be correct for everything. Fundulus heteroclitus populations range in diversity from 0.008 to 0.02. Again, with high coverage data this prior will have less influence that with low. You can change it with -T &lt;expected diversity&gt;. You can turn off both this and the population prior with -k, a good idea if you have low coverage.\nSpecifying ploidy: To set default ploidy, or to provide ploidy (or copy number) by region, and even by sample, look at the options --ploidy and --cnv-map.\n\n\n\n4.3.2.3 Resource usage\nfreebayes is pretty lightweight. It can only use 1 cpu. The amount of memory required depends on the number of samples and the depth of coverage. For our 3 samples at ~50x coverage, we get the following seff output:\n$ seff 8846713\n\nJob ID: 8846713\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 01:03:01\nCPU Efficiency: 14.30% of 07:20:32 core-walltime\nJob Wall-clock time: 01:02:56\nMemory Utilized: 718.59 MB\nMemory Efficiency: 7.02% of 10.00 GB\nFor this run we considerably over-requested CPUs as we can only use 1. Memory is harder to predict and will increase with more samples. It took freebayes about an hour to process all of our data.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#bcftools",
    "href": "02_2_variantCalling.html#bcftools",
    "title": "4¬† Variant Calling",
    "section": "4.4 bcftools",
    "text": "4.4 bcftools\nbcftools is part of the htslib project mentioned above. It includes lots of tools for calling variants and for manipulating them downstream. Its variant calling pipeline is much, much faster than freebayes (or GATK, below), but it tends to produce slightly worse calls, mostly at loci where there is complex indel/snp variation. This comes down to the fact that it does not do haplotypic inference as freebayes does or local assembly as GATK does. Depending on your application, the speed and convenience gains may greatly outweigh the computational costs. It also has the benefit of being part of a large open-source project that is continuously developed and supported. Like freebayes, bcftools is very much developed around the Unix ethos of simple tools built for piping.\n\n4.4.1 Running bcftools\nCalling variants with bcftools actually involves two steps. First, a summary of the read pileup across samples is generated. This digests the raw mapped BAM file into a tabular format that tracks potential alternate alleles and their evidence in each sample. That ‚Äúpileup‚Äù file is then passed to a variant caller that evaluates the evidence and outputs variant calls and genotypes in VCF format.\nLet‚Äôs have a look at the script 02_bcftools.sh.\n# make a list of bam files\nls ${INDIR}/*.bam &gt;${INDIR}/bam_list.txt\n\n# set reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# call variants\nbcftools mpileup -f ${GEN} -b ${INDIR}/bam_list.txt -q 20 -Q 30 | bcftools call -m -v -Oz -o ${OUTDIR}/bcftools.vcf.gz \n\n# index vcf\ntabix -p vcf ${INDIR}/bcftools.vcf.gz\nAs above, we provide a list of bam files and the reference genome. We can do the full variant calling analysis by piping the two steps together bcftools mpileup which produces the summary of the evidence and bcftools call, which does the variant calling and genotyping.\nIn mpileup we have also provided input filters: -q 20 ignores all bases with base quality &lt; 20 and -Q 30 ignores all reads with mapping quality &lt; 30.\nIn call we provide some options: -m with uses the recommended ‚Äúmulti-allelic‚Äù caller, -v which outputs only variable sites, -Oz which outputs bgzip compressed variant calls and -o ${OUTDIR}/bcftools.vcf.gz to specify an output file name. We could also have let it write to stdout and redirected to a file (or piped the output to something else!).\n\n\n4.4.2 bcftools options\nWe‚Äôll keep this quick. bcftools has similar options to those mentioned above for freebayes. They can be found in the usage for mpileup and call. Note that there is no way to turn off the population priors, but you can provide a --group-samples file as with freebayes and put each sample in its own ‚Äúpopulation‚Äù.\n\n\n4.4.3 Resource usage\nbcftools is very lightweight. It finished in 1/10th of the time that freebayes did and used 1/4 of the memory.\nCores: 1\nCPU Utilized: 00:06:17\nCPU Efficiency: 99.74% of 00:06:18 core-walltime\nJob Wall-clock time: 00:06:18\nMemory Utilized: 164.06 MB\nMemory Efficiency: 3.20% of 5.00 GB",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#parallelizing-variant-calling",
    "href": "02_2_variantCalling.html#parallelizing-variant-calling",
    "title": "4¬† Variant Calling",
    "section": "4.5 Parallelizing variant calling",
    "text": "4.5 Parallelizing variant calling\nWe won‚Äôt do this here, but note that it is relatively straightforward to parallelize variant calling. The steps are:\n\nSpecify a set of windows within which to call variants.\nCall variants in each window separately (perhaps using GNU parallel or an array job).\nCombine the windowed variant calls, removing duplicate variants that overlap window edges.\n\nfreebayes distributes a script freebayes_parallel. You can find it here /isg/shared/apps/freebayes/1.3.4/freebayes-1.3.4/scripts/freebayes-parallel on Xanadu, or on github.\nIt‚Äôs a simple bash script. The meat of it is:\n\ncommand=(\"freebayes\" \"$@\")\n\n(\ncat \"$regionsfile\" | parallel -k -j \"$ncpus\" \"${command[@]}\" --region {}\n) | \nvcffirstheader |\nvcfstreamsort -w 1000 | \nvcfuniq \ncat $regionsfile pipes the genomic windows, however you define them, to parallel. The parallel option -k means process the input in order (and output it that way).-j gives a number of jobs to run simultaneously (one for each cpu in this case). Then you have the freebayes command line \"${command[@]}\" with --region {} appended to the end, so freebayes runs on just that region. All of this is wrapped in (), which typically groups commands into a single stdout stream. It‚Äôs not doing anything useful here, but it‚Äôs in the original script.\nThe output here is a single stream of VCF files\nThe next three commands process the output stream of variant calls. They are part of the vcflib suite. vcffirstheader retains the header from just the first VCF output (because these are parallel invocations of freebayes, each output VCF will have a header). vcfstreamsort sorts variants in a small window (1000 sites) to account for any weirdness due to duplicated variant calls at region edges. vcfuniq then removes any such duplicated variants.\nThis is handy for cases where the job can be usefully accelerated within a single node. Instead of using parallel, you could also break this out into an array job and spread the work over 100 (max number of jobs on Xanadu) simultaneous array tasks for the first step, and then in a second job combine the results.\nIf your independent project involves variant calling, you should really consider some version of this to speed things up. You can do it for any of the variant callers.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#two-step-variant-calling-with-gatk",
    "href": "02_2_variantCalling.html#two-step-variant-calling-with-gatk",
    "title": "4¬† Variant Calling",
    "section": "4.6 Two-step variant calling with gatk",
    "text": "4.6 Two-step variant calling with gatk\nThe last variant caller we‚Äôll demonstrate is the HaplotypeCaller module from the package GATK.\nGATK is a very large suite of tools centered mostly on variant calling. It‚Äôs developed at The Broad Institute, a large biomedical research institute focused on genomics, and affiliated with Harvard and M.I.T. GATK is one of the most widely used packages for variant calling and performs very well in tests. It has extensive documentation, and has published ‚Äúbest-practice‚Äù recommendations for how to use their software. Unfortunately, the best practices are often very complex 1, and sometimes require resources that are unavailable in model systems (e.g.¬†‚Äúgold-standard‚Äù variant call sets). In recent years the documentation has become out of date and hard to parse, with lots of broken links.\nWhile HaplotypeCaller can be run in one step just like freebayes, a cool feature of GATK is its ability to do stepwise joint calling of multiple samples. The advantage of this procedure arises mainly if you have a study where you are likely going to have sampling that increases over time. In this procedure, the heaviest computation is done first, and can be done independently for each sample. If you save the products of that computation, then when you get new samples, you only need to do the initial step for the new samples before combining all samples together. This can save lots of time over standard joint calling in which all the computation needs to be redone every time a new sample is obtained.\n\n4.6.1 Running the workflow\nThere are three key steps:\n\nRun HaplotypeCaller in GVCF mode on each sample. A GVCF is a modification of VCF format (which we haven‚Äôt covered yet). This is the heavy compute. It can be parallelized across samples and genomic regions if necessary.\nRun GenomicsDBImport to create a database of your samples (this runs quickly).\nRun GenotypeGVCFs to create joint genotype calls (this also runs quickly).\n\nLet‚Äôs look at the scripts.\n\n4.6.1.1 Creating the sequence dictionary\nWe need one helper file that we don‚Äôt already have, a sequence dictionary for the reference genome. We‚Äôre going to run 03_createDict.sh to create that. It uses Picard, a toolkit for manipulating genomes, alignment files, and variant calls.\n# load required software\nmodule load picard/2.23.9\n\n# input/output\nINDIR=../../results/03_Alignment/bwa_align/\n\nOUTDIR=../../results/05_variantCalling/gatk\nmkdir -p $OUTDIR\n\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# create required .dict file\njava -jar $PICARD CreateSequenceDictionary R=$GEN\nWe have seen Java programs run this way before, but let‚Äôs reinforce what‚Äôs happening. When we load the module, an environment variable is created that points at a java ‚Äújar‚Äù file.\n$ module load picard/2.23.9\n$ echo $PICARD\n/isg/shared/apps/picard/2.23.9/picard.jar\nWe start the program with java -jar $PICARD. We can modify the memory usage using command line options as we‚Äôll see in a later script.\n\n\n4.6.1.2 Generating the GVCF files\nIn our next step, we‚Äôll generate one GVCF file for each sample, using an array job. Look at the script 04_makeGVCFs.sh\nSAMPLELIST=(son dad mom)\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# run haplotype caller on one sample\ngatk HaplotypeCaller \\\n     -R ${GEN} \\\n     -I ${INDIR}/${SAMPLE}.bam \\\n     -ERC GVCF \\\n     --output ${OUTDIR}/${SAMPLE}.g.vcf\nThe flag -ERC GVCF directs GATK to produce the GVCF file. ERC stands for ‚Äúemit reference confidence‚Äù. So these files are actually tracking whether or not there is good evidence for a homozygous reference genotype in the sample.\nAfter we‚Äôve run this we should see this in the results directory:\n$ ll ../../results/05_variantCalling/gatk/\n-rw-r--r-- 1 nreid cbc  29M Feb  5 16:50 dad.g.vcf\n-rw-r--r-- 1 nreid cbc  91K Feb  5 16:50 dad.g.vcf.idx\n-rw-r--r-- 1 nreid cbc  26M Feb  5 16:51 mom.g.vcf\n-rw-r--r-- 1 nreid cbc  82K Feb  5 16:51 mom.g.vcf.idx\n-rw-r--r-- 1 nreid cbc  25M Feb  5 16:55 son.g.vcf\n-rw-r--r-- 1 nreid cbc  80K Feb  5 16:55 son.g.vcf.idx\nThis step uses the most resources. For one of the three array tasks:\n$ seff 8847173_0\nJob ID: 8847174\nArray Job ID: 8847173_0\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 02:05:44\nCPU Efficiency: 23.89% of 08:46:17 core-walltime\nJob Wall-clock time: 01:15:11\nMemory Utilized: 13.30 GB\nMemory Efficiency: 66.51% of 20.00 GB\nSo in fact we used something like 3hrs 45min of wall time to run these three tasks. This probably would have been a bit faster if we had specified that it should only run on our focal 5mb region instead of crawling across the whole genome. In any case, it took more resources that freebayes and vastly more resources than bcftools.\n\n\n4.6.1.3 Creating the database\nIn the next step we‚Äôre going to create our database. This step can be done on at most one reference sequence at a time. So you must specify a region (in this case all of chr20) when you do this step, and you must do this step once for each sequence in your reference genome. So in some sense GATK encourages parallelism at this stage, though this step and the next are quick enough that it doesn‚Äôt seem to that important in practice. Have a look at the script 05_DBimport.sh.\n# make an \"arguments\" file to provide all samples\nfind ${INDIR} -name \"*g.vcf\" &gt;${INDIR}/args.txt\nsed -i 's/^/-V /' ${INDIR}/args.txt\n\n#IMPORTANT: The -Xmx value the tool is run with should be less than the total amount of physical memory available by at least a few GB, as the native TileDB library requires additional memory on top of the Java memory. Failure to leave enough memory for the native code can result in confusing error messages!\ngatk --java-options \"-Xmx10g -Xms4g\" GenomicsDBImport \\\n  --genomicsdb-workspace-path ${OUTDIR} \\\n  --overwrite-existing-genomicsdb-workspace true \\\n  -L chr20 \\\n  --arguments_file ${INDIR}/args.txt\nWe need to tell it which samples to use. You can specify them on the command line with -V sample1.g.vcf, but that gets tedious. So we‚Äôre going to provide a file with a list of arguments to append to the command line ‚Äúargs.txt‚Äù. It contains a -V flag for each GVCF file.\nWhen we run GenomicsDBImport we specify some java options: --java-options \"-Xmx10g -Xms4g\". Those give the maximum and minimum memory boundaries for this execution of the program. See the comment line saying we need to request more memory than the java option max from SLURM.\nIn this case we‚Äôre providing a workspace path, but also telling the program to overwrite any existing data there (helpful when you‚Äôre testing the code out and running it over and over again).\nWhen we finally run this, it goes quickly, taking only 13 minutes:\n$ seff 8847371\nJob ID: 8847371\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 00:00:57\nCPU Efficiency: 7.14% of 00:13:18 core-walltime\nJob Wall-clock time: 00:01:54\nMemory Utilized: 2.08 GB\nMemory Efficiency: 13.86% of 15.00 GB\n\n\n4.6.1.4 Generating the VCF file\nFinally we can run the script that actually generates our VCF output, 06_genotypeGVCFs.sh. Let‚Äôs have a look at it:\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\ngatk GenotypeGVCFs \\\n    -R ${GEN} \\\n    -V gendb://../../results/05_variantCalling/gatk/db \\\n    -O ${OUTDIR}/gatk.vcf \n\nbgzip ${OUTDIR}/gatk.vcf \ntabix -p vcf ${OUTDIR}/gatk.vcf.gz\nAt this point it‚Äôs very straightforward. Provide the reference genome, the location of the database and an output file name. Because the database is already region-restricted, we don‚Äôt need to specify a region.\nAfter we output the VCF file, we bgzip it and tabix index it.\nAgain, this step does not require much in the way of resources. It took less than 1 minute to process chr20.\n$ seff 8847490\nJob ID: 8847490\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 00:00:49\nCPU Efficiency: 15.91% of 00:05:08 core-walltime\nJob Wall-clock time: 00:00:44\nMemory Utilized: 1.13 GB\nMemory Efficiency: 7.51% of 15.00 GB\n\n\n\n4.6.2 Options in the GATK approach\nIt is possible to split up the GVCF calling into subregions to speed it up, though it gets a little complex parallelizing across individuals and regions, requiring some work to organize.\nGATK also uses a population prior, but we don‚Äôt know how to turn it off or specify population groupings! You can change the expected genetic diversity, however, which by default is again set at 0.001 (this value is ubiquitous because it is approximately the value in humans).",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#what-about-aiml",
    "href": "02_2_variantCalling.html#what-about-aiml",
    "title": "4¬† Variant Calling",
    "section": "4.7 What about AI/ML!?!",
    "text": "4.7 What about AI/ML!?!\nThere is another big variant caller we are not covering in this course, but it is a pretty cool one: Google‚Äôs DeepVariant.\nDeepVariant uses machine learning rather than a probabilistic model to distinguish sequencing and mapping errors from true variation and genotype samples. In tests, it works well. The trick is that ML models like those used in DeepVariant need to be trained on a set of true variants. Ideally those true variants need to share similar characteristics with those in the data that will ultimately be analyzed.\nIn model systems (human, mouse, Drosophila) there are often resources available to train models, and the trained models may already be available (the one distributed with DeepVariant is trained on human data). Training is not computationally trivial and relies heavily on the quality of the input.\nIn non-model systems, good training data may not be available, or you may not have the time or expertise to do your own training. So the question becomes, will a variant detection model trained on another species do well on mine? The answer to that is‚Ä¶ maybe, or maybe not. It really depends on what features of the data the model has learned to associate with true variants and artifacts, and whether those are consistent with your species. For genetically diverse species, such as Fundulus heteroclitus, it‚Äôs highly likely that DeepVariant would do very poorly when applying a human-trained model. In humans, tight clusters of variants often signal false positives. In killifish, there is so much genetic diversity that all variants will look crowded together by comparison. This could conceivably cause problems.\nSee this blog post from google about this very question, suggesting that a human trained model didn‚Äôt do so well when run on mosquitoes, but species-specific training greatly improved the situation.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#the-vcf-format",
    "href": "02_2_variantCalling.html#the-vcf-format",
    "title": "4¬† Variant Calling",
    "section": "4.8 The VCF format",
    "text": "4.8 The VCF format\nOk, we‚Äôve finally gotten our VCF files. Let‚Äôs go over what a VCF file actually is. VCF (or Variant Call Format) is the dominant file format used to store variant calls generated from high-throughput sequencing data. VCF format is a bit of a bear because it packs in so much information. After we cover it here, we‚Äôll look at tools for extracting useful information from it.\nAt this point, the format will probably seem a little familiar, as it is a tabular file that begins with a (sometimes) extensive header. It even has one field (INFO) that serves as a garbage bin of semi-colon separated tags (just like the attributes field in GTF/GFF!).\nYou can find a formal specification here. But we‚Äôll also cover the basics in this section.\nWe‚Äôre going to use bcftools to extract bits of the VCF to view them. We saw above how we can use it for variant calling, but it‚Äôs got tons of other functionality (much like samtools!) so it‚Äôs worth skimming through the documentation to see what it can do beyond what you see here.\nWe‚Äôre going to look at the freebayes VCF because it produces the most annotations for each variant.\n\n4.8.1 The header\nVCF begins with a header. bcftools view prints a compressed VCF file to stdout, and -h prints only the header.\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\n##fileformat=VCFv4.2\n##FILTER=&lt;ID=PASS,Description=\"All filters passed\"&gt;\n##fileDate=20250205\n##source=freeBayes v1.3.4\n##reference=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n##contig=&lt;ID=chr1,length=248956422&gt;\n##contig=&lt;ID=chr2,length=242193529&gt;\n##contig=&lt;ID=chr3,length=198295559&gt;\n##contig=&lt;ID=chr4,length=190214555&gt;\n##contig=&lt;ID=chr5,length=181538259&gt;\n\n\nThe biggest part of the header is a sequence dictionary, listing all reference sequences and their lengths.\nLet‚Äôs look at the last few lines of the header:\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | tail -n 20 \n\n##INFO=&lt;ID=LEN,Number=A,Type=Integer,Description=\"allele length\"&gt;\n##INFO=&lt;ID=MQM,Number=A,Type=Float,Description=\"Mean mapping quality of observed alternate alleles\"&gt;\n##INFO=&lt;ID=MQMR,Number=1,Type=Float,Description=\"Mean mapping quality of observed reference alleles\"&gt;\n##INFO=&lt;ID=PAIRED,Number=A,Type=Float,Description=\"Proportion of observed alternate alleles which are supported by properly paired read fragments\"&gt;\n##INFO=&lt;ID=PAIREDR,Number=1,Type=Float,Description=\"Proportion of observed reference alleles which are supported by properly paired read fragments\"&gt;\n##INFO=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum depth in gVCF output block.\"&gt;\n##INFO=&lt;ID=END,Number=1,Type=Integer,Description=\"Last position (inclusive) in gVCF output record.\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=GQ,Number=1,Type=Float,Description=\"Genotype Quality, the Phred-scaled marginal (or unconditional) probability of the called genotype\"&gt;\n##FORMAT=&lt;ID=GL,Number=G,Type=Float,Description=\"Genotype Likelihood, log10-scaled likelihoods of the data given the called genotype for each possible genotype generated from the reference and alternate alleles given the sample ploidy\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Number of observation for each allele\"&gt;\n##FORMAT=&lt;ID=RO,Number=1,Type=Integer,Description=\"Reference allele observation count\"&gt;\n##FORMAT=&lt;ID=QR,Number=1,Type=Integer,Description=\"Sum of quality of the reference observations\"&gt;\n##FORMAT=&lt;ID=AO,Number=A,Type=Integer,Description=\"Alternate allele observation count\"&gt;\n##FORMAT=&lt;ID=QA,Number=A,Type=Integer,Description=\"Sum of quality of the alternate observations\"&gt;\n##FORMAT=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum depth in gVCF output block.\"&gt;\n##bcftools_viewVersion=1.21+htslib-1.21\n##bcftools_viewCommand=view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz; Date=Sat Feb 22 15:39:17 2025\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  son dad mom\n\n\nThey record how we produced the VCF we‚Äôre viewing now (unfortunately freebayes did not add its command line call to the header).\nNote lines beginning: INFO=&lt;ID=... and FORMAT=&lt;ID=.... These give the definitions of tags found in the INFO and FORMAT fields of the tabular data. If you want to find out what the tag DP means, for example, you can do:\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"=DP,\"\n\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Total read depth at the locus\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n\n\nThe final line is the header line for the tabular data that follows. The fields are:\n\nCHROM: The reference sequence of the variant record, e.g.¬†chr20 or possibly an NCBI accession.\nPOS: The left most position of the variant. In case of an insertion, will be the base to the left of an insertion. 1-indexed (not zero).\nID: Any database identifiers that have been attached to the variant (such as dbSNP IDs. see next chapter). Empty by default.\nREF: The reference allele. Cannot be empty, as in the case of an insertion. Will be th base to the left.\nALT: One or more alternate alleles. Comma-separated. Cannot be empty. In case of a deletion will be the base to the left of the deletion.\nQUAL: Phred-scaled variant quality determined by variant caller. You should know what these are by now!\nFILTER: Can be populated with values like PASS or FAIL or LowQual to indicate variants that pass some filtering procedure. Usually empty by default.\nINFO: A semicolon separated list of annotations of the variant. Almost always contains basic information like the total depth at the locus, the counts of each observed allele and some other useful stuff. We‚Äôll see how to extract it into an easier format later.\nFORMAT: Gives the format of the following fields, which contain genotypes.\ngenotypes for sample X: Every field from 10 onward contains genotype information for a sample along with (typically) some annotations, as defined in the FORMAT field.\n\nEach column from 10 onwards represents one sample‚Äôs vector of genotypes.\n\n\n4.8.2 The tabular data\nNow let‚Äôs look at the tabular data:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\nchr20   33100435    .   T   C   7253.87 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=218;CIGAR=1X;DP=218;DPB=218;DPRA=0;EPP=3.6478;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=91.0153;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8104;QR=0;RO=0;RPL=116;RPP=4.96263;RPPR=0;RPR=102;RUN=1;SAF=119;SAP=6.99465;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0  1/1:80:0,80:0:0:80:2938:-264.561,-24.0824,0 1/1:65:0,65:0:0:65:2454:-221.045,-19.567,0\nchr20   33100454    .   C   T   7306.07 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=219;CIGAR=1X;DP=219;DPB=219;DPRA=0;EPP=3.49615;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=87.0387;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8160;QR=0;RO=0;RPL=108;RPP=3.09954;RPPR=0;RPR=111;RUN=1;SAF=120;SAP=7.38299;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 1/1:76:0,76:0:0:76:2851:-256.745,-22.8783,0 1/1:84:0,84:0:0:84:3111:-280.139,-25.2865,0 1/1:59:0,59:0:0:59:2198:-198.023,-17.7608,0\nchr20   33100524    .   C   T   7087.29 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=222;CIGAR=1X;DP=222;DPB=222;DPRA=0;EPP=3.04943;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=84.1199;PAIRED=0.995495;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7917;QR=0;RO=0;RPL=102;RPP=6.17948;RPPR=0;RPR=120;RUN=1;SAF=118;SAP=4.92746;SAR=104;SRF=0;SRP=0;SRR=0;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 1/1:80:0,80:0:0:80:3005:-270.607,-24.0824,0 1/1:88:0,88:0:0:88:3005:-270.594,-26.4906,0 1/1:54:0,54:0:0:54:1907:-171.854,-16.2556,0\nchr20   33100922    .   TCCAT   TCAT    6892.35 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=225;CIGAR=1M1D3M;DP=226;DPB=180.8;DPRA=0;EPP=6.49431;EPPR=0;GTI=0;LEN=1;MEANALT=1.33333;MQM=59.7733;MQMR=0;NS=3;NUMALT=1;ODDS=89.5189;PAIRED=0.986667;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7707;QR=0;RO=0;RPL=108;RPP=3.79203;RPPR=0;RPR=117;RUN=1;SAF=124;SAP=8.11567;SAR=101;SRF=0;SRP=0;SRR=0;TYPE=del  GT:DP:AD:RO:QR:AO:QA:GL 1/1:96:0,95:0:0:95:3234:-291.03,-28.5979,0  1/1:61:0,61:0:0:61:2117:-190.736,-18.3628,0 1/1:69:0,69:0:0:69:2356:-212.226,-20.7711,0\nchr20   33101062    .   G   C   6054.04 .   AB=0.515152;ABP=3.1419;AC=5;AF=0.833333;AN=6;AO=195;CIGAR=1X;DP=228;DPB=228;DPRA=0;EPP=12.3755;EPPR=8.34028;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=69.7788;PAIRED=1;PAIREDR=0.969697;PAO=0;PQA=0;PQR=0;PRO=0;QA=7079;QR=1205;RO=33;RPL=115;RPP=16.6516;RPPR=8.34028;RPR=80;RUN=1;SAF=94;SAP=3.55595;SAR=101;SRF=21;SRP=8.34028;SRR=12;TYPE=snp GT:DP:AD:RO:QR:AO:QA:GL 1/1:87:0,87:0:0:87:3248:-292.458,-26.1896,0 0/1:66:32,34:32:1184:34:1175:-86.148,0,-86.9768 1/1:75:1,74:1:21:74:2656:-237.119,-20.4773,0\nchr20   33101256    .   C   T   1054.39 .   AB=0.58209;ABP=6.93191;AC=1;AF=0.166667;AN=6;AO=39;CIGAR=1X;DP=220;DPB=220;DPRA=0.875817;EPP=5.73856;EPPR=5.37479;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=63.486;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1470;QR=6671;RO=180;RPL=21;RPP=3.51141;RPPR=4.21667;RPR=18;RUN=1;SAF=21;SAP=3.51141;SAR=18;SRF=88;SRP=3.20332;SRR=92;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 0/0:84:83,0:83:3020:0:0:0,-24.9855,-271.782 0/0:69:69,0:69:2583:0:0:0,-20.7711,-232.64  0/1:67:28,39:28:1068:39:1470:-112.389,0,-76.2508\nchr20   33101562    .   CTTTTCTTTTGA    CTTCTTTTGA  529.868 .   AB=0.40678;ABP=7.46366;AC=1;AF=0.166667;AN=6;AO=24;CIGAR=1M2D9M;DP=225;DPB=221;DPRA=0.710843;EPP=3.37221;EPPR=4.11819;GTI=0;LEN=2;MEANALT=2;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=81.6124;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=875;QR=7339;RO=196;RPL=10;RPP=4.45795;RPPR=26.4533;RPR=14;RUN=1;SAF=17;SAP=12.0581;SAR=7;SRF=128;SRP=42.8945;SRR=68;TYPE=del    GT:DP:AD:RO:QR:AO:QA:GL 0/0:76:75,0:75:2831:0:0:0,-22.5772,-254.814 0/1:59:34,24:34:1257:24:875:-61.466,0,-95.8355  0/0:90:87,0:87:3251:0:0:0,-26.1896,-292.616\nchr20   33101824    .   G   A   725.819 .   AB=0.461538;ABP=3.84548;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=219;DPB=219;DPRA=0.844156;EPP=26.4622;EPPR=3.42611;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=67.3506;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1156;QR=7082;RO=188;RPL=20;RPP=10.2485;RPPR=3.0103;RPR=10;RUN=1;SAF=6;SAP=26.4622;SAR=24;SRF=69;SRP=31.8863;SRR=119;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 0/0:91:90,0:90:3378:0:0:0,-27.0927,-304.011 0/0:63:63,0:63:2374:0:0:0,-18.9649,-213.856 0/1:65:35,30:35:1330:30:1156:-84.7577,0,-100.405\nchr20   33102044    .   T   G   0   .   AB=0;ABP=0;AC=0;AF=0;AN=6;AO=4;CIGAR=1X;DP=193;DPB=193;DPRA=0.919118;EPP=11.6962;EPPR=4.97275;GTI=0;LEN=1;MEANALT=1.5;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=61.3932;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=72;QR=6826;RO=187;RPL=0;RPP=11.6962;RPPR=7.20229;RPR=4;RUN=1;SAF=0;SAP=11.6962;SAR=4;SRF=101;SRP=5.62303;SRR=86;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:72:71,1:71:2576:1:13:0,-20.3742,-230.715    0/0:53:48,3:48:1794:3:59:0,-10.1644,-156.119    0/0:68:68,0:68:2456:0:0:0,-20.47,-221.219\nchr20   33102134    .   T   C,G 7.50367e-15 .   AB=0,0;ABP=0,0;AC=0,0;AF=0,0;AN=6;AO=2,3;CIGAR=1X,1X;DP=171;DPB=171;DPRA=0.571429,0.655405;EPP=7.35324,9.52472;EPPR=4.31842;GTI=0;LEN=1,1;MEANALT=2,1.5;MQM=60,60;MQMR=60;NS=3;NUMALT=2;ODDS=46.6774;PAIRED=1,1;PAIREDR=1;PAO=0,0;PQA=0,0;PQR=0;PRO=0;QA=55,37;QR=5947;RO=166;RPL=1,0;RPP=3.0103,9.52472;RPPR=4.31842;RPR=1,3;RUN=1,1;SAF=1,0;SAP=3.0103,9.52472;SAR=1,3;SRF=86;SRP=3.48122;SRR=80;TYPE=snp,snp GT:DP:AD:RO:QR:AO:QA:GL 0/0:74:74,0,0:74:2603:0,0:0,0:0,-22.2762,-234.443,-22.2762,-234.443,-234.443    0/0:38:34,2,2:34:1226:2,2:55,25:0,-5.81527,-105.463,-8.66201,-104.431,-108.169  0/0:59:58,0,1:58:2118:0,1:0,12:0,-17.4597,-190.705,-16.5608,-189.93,-189.629\n\n\nIn this case we specify -H to suppress the header and -r chr20:33100000-33200000 to identify a particular region.\nWe can see there‚Äôs a lot of information there. Let‚Äôs take it a bit at a time.\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 1-7\n\nchr20   33100435    .   T   C   7253.87 .\nchr20   33100454    .   C   T   7306.07 .\nchr20   33100524    .   C   T   7087.29 .\nchr20   33100922    .   TCCAT   TCAT    6892.35 .\nchr20   33101062    .   G   C   6054.04 .\nchr20   33101256    .   C   T   1054.39 .\nchr20   33101562    .   CTTTTCTTTTGA    CTTCTTTTGA  529.868 .\nchr20   33101824    .   G   A   725.819 .\nchr20   33102044    .   T   G   0   .\nchr20   33102134    .   T   C,G 7.50367e-15 .\n\n\nMuch cleaner! We can see a diversity of variant calls here. We have 7 biallelic SNPs, one multi-allelic SNP and two longer variants. The last two variants have very bad quality scores (0 and 7.50367e-15) respectively. Those are an example of freebayes opting for extreme high sensitivity, and we will definitely filter those out later. The other variant calls have rather extreme high quality scores. Improbably high, really. The probability of error for QUAL=7253.87 is 10-725. Take a moment to ask yourself whether we should ever believe any statistical output with that degree of confidence.\nAnyway‚Ä¶ these high quality variants are most likely real. The SNPs are straightforward. The haplotypes represent deletions, but they don‚Äôt seem to be represented in a very parsimonious way. The first one might better have the alleles TC and T indicating a single-base deletion. We‚Äôll see how we can standardize representation later, but we can in fact see that‚Äôs exactly how GATK does it:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/gatk/gatk.vcf.gz | head | cut -f 1-7\n\nchr20   33100435    .   T   C   8075.9  .\nchr20   33100454    .   C   T   8110.9  .\nchr20   33100524    .   C   T   7343.9  .\nchr20   33100922    .   TC  T   8409.86 .\nchr20   33101062    .   G   C   6336.13 .\nchr20   33101256    .   C   T   1110.13 .\nchr20   33101562    .   CTT C   935.09  .\nchr20   33101824    .   G   A   997.13  .\nchr20   33102548    .   G   A   4207.13 .\nchr20   33103215    .   G   A   856.13  .\n\n\nNow let‚Äôs look at the INFO field:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 8\n\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=218;CIGAR=1X;DP=218;DPB=218;DPRA=0;EPP=3.6478;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=91.0153;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8104;QR=0;RO=0;RPL=116;RPP=4.96263;RPPR=0;RPR=102;RUN=1;SAF=119;SAP=6.99465;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=219;CIGAR=1X;DP=219;DPB=219;DPRA=0;EPP=3.49615;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=87.0387;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8160;QR=0;RO=0;RPL=108;RPP=3.09954;RPPR=0;RPR=111;RUN=1;SAF=120;SAP=7.38299;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=222;CIGAR=1X;DP=222;DPB=222;DPRA=0;EPP=3.04943;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=84.1199;PAIRED=0.995495;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7917;QR=0;RO=0;RPL=102;RPP=6.17948;RPPR=0;RPR=120;RUN=1;SAF=118;SAP=4.92746;SAR=104;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=225;CIGAR=1M1D3M;DP=226;DPB=180.8;DPRA=0;EPP=6.49431;EPPR=0;GTI=0;LEN=1;MEANALT=1.33333;MQM=59.7733;MQMR=0;NS=3;NUMALT=1;ODDS=89.5189;PAIRED=0.986667;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7707;QR=0;RO=0;RPL=108;RPP=3.79203;RPPR=0;RPR=117;RUN=1;SAF=124;SAP=8.11567;SAR=101;SRF=0;SRP=0;SRR=0;TYPE=del\nAB=0.515152;ABP=3.1419;AC=5;AF=0.833333;AN=6;AO=195;CIGAR=1X;DP=228;DPB=228;DPRA=0;EPP=12.3755;EPPR=8.34028;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=69.7788;PAIRED=1;PAIREDR=0.969697;PAO=0;PQA=0;PQR=0;PRO=0;QA=7079;QR=1205;RO=33;RPL=115;RPP=16.6516;RPPR=8.34028;RPR=80;RUN=1;SAF=94;SAP=3.55595;SAR=101;SRF=21;SRP=8.34028;SRR=12;TYPE=snp\nAB=0.58209;ABP=6.93191;AC=1;AF=0.166667;AN=6;AO=39;CIGAR=1X;DP=220;DPB=220;DPRA=0.875817;EPP=5.73856;EPPR=5.37479;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=63.486;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1470;QR=6671;RO=180;RPL=21;RPP=3.51141;RPPR=4.21667;RPR=18;RUN=1;SAF=21;SAP=3.51141;SAR=18;SRF=88;SRP=3.20332;SRR=92;TYPE=snp\nAB=0.40678;ABP=7.46366;AC=1;AF=0.166667;AN=6;AO=24;CIGAR=1M2D9M;DP=225;DPB=221;DPRA=0.710843;EPP=3.37221;EPPR=4.11819;GTI=0;LEN=2;MEANALT=2;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=81.6124;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=875;QR=7339;RO=196;RPL=10;RPP=4.45795;RPPR=26.4533;RPR=14;RUN=1;SAF=17;SAP=12.0581;SAR=7;SRF=128;SRP=42.8945;SRR=68;TYPE=del\nAB=0.461538;ABP=3.84548;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=219;DPB=219;DPRA=0.844156;EPP=26.4622;EPPR=3.42611;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=67.3506;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1156;QR=7082;RO=188;RPL=20;RPP=10.2485;RPPR=3.0103;RPR=10;RUN=1;SAF=6;SAP=26.4622;SAR=24;SRF=69;SRP=31.8863;SRR=119;TYPE=snp\nAB=0;ABP=0;AC=0;AF=0;AN=6;AO=4;CIGAR=1X;DP=193;DPB=193;DPRA=0.919118;EPP=11.6962;EPPR=4.97275;GTI=0;LEN=1;MEANALT=1.5;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=61.3932;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=72;QR=6826;RO=187;RPL=0;RPP=11.6962;RPPR=7.20229;RPR=4;RUN=1;SAF=0;SAP=11.6962;SAR=4;SRF=101;SRP=5.62303;SRR=86;TYPE=snp\nAB=0,0;ABP=0,0;AC=0,0;AF=0,0;AN=6;AO=2,3;CIGAR=1X,1X;DP=171;DPB=171;DPRA=0.571429,0.655405;EPP=7.35324,9.52472;EPPR=4.31842;GTI=0;LEN=1,1;MEANALT=2,1.5;MQM=60,60;MQMR=60;NS=3;NUMALT=2;ODDS=46.6774;PAIRED=1,1;PAIREDR=1;PAO=0,0;PQA=0,0;PQR=0;PRO=0;QA=55,37;QR=5947;RO=166;RPL=1,0;RPP=3.0103,9.52472;RPPR=4.31842;RPR=1,3;RUN=1,1;SAF=1,0;SAP=3.0103,9.52472;SAR=1,3;SRF=86;SRP=3.48122;SRR=80;TYPE=snp,snp\n\n\nThere‚Äôs a ton of INFO. Many of these are statistics you could use to filter on. The tags are all defined in the header, as we mentioned above. A couple we look at a lot are DP, giving the total depth, AF giving the alternate allele frequency, and AO, giving the number of reads supporting the alternate allele.\nNow let‚Äôs look at the format and genotype fields:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 9-\n\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0  1/1:80:0,80:0:0:80:2938:-264.561,-24.0824,0 1/1:65:0,65:0:0:65:2454:-221.045,-19.567,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:76:0,76:0:0:76:2851:-256.745,-22.8783,0 1/1:84:0,84:0:0:84:3111:-280.139,-25.2865,0 1/1:59:0,59:0:0:59:2198:-198.023,-17.7608,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:80:0,80:0:0:80:3005:-270.607,-24.0824,0 1/1:88:0,88:0:0:88:3005:-270.594,-26.4906,0 1/1:54:0,54:0:0:54:1907:-171.854,-16.2556,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:96:0,95:0:0:95:3234:-291.03,-28.5979,0  1/1:61:0,61:0:0:61:2117:-190.736,-18.3628,0 1/1:69:0,69:0:0:69:2356:-212.226,-20.7711,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:87:0,87:0:0:87:3248:-292.458,-26.1896,0 0/1:66:32,34:32:1184:34:1175:-86.148,0,-86.9768 1/1:75:1,74:1:21:74:2656:-237.119,-20.4773,0\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:84:83,0:83:3020:0:0:0,-24.9855,-271.782 0/0:69:69,0:69:2583:0:0:0,-20.7711,-232.64  0/1:67:28,39:28:1068:39:1470:-112.389,0,-76.2508\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:76:75,0:75:2831:0:0:0,-22.5772,-254.814 0/1:59:34,24:34:1257:24:875:-61.466,0,-95.8355  0/0:90:87,0:87:3251:0:0:0,-26.1896,-292.616\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:91:90,0:90:3378:0:0:0,-27.0927,-304.011 0/0:63:63,0:63:2374:0:0:0,-18.9649,-213.856 0/1:65:35,30:35:1330:30:1156:-84.7577,0,-100.405\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:72:71,1:71:2576:1:13:0,-20.3742,-230.715    0/0:53:48,3:48:1794:3:59:0,-10.1644,-156.119    0/0:68:68,0:68:2456:0:0:0,-20.47,-221.219\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:74:74,0,0:74:2603:0,0:0,0:0,-22.2762,-234.443,-22.2762,-234.443,-234.443    0/0:38:34,2,2:34:1226:2,2:55,25:0,-5.81527,-105.463,-8.66201,-104.431,-108.169  0/0:59:58,0,1:58:2118:0,1:0,12:0,-17.4597,-190.705,-16.5608,-189.93,-189.629\n\n\nThe FORMAT field is a colon separated list of tags indicating what each genotype contains. These are also defined in the header.\nConsider this format field GT:DP:AD:RO:QR:AO:QA:GL and this genotype field 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0. It works out like this:\n\n\n\nGT\nDP\nAD\nRO\nQR\nAO\nQA\nGL\n\n\n\n\n1/1\n73\n0,73\n0\n0\n73\n2712\n-244.24,-21.9752,0\n\n\n\nGT is the big one! Alleles are numbered from 0 as REF, ALT1, ALT2‚Ä¶ALTN. A genotype 0/1 indicates a REF/ALT heterozygote. The allele separator / indicates an unphased genotype. If the separator were instead | (0|1) that would indicate that all consecutive genotypes from that sample also using the separator were phase-known. For example, at position 1 you have 0|1, position 10 you have 1|0 and position 20 you have 0|3. You know that the two haplotypes spanning positions 1-20 are 010 and 103.\nUnphased genotypes have alleles sorted numerically, so a REF/ALT heterozygote will always be 0/1 and not 1/0. Obviously the ordering matters for phased genotypes.\nMissing genotypes are encoded differently by different programs. They may simply be . or they may be ./.. For some programs missing genotypes will also have the rest of the annotations, but also missing, e.g ./.:.:.:.:.:.:.:..\nDP gives the total depth for the sample. AD is the depth for each possible allele in the genotype. We don‚Äôt often scrutinize these except in cases where we are interested in particular variants and their genotypes, or we are curious about some behavior of the variant caller that doesn‚Äôt match our expectations for the data. RO and AO give counts of REF and ALT allele observations, while QR and QAgive the sums of the phred base qualities for REF and ALT alleles.\nFor most callers there will be some version of a comma-separated genotype likelihood vector, in this case GL, which is the log10 scaled likelihood (i.e.¬†the probability of the read data) for each possible genotype given the alleles. The values are scaled so that they are relative to the highest likelihood genotype (which makes that value 0).\n\n\n4.8.3 How do we dig in to a VCF file?\nWe‚Äôll look at a few very basic things with linux/bash here, but we‚Äôll save the bulk for the next chapter. These tricks are useful when you want a quick glance at what‚Äôs going on in a VCF file. You should only really use them on smallish regions as exploratory measures when you get results back.\nLet‚Äôs start by asking how many variants we‚Äôve got we‚Äôve got in our target region.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | wc -l\n\n   57423\n\n\nWhat do the QUAL scores look like? We‚Äôll try four categories: &lt; 10, &gt;= 10 & &lt; 30, &gt;= 30 & &lt; 100, &gt;= 100.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nawk '\n  {if($6 &lt; 10){w+=1}}\n  {if($6 &gt;= 10 && $6 &lt; 30){x+=1}}\n  {if($6 &gt;= 30 && $6 &lt; 100){y+=1}}\n  {if($6 &gt;= 100){z+=1}}\n  END {print w,x,y,z}'\n\n39872 408 883 16260\n\n\nThe vast majority are pretty very low quality. At least in this dataset, setting a QUAL threshold anywhere between 10 and 100 won‚Äôt have a big impact on the number of variants retained. GATK and bcftools won‚Äôt output so many garbage variants. Extremely high sensitivity is just how freebayes rolls.\nWe can also quickly pull out INFO field tags if we want. Let‚Äôs grab the overall depth tag DP and summarize it along the lines of our coverage thresholds we considered previously.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nggrep -oP \"(?&lt;=DP=)[0-9]+\" |\nawk '\n  {if($1 &lt; 90){w+=1}}\n  {if($1 &gt;= 90 && $1 &lt; 260){x+=1}}\n  {if($1 &gt;= 260 && $1 &lt; 1000){y+=1}}\n  {if($1 &gt;= 1000){z+=1}}\n  END {print w,x,y,z}'\n\n9748 38025 7154 2496\n\n\nNote the ggrep is because this is being compiled on a mac, which natively has BSD versions of grep with slightly different options ggrep is the GNU version of grep that is installed as plain old grep on linux systems.\nThe regex \"(?&lt;=DP=)[0-9]+\" contains a zero-length assertion, it pulls out strings preceded by DP=.\nThere isn‚Äôt really strong concordance between the QUAL numbers and the DP numbers. Not all bad variant records fall outside the depth thresholds.\nLet‚Äôs pull out one more tag that freebayes produces, the TYPE tag.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nggrep -oP \"(?&lt;=TYPE=)[a-zA-Z]+\" |\nsort | uniq -c | sort -g\n\n 318 mnp\n1144 ins\n1416 del\n3075 complex\n51470 snp\n\n\nWe can see the vast majority of variant records are categorized as SNPs, though we also have insertions, deletions, complex variants, and multi-nucleotide polymorphisms (typically short haployptes of just SNPs).\nNow let‚Äôs look at the distribution of genotypes in just one sample:\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\ncut -f 10 |\nsed 's/:.*//' |\nsort | uniq -c | sort -g\n\n   1 0/5\n   1 0/7\n   1 4/6\n   3 0/4\n   3 1/4\n   3 2/3\n   8 2/2\n   9 1/3\n  21 0/3\n 141 .\n 158 0/2\n 427 1/2\n2781 1/1\n13722 0/1\n40144 0/0\n\n\nAs we might expect, most genotypes are 0/0 in this sample, with the next most common being 0/1. We can see there are a handful of loci that have many alleles. These are probably mostly false positives. After all, with only 3 diploids, it‚Äôs impossible to have more than 6 alleles at a site, and if sites with such allelic diversity existed, it would be next to impossible to sample 6 different alleles with only three individuals anyway.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#conclusions",
    "href": "02_2_variantCalling.html#conclusions",
    "title": "4¬† Variant Calling",
    "section": "4.9 Conclusions",
    "text": "4.9 Conclusions\nWe‚Äôve now seen several pieces of software for calling variants from mapped data. We‚Äôve covered the VCF format and how to dig into a little bit. In the next chapter we will look at tools for manipulating and extracting information from VCF files, summarizing them, filtering them and reformatting.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#footnotes",
    "href": "02_2_variantCalling.html#footnotes",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "For preprocessing, they recommend that fastq files be converted to unmapped bam so that sequences can be annotated (e.g.¬†with adapter contamination), then converted back to fastq, then aligned, then finally for the aligned sequences to be merged with the unmapped bam to reintroduce the annotations made on the unmapped sequence. Whew.‚Ü©Ô∏é",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html",
    "href": "02_3_filteringVariants.html",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "",
    "text": "5.1 Learning Objectives\nIn this chapter we‚Äôll look at ways to extract information from VCF, and apply hard filters to variants.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html#learning-objectives",
    "href": "02_3_filteringVariants.html#learning-objectives",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "",
    "text": "Learning Objectives:\n\n\nManipulate the VCF format.\n\n\nApply strategies for filtering problematic variants.\n\n\nAssessing variant call set quality",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html#summarizing-variants-call-sets",
    "href": "02_3_filteringVariants.html#summarizing-variants-call-sets",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "5.2 Summarizing variants call sets",
    "text": "5.2 Summarizing variants call sets\nWhen we get a VCF file back, we want to calculate some high level summaries to give us a sense of the general characteristics of the variant call set. Some standard statistics are the number of variants, numbers of variants in different categories and the transition/transversion ratio. We may also dig in a little more to look at other features like the depth of coverage, the distribution of quality scores, rates of missing data.\nSome of these we may be able to develop a prior expectations for, like the numbers of variants we expect to detect in a sample of individuals from a population. Across most of the tree of life, you can expect genetic diversity to fall within the window 0.0005-0.01. What this means is that you can expect somewhere between 0.5 and 10 heterozygous sites per 1000bp in a diploid individual (that is not inbred). In humans, this number should be around 0.001, or 1 heterozygous site per 1000bp. What does this mean for a VCF file with variants from multiple individuals? Well, a commonly used estimator of genetic diversity Watterson‚Äôs theta gives a simple relationship:\n\\[\n\\hat{\\theta}_w = \\frac{K}{a_n}\n\\] And\n\\[\na_n = \\sum_{i=1}^{n-1} \\frac{1}{i}\n\\]\n\\(\\hat{\\theta}_w\\) is the genetic diversity. \\(K\\) is the number of segregating sites (or variable sites in our VCF file). \\(a_n\\) is a function of the number of alleles we‚Äôve sampled (in 10 diploid individuals \\(n = 20\\)).\nTo get a per-site value, divide \\(\\hat{\\theta}_w\\) by the total number of sites analyzed.\nIf we replaced \\(\\hat{\\theta}_w\\) with our expected value (e.g.¬†0.001 in humans), multiplied by \\(a_n\\), and multiplied by the total number of sites analyzed, that would give us a rough estimate of how many variable sites we should see.\nThis is very rough. Lots of factors contribute to genetic diversity, and it can change among populations within species and among genomic regions. Nevertheless, having a ballpark sense of how many variants you should see can help you understand if you‚Äôre on the right track.\nWith that very light detour through population genetics out of the way‚Ä¶\nFor other statistics, we may just need to look at the empirical distributions in our data to get a sense of what ‚Äúnormal‚Äù is.\nWe used bash tools to extract some pieces of the VCF file to inspect in the previous chapter. Clever bash/awk one-liners are always great for quick inspection, but there are other more effective tools we can use to get a better picture.\n\n5.2.1 bcftools stats\nLet‚Äôs start with bcftools stats. Like samtools stats, it will output several tables in one output stream with data from each table prepended by a prefix. We can grab them by grepping out the prefix of interest. To list out all possible prefixes:\n\nbcftools stats variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"^#\"\n\n# This file was produced by bcftools stats (1.21+htslib-1.21) and can be plotted using plot-vcfstats.\n# The command line was: bcftools stats  variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\n#\n# Definition of sets:\n# ID    [2]id   [3]tab-separated file names\n# SN, Summary numbers:\n#   number of records   .. number of data rows in the VCF\n#   number of no-ALTs   .. reference-only sites, ALT is either \".\" or identical to REF\n#   number of SNPs      .. number of rows with a SNP\n#   number of MNPs      .. number of rows with a MNP, such as CC&gt;TT\n#   number of indels    .. number of rows with an indel\n#   number of others    .. number of rows with other type, for example a symbolic allele or\n#                          a complex substitution, such as ACT&gt;TCGA\n#   number of multiallelic sites     .. number of rows with multiple alternate alleles\n#   number of multiallelic SNP sites .. number of rows with multiple alternate alleles, all SNPs\n# \n#   Note that rows containing multiple types will be counted multiple times, in each\n#   counter. For example, a row with a SNP and an indel increments both the SNP and\n#   the indel counter.\n# \n# SN    [2]id   [3]key  [4]value\n# TSTV, transitions/transversions\n#   - transitions, see https://en.wikipedia.org/wiki/Transition_(genetics)\n#   - transversions, see https://en.wikipedia.org/wiki/Transversion\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\n# SiS, Singleton stats:\n#   - allele count, i.e. the number of singleton genotypes (AC=1)\n#   - number of transitions, see above\n#   - number of transversions, see above\n#   - repeat-consistent, inconsistent and n/a: experimental and useless stats [DEPRECATED]\n# SiS   [2]id   [3]allele count [4]number of SNPs   [5]number of transitions    [6]number of transversions  [7]number of indels [8]repeat-consistent    [9]repeat-inconsistent  [10]not applicable\n# AF, Stats by non-reference allele frequency:\n# AF    [2]id   [3]allele frequency [4]number of SNPs   [5]number of transitions    [6]number of transversions  [7]number of indels [8]repeat-consistent    [9]repeat-inconsistent  [10]not applicable\n# QUAL, Stats by quality\n# QUAL  [2]id   [3]Quality  [4]number of SNPs   [5]number of transitions (1st ALT)  [6]number of transversions (1st ALT)    [7]number of indels\n# IDD, InDel distribution:\n# IDD   [2]id   [3]length (deletions negative)  [4]number of sites  [5]number of genotypes  [6]mean VAF\n# ST, Substitution types:\n# ST    [2]id   [3]type [4]count\n# DP, depth:\n#   - set id, see above\n#   - the depth bin, corresponds to the depth (unless --depth was given)\n#   - number of genotypes with this depth (zero unless -s/-S was given)\n#   - fraction of genotypes with this depth (zero unless -s/-S was given)\n#   - number of sites with this depth\n#   - fraction of sites with this depth\n# DP, Depth distribution\n# DP    [2]id   [3]bin  [4]number of genotypes  [5]fraction of genotypes (%)    [6]number of sites  [7]fraction of sites (%)\n\n\nLet‚Äôs start with the summary numbers:\n\nbcftools stats variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"^SN\"\n\nSN  0   number of samples:  3\nSN  0   number of records:  71579\nSN  0   number of no-ALTs:  0\nSN  0   number of SNPs: 64307\nSN  0   number of MNPs: 5055\nSN  0   number of indels:   3375\nSN  0   number of others:   760\nSN  0   number of multiallelic sites:   3849\nSN  0   number of multiallelic SNP sites:   1565\n\n\nSo we‚Äôve got 71,579 records. We‚Äôre looking at a 5mb region. That‚Äôs an average of 1.4 variable sites in every 100bp. That‚Äôs waaaay too many. We‚Äôve sampled two independent individuals (you don‚Äôt expect to discover new variants in the son), so it would yield a Watterson‚Äôs theta that is almost 8x too high:\n\\[\n\\frac{71,579 / (5*10^6)} {\\sum_{i=1}^{3} \\frac{1}{i}}  = 0.007808618\n\\]\nOr in R code: 71579 / 5e6 / sum(1/(1:3)) . Freebayes is outputting a lot of junk we‚Äôll filter later.\nWe can apply a filter right now and recalculate the stats:\n\nbcftools stats -e \"QUAL &lt; 30\" variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"^SN\"\n\nSN  0   number of samples:  3\nSN  0   number of records:  23679\nSN  0   number of no-ALTs:  0\nSN  0   number of SNPs: 19186\nSN  0   number of MNPs: 2211\nSN  0   number of indels:   2456\nSN  0   number of others:   459\nSN  0   number of multiallelic sites:   987\nSN  0   number of multiallelic SNP sites:   41\n\n\n-e \"QUAL &lt; 30\" excludes all variants with quality less than 30.\nOur number of variable sites is a bit high still, leading to a Watterson‚Äôs theta of 0.002583164. About twice what we expect. However, we‚Äôre only looking at 5mb of the genome, and a challenging 5mb at that. We also haven‚Äôt really thought hard about filtering yet.\nWe should point out one last thing here about per-site statistics. Here we are dividing Watterson‚Äôs theta by 5mb to get genetic diversity per site. That assumes that ALL 5mb in our window can be accurately variant-called. We know that‚Äôs not true from our prior evaluation of coverage. Some regions had excess coverage, some had very little. This is a consequence of VCF tracking only sites where variation has been observed! We also are assuming each variant represents one site. This is definitely not true with freebayes! If you want to calculate genetic diversity statistics for real, not just in this rough back-of-the-napkin ballpark kind of way, you need to accurately count up the number of sites where you could have detected genetic variation if it was there, and you either need to break down variants into their constituent parts and count only SNPs, or account for the possibility of multiple differences between haplotypes.\nNext let‚Äôs look at the transition/transversion ratio:\n\nbcftools stats variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"TSTV\"\n\n# TSTV, transitions/transversions\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\nTSTV    0   26920   36791   0.73    26230   35205   0.75\n\n\nTransitions are changes between purines, so A&lt;-&gt;G and C&lt;-&gt;T. The other four changes are pyrimidines. So if changes are random, you expect a ts/tv ratio of 0.5. The true genome-wide average in most organisms is much higher. In humans it‚Äôs &gt; 2, though it varies across the genome and among non-coding, synonymous and non-synonymous sites. Here we observe 0.73. We might generally take this to mean our variant call set was full of bad calls (which it currently is!), but again, part of our window is a challenging region of the genome.\nIf we apply that quality filter again we get a very different result:\n\nbcftools stats -e \"QUAL &lt; 30\" variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"TSTV\"\n\n# TSTV, transitions/transversions\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\nTSTV    0   10898   7440    1.46    10846   7376    1.47\n\n\nts/tv = 1.46. Still way below our genome-wide average, but much higher.\n\n\n5.2.2 vt peek\nThis is a bit redundant to bcftools, but an old, but pretty helpful program called vt (installed on Xanadu) can produce a nice summary of a VCF file with the submodule peek:\nvt peek variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\npeek v0.5\n\noptions:     input VCF file            variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\n\n\nstats: no. of samples                     :          3\n       no. of chromosomes                 :         59\n\n       ========== Micro variants ==========\n\n       no. of SNP                         :      61009\n           2 alleles                      :           59555 (0.75) [25615/33940]\n           3 alleles                      :            1417 (0.36) [752/2082]\n           4 alleles                      :              37 (0.50) [37/74]\n\n       no. of MNP                         :       1147\n           2 alleles                      :            1137 (1.25) [1305/1040]\n           3 alleles                      :               9 (0.74) [17/23]\n           4 alleles                      :               1 (0.20) [1/5]\n\n       no. of INDEL                       :       2873\n           2 alleles                      :            2573 (0.80) [1143/1430]\n           3 alleles                      :             238 (1.12) [251/225]\n           4 alleles                      :              48 (1.15) [77/67]\n           &gt;=5 alleles                    :              14 (1.07) [29/27]\n\n       no. of SNP/MNP                     :       1877\n           2 alleles                      :            1522 (0.80) [678/844]\n           3 alleles                      :             292 (0.77) [341/443]\n           4 alleles                      :              54 (0.67) [84/126]\n           &gt;=5 alleles                    :               9 (0.35) [12/34]\n\n       no. of SNP/INDEL                   :        537\n           2 alleles                      :             148 (0.78) [65/83] (0.72) [62/86]\n           3 alleles                      :             282 (0.78) [137/176] (0.64) [122/192]\n           4 alleles                      :              84 (0.66) [54/82] (0.62) [55/89]\n           &gt;=5 alleles                    :              23 (0.49) [19/39] (0.59) [16/27]\n\n       no. of MNP/INDEL                   :         99\n           2 alleles                      :              76 (0.46) [61/133] (0.41) [22/54]\n           3 alleles                      :              20 (0.74) [26/35] (0.58) [11/19]\n           4 alleles                      :               3 (0.29) [2/7] (0.40) [2/5]\n\n       no. of SNP/MNP/INDEL               :         16\n           3 alleles                      :               9 (0.50) [12/24] (0.50) [3/6]\n           4 alleles                      :               5 (1.12) [9/8] (2.50) [5/2]\n           &gt;=5 alleles                    :               2 (3.00) [9/3] (0.00) [0/3]\n\n       no. of MNP/CLUMPED                 :       2565\n           2 alleles                      :            2462 (1.04) [2874/2776]\n           3 alleles                      :             100 (0.93) [253/272]\n           4 alleles                      :               3 (0.57) [8/14]\n\n       no. of SNP/MNP/CLUMPED             :        976\n           3 alleles                      :             665 (0.71) [862/1209]\n           4 alleles                      :             246 (0.55) [387/706]\n           &gt;=5 alleles                    :              65 (0.71) [182/257]\n\n       no. of INDEL/CLUMPED               :        372\n           2 alleles                      :             257 (0.77) [112/145]\n           3 alleles                      :              79 (0.72) [51/71]\n           4 alleles                      :              23 (0.59) [22/37]\n           &gt;=5 alleles                    :              13 (0.53) [17/32]\n\n       no. of SNP/INDEL/CLUMPED           :         97\n           3 alleles                      :              34 (1.15) [53/46] (0.54) [14/26]\n           4 alleles                      :              32 (0.81) [66/81] (0.71) [17/24]\n           &gt;=5 alleles                    :              31 (0.52) [58/111] (1.57) [33/21]\n\n       no. of MNP/INDEL/CLUMPED           :          8\n           3 alleles                      :               5 (0.42) [8/19] (1.00) [4/4]\n           4 alleles                      :               3 (0.32) [8/25] (1.00) [3/3]\n\n       no. of SNP/MNP/INDEL/CLUMPED       :          3\n           4 alleles                      :               2 (1.75) [7/4] (0.00) [0/2]\n           &gt;=5 alleles                    :               1 (0.67) [2/3] (0.50) [1/2]\n\n       no. of micro variants              :      71579\n\n       ++++++ Other useful categories +++++\n\n        no. of clumped variants           :       4021\n           2 alleles                      :            2719 (1.01) [3190/3153] (0.77) [112/145]\n           3 alleles                      :             883 (0.75) [1334/1773] (0.68) [69/101]\n           4 alleles                      :             309 (0.62) [567/920] (0.64) [42/66]\n           &gt;=5 alleles                    :             110 (1.22) [469/383] (0.93) [51/55]\n\n        no. of block substitutions        :       6565\n           2 alleles                      :            5121 (1.04) [4857/4660]\n           3 alleles                      :            1066 (0.76) [1473/1947]\n           4 alleles                      :             304 (0.56) [480/851]\n           &gt;=5 alleles                    :              74 (0.67) [194/291]\n\n        no. of complex substitutions      :       1132\n           2 alleles                      :             481 (0.75) [442/593] (0.69) [196/285]\n           3 alleles                      :             429 (0.75) [394/527] (0.64) [205/318]\n           4 alleles                      :             152 (0.80) [237/297] (0.64) [104/162]\n           &gt;=5 alleles                    :              70 (1.88) [315/168] (0.79) [67/85]\n\n\n       ========= General summary ==========\n\n       no. of VCF records                        :      71579\n\n\nTime elapsed: 0.93s\n\n\n5.2.3 bcftools query\nSometimes we want to pull specific bits of information out of the VCF, read it into R and make plots. This can help us understand our data. There are a few different tools we can use to do this. In R there are two packages vcfR and VariantAnnotation, but for simplicity, here we‚Äôll focus on bcftools query. We can use that to extract what we need and then read that into R. This document gives some details.\nIt‚Äôs fairly straightforward, though format strings do look messy. The syntax is:\nbcftools query -f &lt;format string&gt; my.vcf.gz\nThe format string can include any of the parts of the VCF file. The main fields can be accessed as %CHROM %POS etc. Tags from the INFO field can be accessed like %INFO/DP for the depth tag. For parts of sample genotypes strings, use square brackets and tags from the FORMAT field and bcftools will loop over all samples. To extract genotypes, use [%GT].\nWhen putting tags together to define the output, if you want it to be tab-separated you need to use \\t in between tags.\nSome examples:\n\nbcftools query --print-header -r chr20:33000000- -f '%CHROM\\t%POS\\t%REF\\t%ALT\\t%QUAL\\t%INFO/DP\\t[%GT\\t]' variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\n#[1]CHROM   [2]POS  [3]REF  [4]ALT  [5]QUAL [6]DP   [7]son:GT   [8]dad:GT   [9]mom:GT   \nchr20   33000241    T   A   1425.05 194 0/1 0/0 0/1 \nchr20   33000521    G   A   1475.03 189 0/1 0/1 0/0 \nchr20   33000741    A   G   1.99799e-14 129 0/0 0/0 0/0 \nchr20   33000745    T   G   0   125 0/0 0/0 0/0 \nchr20   33000750    T   G   0   122 0/0 0/0 0/0 \nchr20   33000788    T   G   4.06677e-14 113 0/0 0/0 0/0 \nchr20   33000795    T   G   4.54383e-17 114 0/0 0/0 0/0 \nchr20   33000805    A   C   3.24255e-14 110 0/0 0/0 0/0 \nchr20   33000812    T   C   0   114 0/0 0/0 0/0 \n\n\n\nbcftools query --print-header -r chr20:33000000- -f '%CHROM\\t%POS\\t%REF\\t%ALT\\t%QUAL\\t%INFO/DP\\t%INFO/AB\\t[%GT\\t][%DP\\t]' variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\n#[1]CHROM   [2]POS  [3]REF  [4]ALT  [5]QUAL [6]DP   [7]AB   [8]son:GT   [9]dad:GT   [10]mom:GT  [11]son:DP  [12]dad:DP  [13]mom:DP  \nchr20   33000241    T   A   1425.05 194 0.455285    0/1 0/0 0/1 60  71  63  \nchr20   33000521    G   A   1475.03 189 0.487395    0/1 0/1 0/0 63  56  70  \nchr20   33000741    A   G   1.99799e-14 129 0   0/0 0/0 0/0 41  38  50  \nchr20   33000745    T   G   0   125 0   0/0 0/0 0/0 37  38  50  \nchr20   33000750    T   G   0   122 0   0/0 0/0 0/0 37  37  48  \nchr20   33000788    T   G   4.06677e-14 113 0   0/0 0/0 0/0 33  33  47  \nchr20   33000795    T   G   4.54383e-17 114 0   0/0 0/0 0/0 34  33  47  \nchr20   33000805    A   C   3.24255e-14 110 0   0/0 0/0 0/0 33  31  46  \nchr20   33000812    T   C   0   114 0   0/0 0/0 0/0 34  33  47  \n\n\nThis is a very simple way to grab these stats so they can be easily read in and summarized in R.\nMake the file:\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\nQOUT=variants/results/05_variantCalling/freebayes/freebayes.query.txt.gz\nbcftools query --print-header -r chr20:29400000-34400000 -f '%CHROM\\t%POS\\t%REF\\t%ALT\\t%QUAL\\t%INFO/DP\\t%INFO/AB\\t[%GT\\t][%DP\\t]' ${VCFIN} | gzip &gt;${QOUT}\n\nRead it in:\n\n# column names will be super ugly\ndf &lt;- read.table(\"variants/results/05_variantCalling/freebayes/freebayes.query.txt.gz\", header=TRUE, comment.char=\"\")\ncolnames(df) &lt;- colnames(df) %&gt;% str_remove(\"^X[.0-9]+\")\n\n# filter out multi-allelic loci for simplicity. fix AB so it's numeric (b/c comma-separated for multiallelic loci)\ndf &lt;- filter(df, !str_detect(ALT, \",\")) %&gt;%\n  mutate(AB=as.numeric(AB))\n\nMake some plots.\nFirst, allele balance in heterozygotes along our focal window, colored by variant quality. We can see lots of sites with really skewed allele balance (should be around 0.5) have low quality, but not all of them!\n\nggplot(df, aes(x=POS, y=AB, color=QUAL &gt; 30)) +\n  geom_point(size=0.2)\n\n\n\n\n\n\n\n\nNow, depth along the chromosome, colored by quality:\n\nggplot(df, aes(x=POS, y=DP, color=QUAL &gt; 30)) +\n  geom_point(size=0.2) +\n  ylim(0,400)\n\n\n\n\n\n\n\n\nWe could look at the distribution of depth of coverage as a histogram as well:\n\nggplot(df, aes(x=DP)) +\n  geom_histogram(binwidth=2) +\n  xlim(0,300)\n\nWarning: Removed 7834 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n\n\n\n\n\n\n\n\n\nWe see a pretty messy distribution. We might consider broader coverage cutoffs, like 50-250 here.\nWe can also do a quick summary of missing data rates by site and by individual. In this file, missing genotypes are encoded with .\n\ncolMeans(df[,8:10]==\".\")\n\n     son.GT      dad.GT      mom.GT \n0.002587896 0.005637916 0.008410662 \n\n\nWe can see overall a very low rate of missing data for individuals. Similarly, very few sites have any missing data. This isn‚Äôt always the case.\n\nrowSums(df[,8:10]==\".\") %&gt;% table()\n\n.\n    0     1     2 \n53320   656   122 \n\n\nBy looking at summaries like these (often for one region or a subsample of our data if we have a huge WGS variant call set), we can get a sense of the distribution of characteristics of our variant calls. We can also see that things like the quality score, allele balance, and depth can all flag variants as questionable, but they don‚Äôt highlight exactly the same set of questionable variants. Depending on how we plot them, they can also flag regions where weird things might be happening and variant calls might be generally problematic.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html#filtering-variants",
    "href": "02_3_filteringVariants.html#filtering-variants",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "5.3 Filtering variants",
    "text": "5.3 Filtering variants\nWe‚Äôve now seen how we can explore some aspects of our data. We can use this information to filter out data we think might be unreliable. What we‚Äôll do here is referred to as ‚Äúhard filtering‚Äù. We will look at individuals and at sites. We are going to identify a set of thresholds on characteristics we think can help us identify individuals or variants that are likely to be problematic (i.e.¬†that are false positives, or have high rates of genotyping error) and remove data based on those thresholds. We could also ‚Äúsoft filter‚Äù (sites only), meaning we would mark sites as not passing (in the FILTER field of the VCF) rather than remove them.\nThere are other approaches that can be used, such as variant quality score recalibration as implemented by GATK, but those require training sets, which aren‚Äôt always available, so we won‚Äôt cover them here.\nWhen we filter, we are always trying to strike a balance between leaving in too many false positives and badly genotyped sites (or individuals) and throwing out too much good data. How we strike that balance is pretty much always dataset, or at least application dependent. Filtering is often a rather ad hoc procedure. Unless you are working on a very established application, or specific experimental context, deciding how and what to filter may require some data exploration and judgement.\nIt‚Äôs important to note that filtering data can have unintended side-effects. As a general strategy, it‚Äôs not a great idea to filter on data characteristics that are directly related to the questions you‚Äôre interested in. If you wanted to calculate inbreeding coefficients for individuals, removing sites that violate Hardy-Weinberg equilibrium would be a really bad idea, as it might completely destroy any signal. Conversely, we have seen people looking to demonstrate genetic divergence between two sampling sites filter loci by how divergent they were between the sampling sites. Lo and behold, there was significant genetic divergence!\nAnother thing to be wary of is filtering for extreme values for putatively beneficial characteristics. Having higher depth of coverage certainly improves accuracy of genotype calling, all things being equal, but grabbing the SNPs in the top 1% of the coverage distribution is going to greatly enrich for false positives relating to mismapping (as we have seen).\nSo let‚Äôs get to it.\n\n5.3.1 Filtering with bcftools.\nWe‚Äôre going to lean heavily on bcftools here. Most of the tools in bcftools accept flags for filtering: -i/--include and -e/--exclude with filtering. We can set up filtering expressions using tags as we did above in bcftools query.\nBelow is an example where we filter on allele balance, depth, and variant quality.\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\nVCFOUT=variants/results/05_variantCalling/freebayes/freebayes_filter_test.vcf.gz\nbcftools view -r chr20:29400000-34400000 --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | AB &lt; .25 | AB &gt; 0.75 | QUAL &lt; 30 | AF &lt; 0.1' ${VCFIN} -Oz -o ${VCFOUT}\n\nWe used a series of or statements (|) to remove records that failed at least one. With bcftools filter We could have populated the FILTER field, rather than remove them:\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\nbcftools filter -r chr20:29400000-34400000 --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | AB &lt; .25 | AB &gt; 0.75 | QUAL &lt; 30 | AF &lt; 0.1' ${VCFIN} -s \"lowQual\" | bcftools view -H | head\n\nchr20   29400057    .   TGT TGGT    6.25005e-14 lowQual AB=0;ABP=0;AC=0;AF=0;AN=6;AO=12;CIGAR=1M1I2M;DP=151;DPB=155;DPRA=0;EPP=3.73412;EPPR=3.15295;GTI=0;LEN=1;MEANALT=1.66667;MQM=20.4167;MQMR=60;NS=3;NUMALT=1;ODDS=36.083;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=228;QR=5116;RO=137;RPL=2;RPP=14.5915;RPPR=7.591;RPR=10;RUN=1;SAF=9;SAP=9.52472;SAR=3;SRF=88;SRP=27.1184;SRR=49;TYPE=ins GT:DP:AD:RO:QR:AO:QA:GL 0/0:53:50,3:50:1871:3:63:0,-10.6117,-163.278    0/0:45:36,8:36:1378:8:126:0,-4.16829,-114.881   0/0:53:51,1:51:1867:1:39:0,-12.2659,-164.805\nchr20   29400070    .   T   C   3.29774e-07 lowQual AB=0.217391;ABP=34.9215;AC=1;AF=0.166667;AN=6;AO=18;CIGAR=1X;DP=156;DPB=156;DPRA=0;EPP=3.49285;EPPR=3.57677;GTI=0;LEN=1;MEANALT=1;MQM=21.0556;MQMR=60;NS=3;NUMALT=1;ODDS=16.3934;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=675;QR=4994;RO=138;RPL=3;RPP=20.3821;RPPR=15.3468;RPR=15;RUN=1;SAF=11;SAP=4.9405;SAR=7;SRF=87;SRP=23.4033;SRR=51;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 0/0:55:50,5:50:1776:5:184:0,-8.75316,-152.258   0/1:46:36,10:36:1312:10:375:-2.90954,0,-104.499 0/0:55:52,3:52:1906:3:116:0,-9.10278,-164.312\nchr20   29400078    .   C   T   0.0286297   lowQual AB=0.254902;ABP=29.6215;AC=1;AF=0.166667;AN=6;AO=23;CIGAR=1X;DP=164;DPB=164;DPRA=0;EPP=5.3706;EPPR=3.25847;GTI=0;LEN=1;MEANALT=1.33333;MQM=19.5217;MQMR=60;NS=3;NUMALT=1;ODDS=5.01857;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=892;QR=5067;RO=140;RPL=4;RPP=24.253;RPPR=8.03571;RPR=19;RUN=1;SAF=13;SAP=3.86001;SAR=10;SRF=87;SRP=20.9405;SRR=53;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:58:51,7:51:1857:7:279:0,-7.42044,-157.315   0/1:51:38,13:38:1395:13:500:-6.84574,0,-110.465 0/0:55:51,3:51:1815:3:113:0,-11.0245,-158.225\nchr20   29400112    .   T   G   34.3868 PASS    AB=0.321429;ABP=18.5208;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=183;DPB=183;DPRA=0;EPP=7.64277;EPPR=3.13803;GTI=0;LEN=1;MEANALT=1;MQM=19.7667;MQMR=60;NS=3;NUMALT=1;ODDS=7.9175;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1120;QR=5652;RO=153;RPL=10;RPP=10.2485;RPPR=9.26925;RPR=20;RUN=1;SAF=15;SAP=3.0103;SAR=15;SRF=88;SRP=10.5182;SRR=65;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:68:60,8:60:2193:8:313:0,-6.91275,-184.017   0/1:56:38,18:38:1412:18:662:-11.0713,0,-110.481 0/0:59:55,4:55:2047:4:145:0,-9.70284,-176.396\nchr20   29400118    .   G   T   0   lowQual AB=0;ABP=0;AC=0;AF=0;AN=6;AO=8;CIGAR=1X;DP=182;DPB=182;DPRA=0.888889;EPP=7.35324;EPPR=3.20998;GTI=0;LEN=1;MEANALT=1;MQM=12.375;MQMR=55.2529;NS=3;NUMALT=1;ODDS=44.9794;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=284;QR=6411;RO=174;RPL=4;RPP=3.0103;RPPR=10.1986;RPR=4;RUN=1;SAF=4;SAP=3.0103;SAR=4;SRF=98;SRP=9.05049;SRR=76;TYPE=snp GT:DP:AD:RO:QR:AO:QA:GL 0/0:67:67,0:67:2443:0:0:0,-20.169,-205.631  0/0:56:48,8:48:1777:8:284:0,-8.15254,-136.193   0/0:59:59,0:59:2191:0:0:0,-17.7608,-192.037\nchr20   29400128    .   G   C   29.2536 lowQual AB=0.327273;ABP=17.2631;AC=1;AF=0.166667;AN=6;AO=31;CIGAR=1X;DP=188;DPB=188;DPRA=0;EPP=3.64073;EPPR=3.02413;GTI=0;LEN=1;MEANALT=1;MQM=19.6129;MQMR=60;NS=3;NUMALT=1;ODDS=6.7347;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1152;QR=5682;RO=157;RPL=12;RPP=6.44263;RPPR=3.35608;RPR=19;RUN=1;SAF=16;SAP=3.08035;SAR=15;SRF=89;SRP=9.10979;SRR=68;TYPE=snp GT:DP:AD:RO:QR:AO:QA:GL 0/0:69:61,8:61:2272:8:311:0,-7.28508,-191.203   0/1:55:37,18:37:1351:18:657:-10.3117,0,-105.309 0/0:64:59,5:59:2059:5:184:0,-9.38522,-175.625\nchr20   29400197    .   ACA AA,AAA  1.73244e-12 lowQual AB=0,0;ABP=0,0;AC=0,0;AF=0,0;AN=6;AO=3,21;CIGAR=1M1D1M,1M1X1M;DP=203;DPB=202;DPRA=0.819444,0;EPP=3.73412,3.1137;EPPR=7.89001;GTI=0;LEN=1,1;MEANALT=3,1.66667;MQM=11,20.6667;MQMR=59.9944;NS=3;NUMALT=2;ODDS=28.5744;PAIRED=1,1;PAIREDR=1;PAO=0,0;PQA=0,0;PQR=0;PRO=0;QA=113,404;QR=6581;RO=178;RPL=2,12;RPP=3.73412,3.94093;RPPR=5.40136;RPR=1,9;RUN=1,1;SAF=1,10;SAP=3.73412,3.1137;SAR=2,11;SRF=95;SRP=4.767;SRR=83;TYPE=del,snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:81:74,0,7:74:2713:0,7:0,140:0,-22.2762,-244.149,-12.8174,-234.887,-232.78   0/0:59:45,3,10:45:1672:3,10:113,186:0,-11.4932,-147.573,-2.00512,-137,-136.038  0/0:63:59,0,4:59:2196:0,4:0,78:0,-17.7608,-197.672,-12.8248,-192.915,-191.711\nchr20   29400216    .   C   T   0   lowQual AB=0;ABP=0;AC=0;AF=0;AN=6;AO=17;CIGAR=1X;DP=210;DPB=210;DPRA=0;EPP=4.1599;EPPR=10.6557;GTI=0;LEN=1;MEANALT=1.33333;MQM=23;MQMR=58.5312;NS=3;NUMALT=1;ODDS=41.9206;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=660;QR=6759;RO=192;RPL=16;RPP=31.7504;RPPR=5.22701;RPR=1;RUN=1;SAF=8;SAP=3.13803;SAR=9;SRF=100;SRP=3.73412;SRR=92;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:82:75,7:75:2606:7:268:0,-12.6769,-222.71    0/0:60:52,7:52:1850:7:278:0,-3.96679,-141.355   0/0:68:65,3:65:2303:3:114:0,-14.4185,-198.615\nchr20   29400224    .   T   C   7.66339e-14 lowQual AB=0;ABP=0;AC=0;AF=0;AN=6;AO=21;CIGAR=1X;DP=211;DPB=211;DPRA=0;EPP=5.59539;EPPR=7.15793;GTI=0;LEN=1;MEANALT=1.33333;MQM=17.9524;MQMR=59.9947;NS=3;NUMALT=1;ODDS=33.4304;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=801;QR=7026;RO=189;RPL=20;RPP=40.3389;RPPR=5.59539;RPR=1;RUN=1;SAF=9;SAP=3.94093;SAR=12;SRF=100;SRP=4.4005;SRR=89;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 0/0:85:78,6:78:2851:6:235:0,-16.3878,-247.681   0/0:58:47,11:47:1736:11:422:0,-0.986367,-140.004    0/0:68:64,4:64:2439:4:144:0,-13.6618,-212.891\nchr20   29400270    .   C   A   8.63075e-14 lowQual AB=0;ABP=0;AC=0;AF=0;AN=6;AO=17;CIGAR=1X;DP=208;DPB=208;DPRA=0;EPP=4.1599;EPPR=3.29452;GTI=0;LEN=1;MEANALT=1;MQM=28.8824;MQMR=58.911;NS=3;NUMALT=1;ODDS=34.4249;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=572;QR=6859;RO=191;RPL=15;RPP=24.5973;RPPR=3.02167;RPR=2;RUN=1;SAF=8;SAP=3.13803;SAR=9;SRF=95;SRP=3.02167;SRR=96;TYPE=snp GT:DP:AD:RO:QR:AO:QA:GL 0/0:82:77,5:77:2767:5:189:0,-16.9701,-241.484   0/0:58:50,8:50:1797:8:254:0,-1.8978,-135.05 0/0:68:64,4:64:2295:4:129:0,-12.4351,-198.71\n\n\nWhat if we wanted to include/exclude entire predefined regions? Let‚Äôs get set of target regions together and see how we could do it. For that we‚Äôll come back to bedtools!\nWe‚Äôre going to first create a set of target regions from our coverage map BED file. That file looks like this, with column 4 as the median coverage per 1kb window:\n\nzcat variants/results/04_alignQC/coverage/coverage_1kb.bed.gz | head\n\nchr1    0   1000    0   0   1000\nchr1    1000    2000    0   0   1000\nchr1    2000    3000    0   0   1000\nchr1    3000    4000    0   0   1000\nchr1    4000    5000    0   0   1000\nchr1    5000    6000    0   0   1000\nchr1    6000    7000    0   0   1000\nchr1    7000    8000    0   0   1000\nchr1    8000    9000    0   0   1000\nchr1    9000    10000   0   0   1000\n\n\nTo get a set of target windows, we will first select the windows we want, then merge them together.\n\nCOV=variants/results/04_alignQC/coverage/coverage_1kb.bed.gz\nTARGETS=variants/results/04_alignQC/coverage/targets.bed.gz\n\nzcat ${COV} |\nawk '$4 &gt; 50 && $4 &lt; 250' |\nbedtools merge -i stdin | \ngzip &gt;${TARGETS}\n\nNow we can intersect that targets file with the VCF file with bedtools intersect. We can even do this in a pipe with our previous filter:\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\nTARGETS=variants/results/04_alignQC/coverage/targets.bed.gz\n\nbcftools filter \\\n  -r chr20:29400000-34400000 \\\n  --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | AB &lt; .25 | AB &gt; 0.75 | QUAL &lt; 30 | AF &lt; 0.1' \\\n  ${VCFIN} |\nbedtools intersect -header -a stdin -b ${TARGETS} |\nbcftools view -H |\nhead\n\n***** WARNING: File variants/results/04_alignQC/coverage/targets.bed.gz has inconsistent naming convention for record:\nKMT2C_chr13_11526357_11569988   1000    5000\n\nchr20   29400112    .   T   G   34.3868 PASS    AB=0.321429;ABP=18.5208;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=183;DPB=183;DPRA=0;EPP=7.64277;EPPR=3.13803;GTI=0;LEN=1;MEANALT=1;MQM=19.7667;MQMR=60;NS=3;NUMALT=1;ODDS=7.9175;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1120;QR=5652;RO=153;RPL=10;RPP=10.2485;RPPR=9.26925;RPR=20;RUN=1;SAF=15;SAP=3.0103;SAR=15;SRF=88;SRP=10.5182;SRR=65;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:68:60,8:60:2193:8:313:0,-6.91275,-184.017   0/1:56:38,18:38:1412:18:662:-11.0713,0,-110.481 0/0:59:55,4:55:2047:4:145:0,-9.70284,-176.396\nchr20   29400296    .   TAAAAAAAAAATAGATG   TAAAAAAAAATAGATG    3292.91 PASS    AB=0.447761;ABP=6.18648;AC=4;AF=0.666667;AN=6;AO=116;CIGAR=1M1D15M;DP=194;DPB=183.824;DPRA=0;EPP=4.20835;EPPR=3.36563;GTI=0;LEN=1;MEANALT=4.66667;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=50.0691;PAIRED=1;PAIREDR=1;PAO=1.66667;PQA=62.3333;PQR=62.3333;PRO=1.66667;QA=4137;QR=2039;RO=55;RPL=57;RPP=3.08518;RPPR=3.99733;RPR=59;RUN=1;SAF=57;SAP=3.08518;SAR=59;SRF=23;SRP=6.20829;SRR=32;TYPE=del  GT:DP:AD:RO:QR:AO:QA:GL 0/1:74:35,32:35:1286:32:1097:-78.6681,0,-95.6981    0/1:60:20,28:20:753:28:1026:-77.9681,0,-53.4091 1/1:60:0,56:0:0:56:2014:-181.187,-16.8577,0\nchr20   29400592    .   T   C   5871.83 PASS    AB=0.509091;ABP=3.04978;AC=5;AF=0.833333;AN=6;AO=191;CIGAR=1X;DP=219;DPB=219;DPRA=0;EPP=3.02167;EPPR=5.02092;GTI=0;LEN=1;MEANALT=1.33333;MQM=54.9581;MQMR=60;NS=3;NUMALT=1;ODDS=79.1104;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=7178;QR=958;RO=27;RPL=100;RPP=3.93119;RPPR=6.95112;RPR=91;RUN=1;SAF=93;SAP=3.29452;SAR=98;SRF=16;SRP=5.02092;SRR=11;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 1/1:76:0,76:0:0:76:2867:-244.837,-22.8783,0 0/1:55:27,28:27:958:28:1047:-75.7836,0,-69.9451 1/1:88:0,87:0:0:87:3264:-274.861,-26.1896,0\nchr20   29400683    .   A   G   1256.23 PASS    AB=0.4375;ABP=6.81038;AC=2;AF=0.333333;AN=6;AO=49;CIGAR=1X;DP=190;DPB=190;DPRA=0.717949;EPP=6.59988;EPPR=4.25774;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=56.5106;NS=3;NUMALT=1;ODDS=74.9432;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1873;QR=5266;RO=141;RPL=23;RPP=3.40914;RPPR=19.7815;RPR=26;RUN=1;SAF=30;SAP=8.37251;SAR=19;SRF=73;SRP=3.39531;SRR=68;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 0/1:63:34,29:34:1277:29:1132:-83.2092,0,-90.1919    0/1:49:29,20:29:1108:20:741:-52.253,0,-84.908   0/0:78:78,0:78:2881:0:0:0,-23.4803,-248.851\nchr20   29402005    .   G   A   2144.97 PASS    AB=0.42268;ABP=8.04722;AC=4;AF=0.666667;AN=6;AO=81;CIGAR=1X;DP=137;DPB=137;DPRA=0;EPP=3.03711;EPPR=3.0103;GTI=0;LEN=1;MEANALT=1;MQM=58.4444;MQMR=58.8929;NS=3;NUMALT=1;ODDS=28.5044;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2961;QR=1719;RO=56;RPL=71;RPP=102.764;RPPR=3.16541;RPR=10;RUN=1;SAF=34;SAP=7.5409;SAR=47;SRF=17;SRP=21.778;SRR=39;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 0/1:55:31,24:31:970:24:846:-59.8663,0,-68.693   0/1:42:25,17:25:749:17:564:-36.2808,0,-55.0179  1/1:40:0,40:0:0:40:1551:-138.712,-12.0412,0\nchr20   29402036    .   TATAGATATATGTACA    TA  1047.45 PASS    AB=0.655172;ABP=21.2057;AC=3;AF=0.5;AN=6;AO=57;CIGAR=1M14D1M;DP=87;DPB=37.8125;DPRA=0;EPP=3.0484;EPPR=22.5536;GTI=1;LEN=14;MEANALT=4;MQM=58.9123;MQMR=60;NS=3;NUMALT=1;ODDS=10.0412;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=19;PRO=0.75;QA=1557;QR=598;RO=16;RPL=37;RPP=14.02;RPPR=37.7539;RPR=20;RUN=1;SAF=14;SAP=35.049;SAR=43;SRF=2;SRP=22.5536;SRR=14;TYPE=del   GT:DP:AD:RO:QR:AO:QA:GL 0/1:40:7,28:7:253:28:724:-54.2057,0,-12.2925    0/1:28:2,18:2:80:18:528:-41.5086,0,-1.2021  0/1:19:7,11:7:265:11:305:-22.1279,0,-18.6084\nchr20   29402158    .   TATACATATATATG  TATATATATG,TATACATATATATATG 2705.35 PASS    AB=0.355263,0.552632;ABP=16.8392,4.83891;AC=4,2;AF=0.666667,0.333333;AN=6;AO=59,42;CIGAR=1M4D9M,5M2I9M;DP=109;DPB=99.4286;DPRA=0,1.15152;EPP=5.99147,16.2459;EPPR=0;GTI=0;LEN=4,2;MEANALT=3.66667,4.5;MQM=59.0339,60;MQMR=0;NS=3;NUMALT=2;ODDS=22.3067;PAIRED=1,1;PAIREDR=0;PAO=0.25,0.583333;PQA=9.25,16.25;PQR=16.25;PRO=0.583333;QA=1990,1499;QR=0;RO=0;RPL=8,12;RPP=71.062,19.7617;RPPR=0;RPR=51,30;RUN=1,1;SAF=30,17;SAP=3.0471,6.31921;SAR=29,25;SRF=0;SRP=0;SRR=0;TYPE=del,ins   GT:DP:AD:RO:QR:AO:QA:GL 1/2:45:0,17,26:0:0:17,26:582,932:-123.162,-76.1295,-71.012,-47.1508,0,-39.324   1/2:31:0,10,16:0:0:10,16:267,567:-67.2189,-46.2356,-43.2253,-20.9987,0,-16.1822 1/1:33:0,32,0:0:0:32,0:1141,0:-102.451,-9.63296,0,-102.451,-9.63296,-102.451\nchr20   29402228    .   T   C   1602.72 PASS    AB=0.596154;ABP=11.3621;AC=2;AF=0.333333;AN=6;AO=62;CIGAR=1X;DP=159;DPB=159;DPRA=0.945455;EPP=4.27115;EPPR=3.21178;GTI=0;LEN=1;MEANALT=1;MQM=59.2419;MQMR=59.6907;NS=3;NUMALT=1;ODDS=41.0717;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2257;QR=3660;RO=97;RPL=19;RPP=23.184;RPPR=8.04722;RPR=43;RUN=1;SAF=33;SAP=3.57068;SAR=29;SRF=53;SRP=4.82359;SRR=44;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/1:61:26,35:26:974:35:1258:-93.6737,0,-69.6021 0/1:43:16,27:16:592:27:999:-77.2721,0,-40.6591  0/0:55:55,0:55:2094:0:0:0,-16.5566,-188.589\nchr20   29402415    .   A   G   1614.76 PASS    AB=0.529915;ABP=3.91972;AC=2;AF=0.333333;AN=6;AO=62;CIGAR=1X;DP=188;DPB=188;DPRA=0.823944;EPP=4.27115;EPPR=3.0103;GTI=0;LEN=1;MEANALT=1;MQM=58.8548;MQMR=59.619;NS=3;NUMALT=1;ODDS=60.5049;PAIRED=1;PAIREDR=0.984127;PAO=0;PQA=0;PQR=0;PRO=0;QA=2300;QR=4667;RO=126;RPL=34;RPP=4.27115;RPPR=7.42218;RPR=28;RUN=1;SAF=30;SAP=3.15039;SAR=32;SRF=66;SRP=3.63072;SRR=60;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 0/1:68:32,36:32:1191:36:1332:-97.6922,0,-87.003 0/1:49:23,26:23:868:26:968:-72.0413,0,-63.6289  0/0:71:71,0:71:2608:0:0:0,-21.3731,-234.82\nchr20   29402494    .   A   G   1900.48 PASS    AB=0.595041;ABP=12.5038;AC=2;AF=0.333333;AN=6;AO=72;CIGAR=1X;DP=190;DPB=190;DPRA=0.876812;EPP=3.13094;EPPR=6.61715;GTI=0;LEN=1;MEANALT=1;MQM=59.6389;MQMR=59.8644;NS=3;NUMALT=1;ODDS=52.7625;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=2614;QR=4501;RO=118;RPL=32;RPP=4.9405;RPPR=3.0103;RPR=40;RUN=1;SAF=37;SAP=3.13094;SAR=35;SRF=61;SRP=3.30474;SRR=57;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/1:66:24,42:24:867:42:1521:-117.279,0,-58.4526 0/1:55:25,30:25:972:30:1093:-81.4681,0,-71.1272 0/0:69:69,0:69:2662:0:0:0,-20.7711,-239.747\n\n\nOne last common filtering need: we could cut down our data to look only at biallelic snps with bcftools view  -m2 -M2 -v snps.\n\n\n5.3.2 Filtering with vcftools\nWe won‚Äôt do it here, but the package vcftools also has lots of tools for filtering variants. Usefully, it can filter by features that are not in the INFO or FORMAT fields, such as the rate of missing data, the number of alleles, the allele frequency, the number of observations of the alternate allele and others. It can also be used to calculate a variety of statistics. Between these two packages, most typical filtering criteria are available. If you need something more complex, you may need specialized software or to write your own filtering script.\n\n\n5.3.3 Filtering our three VCF files\nSo that we have a consistent set of variants to consider next, let‚Äôs run the filtering script scripts/06_filteringAnnotating/01_filterVariants.sh\nIt applies the following filters:\n# freebayes: use AB/AF/DP/QUAL\nbcftools filter \\\n  -r chr20:29400000-34400000 \\\n  --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | AB &lt; .25 | AB &gt; 0.75 | QUAL &lt; 30 | AF &lt; 0.1' \\\n  ${FREEBAYES} |\nbedtools intersect -header -a stdin -b ${TARGETS} |\nbgzip &gt;${FREEBAYESOUT}\n\ntabix -p vcf ${FREEBAYESOUT}\n\n# gatk: use DP/QUAL (others not available)\nbcftools filter \\\n  -r chr20:29400000-34400000 \\\n  --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | QUAL &lt; 30' \\\n  ${GATK} |\nbedtools intersect -header -a stdin -b ${TARGETS} |\nbgzip &gt;${GATKOUT}\n\ntabix -p vcf ${GATKOUT}\n\n# bcftools: use DP/QUAL (others not available)\nbcftools filter \\\n  -r chr20:29400000-34400000 \\\n  --exclude 'INFO/DP &lt; 50 | INFO/DP &gt; 250 | QUAL &lt; 30' \\\n  ${BCFTOOLS} |\nbedtools intersect -header -a stdin -b ${TARGETS} |\nbgzip &gt;${BCFTOOLSOUT}\n\ntabix -p vcf ${BCFTOOLSOUT}",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html#assessing-variant-call-set-quality",
    "href": "02_3_filteringVariants.html#assessing-variant-call-set-quality",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "5.4 Assessing variant call set quality",
    "text": "5.4 Assessing variant call set quality\nThere are a number of ways we can go about this. We‚Äôll look at the numbers and types of variants, the transition/transversion ratio, and because we have a known pedigree for our samples, we can count up the rate of violations of Mendelian inheritance (i.e.¬†the number of sites where offspring genotypes are inconsistent with their parents, like Father=AA Mother=BB, Child=AA).\n\n5.4.1 Total numbers of variants:\nLet‚Äôs do bcftools stats for both pre- and post-filtered VCF files (focusing on just our region). First pre-filter:\n\nPRE=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\n\nbcftools stats -r chr20:29400000-34400000 $PRE | grep ^SN\n\nSN  0   number of samples:  3\nSN  0   number of records:  57423\nSN  0   number of no-ALTs:  0\nSN  0   number of SNPs: 52753\nSN  0   number of MNPs: 3122\nSN  0   number of indels:   2743\nSN  0   number of others:   464\nSN  0   number of multiallelic sites:   3325\nSN  0   number of multiallelic SNP sites:   1349\n\n\nNow post-filter:\n\nPOST=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\n\nbcftools stats -r chr20:29400000-34400000 $POST | grep ^SN\n\nSN  0   number of samples:  3\nSN  0   number of records:  7718\nSN  0   number of no-ALTs:  0\nSN  0   number of SNPs: 6401\nSN  0   number of MNPs: 191\nSN  0   number of indels:   1150\nSN  0   number of others:   89\nSN  0   number of multiallelic sites:   262\nSN  0   number of multiallelic SNP sites:   2\n\n\nWe have dramatically cut down our number of sites. Part of this is that we also exclude a bit of our initial 5mb with the targets file (it‚Äôs now 4226000, per the targets file), but we have also removed tons of false positives. So our Watterson‚Äôs theta is down near 0.001 where it should be (for humans), versus 0.007. Again, this is a very rough way of calculating this value as a sanity check, not how you should approach it if you needed to know the real number to report in a publication.\n\n\n5.4.2 ts/tv\nNext we can look at our ts/tv ratio. First pre-filter:\n\nPRE=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\n\nbcftools stats -r chr20:29400000-34400000 $PRE | grep TS\n\n# TSTV, transitions/transversions\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\nTSTV    0   20940   31363   0.67    20348   29973   0.68\n\n\nThen post-filter:\n\nPOST=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\n\nbcftools stats -r chr20:29400000-34400000 $POST | grep TS\n\n# TSTV, transitions/transversions\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\nTSTV    0   4198    2057    2.04    4189    2050    2.04\n\n\nWe see a pretty dramatic shift, from 0.67 before filtering to 2.04 after filtering. And in fact, if we limit our focus to the region away from the centromere (chr20:32400000-34400000) it will rise a bunch more.\n\nPOST=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\n\nbcftools stats -r chr20:32400000-34400000 $POST | grep TS\n\n# TSTV, transitions/transversions\n# TSTV  [2]id   [3]ts   [4]tv   [5]ts/tv    [6]ts (1st ALT) [7]tv (1st ALT) [8]ts/tv (1st ALT)\nTSTV    0   1544    594 2.60    1540    592 2.60\n\n\nRemember that this is a very rough way of evaluating variant quality and ts/tv values are different among species and regions of the genome. There‚Äôs no perfect target to hit here.\n\n\n5.4.3 Mendelian violations\nIf we have a pedigree, we can also look at rates of violation of Mendelian inheritance. bcftools has a plugin that can do this (note also that vt has a really nice module profile_mendelian that produces a nice report). The plugin +mendelian2 can also filter variants by the presence of Mendelian errors, but you should be careful with this. Even in a great dataset, you expect some rate of genotyping error and if you have a big enough pedigree, you could carelessly throw out tons of good data on account of what is really a pretty small error rate.\nFor our\n\nPRE=variants/results/05_variantCalling/freebayes/freebayes.vcf.gz\nbcftools +mendelian2 -r chr20:29400000-34400000 -p son,dad,mom -m c ${PRE}\n\n# Summary stats\nsites_ref_only  0   # sites skipped because there was no ALT allele\nsites_many_als  0   # skipped because of too many ALT alleles\nsites_fail  0   # skipped because of failed -i/-e filter\nsites_no_GT 0   # skipped because of absent FORMAT/GT field\nsites_not_diploid   0   # skipped because FORMAT/GT not formatted diploid\nsites_missing   793 # number of sites with at least one trio GT missing\nsites_merr  1300    # number of sites with at least one Mendelian error\nsites_good  55364   # number of sites with at least one good trio\n# Per-trio stats, each column corresponds to one trio. List of trios is below.\n# The meaning of per-trio stats is the same as described above, ngood_alt is\n# the number of good genotypes with at least one non-reference allele, and is\n# included in the ngood counter\nngood   55364\nngood_alt   19676\nnmerr   1300\nnmissing    793\nnfail   0\n# List of trios. Their ids are in the same order as the values listed in the stats lines above. For\n# example, the values for the first trio (id=1) and the third trio (id=3) are in the 2nd and the 4th\n# column and their stats can be obtained with the unix command\n#     cat stats.txt | grep ^n | cut -f1,2,4\n# TRIO  [2]id   [3]child    [4]father   [5]mother\nTRIO    1   son dad mom\n\n\nThis gives us a Mendelian error rate of 1300 / (1300 + 55364) = 0.02294226. Not too bad for zero filtering! In fact, it‚Äôs weirdly low! We could investigate further extracting our low-quality sites and looking at the distribution of genotypes. It could be there are lots of parent1:AB parent2:AA offspring:AA where allele B is a false positive. vt would give us a bit more detailed of a report as well.\nLet‚Äôs look at post-filter data:\n\nPOST=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nbcftools +mendelian2 -r chr20:29400000-34400000 -p son,dad,mom -m c ${POST}\n\n# Summary stats\nsites_ref_only  0   # sites skipped because there was no ALT allele\nsites_many_als  0   # skipped because of too many ALT alleles\nsites_fail  0   # skipped because of failed -i/-e filter\nsites_no_GT 0   # skipped because of absent FORMAT/GT field\nsites_not_diploid   0   # skipped because FORMAT/GT not formatted diploid\nsites_missing   1   # number of sites with at least one trio GT missing\nsites_merr  13  # number of sites with at least one Mendelian error\nsites_good  7704    # number of sites with at least one good trio\n# Per-trio stats, each column corresponds to one trio. List of trios is below.\n# The meaning of per-trio stats is the same as described above, ngood_alt is\n# the number of good genotypes with at least one non-reference allele, and is\n# included in the ngood counter\nngood   7704\nngood_alt   7704\nnmerr   13\nnmissing    1\nnfail   0\n# List of trios. Their ids are in the same order as the values listed in the stats lines above. For\n# example, the values for the first trio (id=1) and the third trio (id=3) are in the 2nd and the 4th\n# column and their stats can be obtained with the unix command\n#     cat stats.txt | grep ^n | cut -f1,2,4\n# TRIO  [2]id   [3]child    [4]father   [5]mother\nTRIO    1   son dad mom\n\n\nNow our Mendelian error rate is 0.001684592! Much better!\n\nPOST=variants/results/05_variantCalling/gatk/gatk_filtered.vcf.gz\nbcftools +mendelian2 -r chr20:29400000-34400000 -p son,dad,mom -m c ${POST}\n\n# Summary stats\nsites_ref_only  0   # sites skipped because there was no ALT allele\nsites_many_als  0   # skipped because of too many ALT alleles\nsites_fail  0   # skipped because of failed -i/-e filter\nsites_no_GT 0   # skipped because of absent FORMAT/GT field\nsites_not_diploid   0   # skipped because FORMAT/GT not formatted diploid\nsites_missing   59  # number of sites with at least one trio GT missing\nsites_merr  354 # number of sites with at least one Mendelian error\nsites_good  11457   # number of sites with at least one good trio\n# Per-trio stats, each column corresponds to one trio. List of trios is below.\n# The meaning of per-trio stats is the same as described above, ngood_alt is\n# the number of good genotypes with at least one non-reference allele, and is\n# included in the ngood counter\nngood   11457\nngood_alt   11457\nnmerr   354\nnmissing    59\nnfail   0\n# List of trios. Their ids are in the same order as the values listed in the stats lines above. For\n# example, the values for the first trio (id=1) and the third trio (id=3) are in the 2nd and the 4th\n# column and their stats can be obtained with the unix command\n#     cat stats.txt | grep ^n | cut -f1,2,4\n# TRIO  [2]id   [3]child    [4]father   [5]mother\nTRIO    1   son dad mom\n\n\n\n\n5.4.4 Other checks\nDepending on the type of study, there are other statistics you can look at. Deviations from Hardy-Weinberg equilibrium is a big one, but only useful in the context of a population sample, and you may expect deviations under certain breeding systems or sampling regimes. You can use functional annotations (do you see an unreasonable number of alleles that disrupt proteins?). You can also zoom and look at the underlying data for a region of interest in IGV. If you have a large set of ‚Äúknown‚Äù variants, you could ask whether you have discovered an unreasonably large set of new variants.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_3_filteringVariants.html#conclusion",
    "href": "02_3_filteringVariants.html#conclusion",
    "title": "5¬† Filtering and Assessing Variants",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\nIn this chapter we‚Äôve seen ways we can summarize variant call sets, filter them and check their overall quality. In the next chapter we‚Äôll cover comparing variant call sets and annotating them.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>Filtering and Assessing Variants</span>"
    ]
  },
  {
    "objectID": "02_4_assessingAnnotating.html",
    "href": "02_4_assessingAnnotating.html",
    "title": "6¬† Comparing and Annotating Variants",
    "section": "",
    "text": "6.1 Learning Objectives",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Comparing and Annotating Variants</span>"
    ]
  },
  {
    "objectID": "02_4_assessingAnnotating.html#learning-objectives",
    "href": "02_4_assessingAnnotating.html#learning-objectives",
    "title": "6¬† Comparing and Annotating Variants",
    "section": "",
    "text": "Learning Objectives:\n\n\nComparing variants.\n\n\nAnnotating variants.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Comparing and Annotating Variants</span>"
    ]
  },
  {
    "objectID": "02_4_assessingAnnotating.html#comparing-variant-call-sets",
    "href": "02_4_assessingAnnotating.html#comparing-variant-call-sets",
    "title": "6¬† Comparing and Annotating Variants",
    "section": "6.2 Comparing variant call sets",
    "text": "6.2 Comparing variant call sets\nSometimes we want to compare sets of variants. It may be because we are trying to evaluate two different approaches to calling variants on our data, we may have two different datasets and we want to see how they differ, or we may want to check our variants against an existing database containing annotations (see below the section on adding database IDs). These all have similar mechanics, and require us to match identical variants across sets. This is conceptually pretty simple, but there are some complications:\n\nVariant callers often represent identical variants in different ways. There may be even be fundamental ambiguity about how they should be represented.\nSee freebayes vs gatk for one site where representation diverges:\n# freebayes\nchr20 29402036    .   TATAGATATATGTACA    TA\n\n# gatk\nchr20 29402036    .   TATAGATATATGTAC   T \nVariant callers may output short haplotypes or single records.\nSee again, freebayes vs gatk\n# freebayes\nchr20 34025687    .   AA  TT\n\n#gatk\nchr20 34025687    .   A   T   \nchr20 34025688    .   A   T   \nMulti-allelic records need to be accounted for. Variant:\nchr20 34091776    .   T   TTTG\nIs a subset of\nchr20 34091776    .   T   TTTG,TTTGG\n\nThe examples above is are easy ones. They can be dealt with through normalization and decomposition as we will see below. Other cases are more difficult, particularly in cases where ambiguous alleles at multiple sites are emitted without phase information. Specialized software such as hap.py and RTG vcfeval have been developed to do this with the greatest rigor for benchmarking studies. See this paper for a discussion of strategies for rigorous variant call comparisons.\nWe‚Äôre going to apply some simpler approaches below to demonstrate the impacts.\n\n6.2.1 Normalization of variants\nThe first thing we‚Äôll do is normalize variants. This site has a nice explanation of the details, but in short, we want to left align ambiguous variants (push them as far left as is consistent with the data), and we want the variants to be parsimonious (we don‚Äôt want the REF and ALT alleles to have extra bases in them).\nWe can do this with bcftools norm. Let‚Äôs first have a look at some variants annotated by freebayes as ‚Äúcomplex‚Äù:\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nbcftools view -r chr20:29416672-29535934 -H -i 'INFO/TYPE=\"complex\"' ${VCFIN} | \ncut -f 1-5\n\nchr20   29416682    .   ATCCCTG GTCTG\nchr20   29422132    .   TTCTC   CTCTT\nchr20   29427951    .   CCAC    GTGG\nchr20   29447781    .   GGAGG   TGAGT\nchr20   29481154    .   GAATT   TAATC\nchr20   29501683    .   GT  AC\nchr20   29505266    .   TTTTCAC CTTTCAT\nchr20   29531934    .   CAAAATC CAAAAAAC\nchr20   29535161    .   CAT AAC\nchr20   29535924    .   TGGGAT  TGGAC\n\n\nNow what happens to them when we apply bcftools norm? Note that we need to give it the reference genome so it can see the context of the variants to be normalized.\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nGENOME=variants/genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n  \nbcftools view -r chr20:29416672-29535934 -i 'INFO/TYPE=\"complex\"'  ${VCFIN} | \n  bcftools norm -f ${GENOME} | \n  bcftools view -H - | \n  cut -f 1-5 \n\nLines   total/split/joined/realigned/removed/skipped:   10/0/0/3/0/0\nchr20   29416682    .   ATCC    GT\nchr20   29422132    .   TTCTC   CTCTT\nchr20   29427951    .   CCAC    GTGG\nchr20   29447781    .   GGAGG   TGAGT\nchr20   29481154    .   GAATT   TAATC\nchr20   29501683    .   GT  AC\nchr20   29505266    .   TTTTCAC CTTTCAT\nchr20   29531939    .   T   AA\nchr20   29535161    .   CAT AAC\nchr20   29535927    .   GAT AC\n\n\nWe can see that this set of 10 variants was already largely left-aligned, but often not parsimonious, so several were trimmed.\n\n\n6.2.2 Decomposition of haplotype variants\nHaplotype variants can sometimes be broken down into their constituent parts. This can make some kinds of analysis easier, and for the simple approach to comparing variant call sets we use below, it‚Äôs necessary for us to do, otherwise concordance will look very low. This mostly applies to freebayes, which outputs tons of small haplotypes, though we can run it later on the GATK output to see if it does anything significant.\nAfter normalization, we can pipe our VCF to vcfallelicprimitives, a part of the package vcflib. Like other packages we‚Äôve mentioned or worked with, it has many useful tools and the documentation is worth perusing.\nvcfallelicprimitives will break down our complex variants into multiple VCF records. Note that it cannot update the annotations in the INFO or FORMAT fields (aside from the genotype) so those are discarded by default. So only use this tool when you think you won‚Äôt need that information anymore.\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nGENOME=variants/genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n  \nbcftools view -r chr20:29416672-29535934 -i 'INFO/TYPE=\"complex\"'  ${VCFIN} | \n  bcftools norm -f ${GENOME} | \n  vcfallelicprimitives |\n  bcftools view -H - | \n  cut -f 1-5\nLines   total/split/realigned/skipped:  10/0/3/0\nchr20   29416682    .   A   G\nchr20   29416683    .   TCC T\nchr20   29422132    .   T   C\nchr20   29422136    .   C   T\nchr20   29427951    .   C   G\nchr20   29427952    .   C   T\nchr20   29427953    .   A   G\nchr20   29427954    .   C   G\nchr20   29447781    .   G   T\nchr20   29447785    .   G   T\nchr20   29481154    .   G   T\nchr20   29481158    .   T   C\nchr20   29501683    .   G   A\nchr20   29501684    .   T   C\nchr20   29505266    .   T   C\nchr20   29505272    .   C   T\nchr20   29531939    .   T   AA\nchr20   29535161    .   C   A\nchr20   29535163    .   T   C\nchr20   29535927    .   GA  A\nchr20   29535929    .   T   C\nHere we can see our 10 variants above decomposed into 21 constituents.\n\n\n6.2.3 Splitting multi-allelic records\nIf we want to have each alternate allele represented by a single VCF record, we can do that with bcftools norm as well. This helps with database matching, where database records represent single alternate alleles, not all the variation known from a site.\nSome multi-allelic records (some of these may be artifacts that remain after filtering):\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nGENOME=variants/genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\nbcftools view -i 'N_ALT&gt;1' -r chr20:32014243-32129138 ${VCFIN} | \n  bcftools norm -f ${GENOME} |\n  bcftools view -H |\n  cut -f 1-5 \n\nLines   total/split/joined/realigned/removed/skipped:   10/0/0/10/0/0\nchr20   32014263    .   TTT ATATATATT,ATATATTT,ATATATA,ATTT\nchr20   32025056    .   CTTCT   CTTCTTTCT,C\nchr20   32038021    .   ACG GCACACA,GCACACACACA\nchr20   32044401    .   TCACACA TCA,T,TCACACACACACA\nchr20   32058605    .   AAC A,AACACACAC\nchr20   32059293    .   T   TTCCTTCCTTCCC,TTCCTTCCT\nchr20   32118625    .   TTATTTATTTATT   TTATTTATG,T\nchr20   32121221    .   T   TT,A\nchr20   32128790    .   CTGTGTG C,CTGTGTGTGTGTGTGTG\nchr20   32129118    .   TA  TT,T\n\n\nNow to give each allele its own record and normalize we can use the argument -m -any to bcftools norm:\n\nVCFIN=variants/results/05_variantCalling/freebayes/freebayes_filtered.vcf.gz\nGENOME=variants/genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\nbcftools view -i 'N_ALT&gt;1' -r chr20:32014243-32129138 ${VCFIN} | \n  bcftools norm -m -any -f ${GENOME} |\n  bcftools view -H |\n  cut -f 1-5 \n\nLines   total/split/joined/realigned/removed/skipped:   10/10/0/23/0/0\nchr20   32014262    .   T   TATATA\nchr20   32014262    .   T   TA\nchr20   32014263    .   T   ATATATA\nchr20   32014263    .   TTT ATATATA\nchr20   32025056    .   C   CTTCT\nchr20   32025056    .   CTTCT   C\nchr20   32038021    .   ACG GCACACA\nchr20   32038021    .   ACG GCACACACACA\nchr20   32044401    .   TCACA   T\nchr20   32044401    .   TCACACA T\nchr20   32044401    .   T   TCACACA\nchr20   32058605    .   AAC A\nchr20   32058605    .   A   AACACAC\nchr20   32059279    .   T   TCTTCCTTC\nchr20   32059293    .   T   TTCCTTCCTTCCC\nchr20   32118601    .   GTATTTATTTATT   G\nchr20   32118633    .   TTATT   G\nchr20   32121209    .   C   CT\nchr20   32121221    .   T   A\nchr20   32128790    .   CTGTGTG C\nchr20   32128790    .   C   CTGTGTGTGTG\nchr20   32129118    .   TA  T\nchr20   32129119    .   A   T\n\n\n\n\n6.2.4 Comparisons\nWe‚Äôre almost ready to do some comparisons. Let‚Äôs run the script scripts/06_filteringAnnotating/02_normalizeVariants.sh to generate some files we can use to filter our variants. This script will create two more VCF files for each variant caller. One where the variants have been just normalized (*norm.vcf.gz), and one where the haplotypic variants have been broken down into their constituent parts with vcfallelicprimitives (*normAP.vcf.gz).\n\n6.2.4.1 Summaries\nTo compare VCF files, we can use a couple tools. To get a quick summary, we will use vt partition (mentioned above). vcftools produces a quick summary as well with the diff options, but this one has a nicer format. Let‚Äôs compare GATK to freebayes:\n#| warning: false\nVCF1=variants/results/05_variantCalling/freebayes/freebayes_normAP.vcf.gz\nVCF2=variants/results/05_variantCalling/gatk/gatk_normAP.vcf.gz\nvt partition ${VCF1} ${VCF2}\npartition v0.5\n\nOptions:     input VCF file a   ../../results/05_variantCalling/freebayes/freebayes_normAP.vcf.gz\n             input VCF file b   ../../results/05_variantCalling/gatk/gatk_normAP.vcf.gz\n         [w] write_partition    false\n\n    A:        8074 variants\n    B:       11867 variants\n\n                   ts/tv  ins/del\n    A-B        392 [0.68] [0.73]\n    A&B       7682 [2.03] [0.94]\n    B-A       4185 [1.32] [1.12]\n    of A     95.1%\n    of B     64.7%\n\nTime elapsed: 0.16s\nNow we notice here that GATK has far more variants than freebayes, but note that the variants unique to GATK have a much lower ts/tv ratio, suggesting something may be amiss with them. Now, we didn‚Äôt filter these the exact same way. Particularly, we used the allele balance annotation provided by freebayes, and that is likely to have flagged a decent number of problematic variants, particularly in the centromere region. We‚Äôd probably see much higher concordance in general if we restricted to the more accessible region &gt;32mb.\nNote also that some of the tools mentioned above hap.py and vcfeval would do a more rigorous job with this.\n\n\n6.2.4.2 Extracting sets of variants\nvt gave us a summary. If we want to extract sets of variants that are shared or not, we can use bcftools isec.\n\nVCF1=variants/results/05_variantCalling/freebayes/freebayes_normAP.vcf.gz\nVCF2=variants/results/05_variantCalling/gatk/gatk_normAP.vcf.gz\nOUT=variants/results/05_variantCalling/freebayes/fb_gatk_vars\nbcftools isec -p ${OUT} ${VCF1} ${VCF2}\n\nls ${OUT}\n\n0000.vcf\n0001.vcf\n0002.vcf\n0003.vcf\nREADME.txt\nsites.txt\n\n\nThis produces 4 VCF files. Variants unique to VCF1, unique to VCF2, shared records from VCF1 and shared records from VCF2. The README explains.\nYou can use these to investigate discrepancies if you‚Äôre interested in that kind of thing.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Comparing and Annotating Variants</span>"
    ]
  },
  {
    "objectID": "02_4_assessingAnnotating.html#annotating-variants",
    "href": "02_4_assessingAnnotating.html#annotating-variants",
    "title": "6¬† Comparing and Annotating Variants",
    "section": "6.3 Annotating variants",
    "text": "6.3 Annotating variants\nIn this section we‚Äôre going to cover annotating variants. There are lots of ways to annotate variants. We‚Äôll cover 2 here. The first is very closely related to our variant comparisons above. The idea is that given some database of known variants, perhaps with literature references associated, you want to see if any of your variants can be found in the database.\nIn the second, you have some annotation of your genome, usually containing genes at the least, and you want to know how variants you‚Äôve discovered might impact the function of your annotated sequences. There are lots of complex models for looking at this, but we‚Äôll stick to asking basic questions like ‚Äúis the variant in a coding region?‚Äù ‚Äúis the variant a missense or nonsense mutation?‚Äù\n\n6.3.1 Adding database IDs\nAs an example, we‚Äôll use dbSNP here. dbSNP is a database of short variants that each of unique IDs (e.g.¬†rsXXXXXX). You can look up information about these variants, if there is any published and linked to the database record.\nTo link our variants to dbSNP records, we are going to download a VCF file of dbSNP variants with the ID column populated with rsIDs and transfer the rsIDs over to our VCF using bcftools. It‚Äôs pretty straightforward. Remember that to do this, we will need to have normalized our variants. It‚Äôs a good idea to also break down multi-allelic records, but we‚Äôre going to skip that step here for convenience as we don‚Äôt have that many anyway.\nHave a look at the script scripts/06_filteringAnnotating/03_dbSNP.sh.\nTo download the dbSNP records we want, we have to do a little bit of fiddling around with the files:\n# get the dbsnp set for chromosome 20\n    # see here for details\n        # https://www.ncbi.nlm.nih.gov/variation/docs/human_variation_vcf/\n\n    # download only a section of chr20 from dbsnp. change `20` to `chr20` in sequence ID column\n    tabix -h ftp://ftp.ncbi.nlm.nih.gov/snp/organisms/human_9606/VCF/00-All.vcf.gz 20:28000000-35000000 | \\\n    sed 's/^20/chr20/' | \\\n    bgzip -c &gt;${OUTDIR}/chr20.dbsnp.vcf.gz\n    tabix -p vcf -f ${OUTDIR}/chr20.dbsnp.vcf.gz\n\n    # update the sequence dictionary\n    gatk UpdateVCFSequenceDictionary -V ${OUTDIR}/chr20.dbsnp.vcf.gz --source-dictionary ${INDIR}/freebayes_normAP.vcf.gz --output ${OUTDIR}/chr20.dbsnp.contig.vcf.gz --replace=true\n    tabix -p vcf -f ${OUTDIR}/chr20.dbsnp.contig.vcf.gz\n\n    # remove intermediate files\n    rm ${OUTDIR}/chr20.dbsnp.vcf.gz*\nFirst note that if you visit the link in the comment, the dbsSNP records are on GRCh38 (even different patches of this genome version have the same coordinate system, so it is compatible with the genome we‚Äôre using, at least for the chromosomes themselves).\nNext, dbSNP has chromosome names like 20 instead of chr20, so we need to fix them so they‚Äôll match up. This is a perennial problem with human genomes. Different copies of GRCh38 may have chromosomes labeled with 20, chr20, Genbank IDs, or RefSeq IDs.\nNow that we‚Äôve altered the VCF records, we next need to update the sequence dictionary in the header. This could be a problem for unassigned scaffolds that might differ between our GRCh38 and the one dbSNP is using if we were doing the whole genome (for the most part, you probably don‚Äôt need rsIDs for all variants in an entire WGS study).\nFinally, remove intermediate files.\nNow we‚Äôve got our file, we can use bcftools annotate to transfer the rsIDs:\nbcftools annotate -c ID \\\n--output-type z \\\n-a ${OUTDIR}/chr20.dbsnp.contig.vcf.gz \\\n${INDIR}/freebayes_normAP.vcf.gz &gt;${OUTDIR}/freebayes_normAP.RSID.vcf.gz\nTo see the results:\nbcftools view -H ${OUTDIR}/freebayes_normAP.RSID.vcf.gz | cut -f 1-5 | head\nYou can see the ID column (3) is populated with rsIDs.\nchr20   29400112        rs1403688300    T       G\nchr20   29400296        rs1408580948    TA      T\nchr20   29400592        rs1379819498    T       C\nchr20   29400683        rs1213763564    A       G\nchr20   29402005        rs1248633832    G       A\nchr20   29402036        rs1353591646    TATAGATATATGTAC T\nchr20   29402158        rs1385687340    TATAC   T\nchr20   29402162        rs1476534343    C       CAT\nchr20   29402228        rs1459724709    T       C\nchr20   29402415        rs1172173923    A       G\nAnd for the first one, you can see there is an entry in dbSNP.\n\n\n6.3.2 Variant effect annotations\nFinally, we‚Äôre going to cover annotating variants with functional impacts. There are a number of tools available for this, including ANNOVAR, VEP from ENSEMBL, snpEff and good old bcftools.\nThese tools classify variants by how they impact annotated sequences. Do they change an amino acid sequence? Are they located in an annotated sequence? At least ANNOVAR can also add database annotations (i.e.¬†dbSNP). They don‚Äôt make predictions about the functional impact from the level of conservation or structural or biochemical properties of the resulting sequence alterations. For that there are other approaches, such as PolyPhen2 and SIFT. For human data, it‚Äôs possible to grab pre-existing predictions from dbNSFP and integrate them using some of the tools above (like snpEff/snpSift).\nBelow we‚Äôll demonstrate bcftools and snpEff.\n\n6.3.2.1 bcftools csq\nFor this method, we need an annotation in GFF3 format and our VCF file. Per the documentation it will only accept ENSEMBL formatted GFF3.\nCheck out the script here scripts/06_filteringAnnotating/04_bcftoolsCSQ.sh.\nJust like with the dbSNP file, we‚Äôre going to grab it and update the chromosome names:\nwget -P ${OUTDIR} https://ftp.ensembl.org/pub/release-113/gff3/homo_sapiens/Homo_sapiens.GRCh38.113.chromosome.20.gff3.gz\ngunzip ${OUTDIR}/Homo_sapiens.GRCh38.113.chromosome.20.gff3.gz\n\n# fix up chromosome 20 names\nsed -i 's/^20/chr20/' ${OUTDIR}/Homo_sapiens.GRCh38.113.chromosome.20.gff3\nThen we can run it pretty simply:\nGFF=${OUTDIR}/Homo_sapiens.GRCh38.113.chromosome.20.gff3\n\n# run bcftools csq\nbcftools csq --phase a -f ${GENOME} -g ${GFF} ${VCFIN} -Oz -o ${VCFOUT}\nThis tool wants to predict variants while accounting for other nearby variants. For that it needs to know the phase. We don‚Äôt have phase information, so we use --phase a to tell it to pretend the data are phased. Not ideal, but otherwise it will error out.\nThis will add annotations to the INFO field. An example:\nBCSQ=missense|REM1|ENST00000201979|protein_coding|+|28H&gt;28R|31476528A&gt;G\nThe information here is |-separated. A single variant can have multiple annotations (annotations are transcript-centered). We can list the fields in the annotation with:\n bcftools +split-vep -l $VCFOUT\n0       Consequence\n1       gene\n2       transcript\n3       biotype\n4       strand\n5       amino_acid_change\n6       dna_change\nThey are fairly self-explanatory. To extract just these annotations, we can use format strings similar to bcftools query\nbcftools +split-vep -s worst -f '%CHROM\\t%POS\\t%Consequence\\t%gene\\t%amino_acid_change' $VCFOUT | head\nchr20   29410595        non_coding      DUX4L34 .\nchr20   29410654        non_coding      DUX4L34 .\nchr20   29410720        non_coding      DUX4L34 .\nchr20   29415522        non_coding      FRG2EP  .\nchr20   29415878        non_coding      FRG2EP  .\nchr20   29416287        non_coding      FRG2EP  .\nchr20   29416682        non_coding      FRG2EP  .\nchr20   29416706        non_coding      FRG2EP  .\nchr20   29416719        non_coding      FRG2EP  .\nchr20   29416809        non_coding      FRG2EP  .\nIf we wanted to summarize the consequence types we could do:\nbcftools +split-vep -s worst -f '%CHROM\\t%POS\\t%Consequence\\t%gene\\t%amino_acid_change' $VCFOUT | cut -f 3 | sort | uniq -c | sort -g\nWarning: fewer BCSQ fields than expected at chr20:29410595, filling with dots. This warning is printed only once.\n      1 inframe_insertion\n      1 missense&nmd_transcript\n      1 stop_gained\n      1 synonymous&nmd_transcript\n      3 splice_region\n      4 5_prime_utr&nmd_transcript\n      4 splice_region&nmd_transcript\n      9 3_prime_utr&nmd_transcript\n     17 5_prime_utr\n     22 synonymous\n     26 missense\n     61 3_prime_utr\n    324 non_coding\n   2352 intron\n\n\n6.3.2.2 snpEff\nsnpEff works similarly to bcftools here, but gives a little more detail, and a nice summary file. It also requires you to create a database of your annotation beforehand and is reasonably tolerant of different annotation files. This makes it a nice choice for non-model systems. The documentation explains how to create a database, but we are going to use a pre-generated one for GRCh38.\nCheck out the script here: scripts/06_filteringAnnotating/05_snpEff.sh.\nAside from the usual accounting for directories, etc, the main call is this:\njava -Xmx8G -jar ${SNPEFF} eff -dataDir $(pwd)/${OUTDIR}/snpeff_data hg38 $VCF | bgzip -c &gt;${VCFANN}\nWe‚Äôre telling it to use hg38, which has the same coordinate system as GRCh38 (and uses chr20 style names).\nLike bcftools, it adds a new tag to the INFO field ANN. There is a pretty long document explaining it and a bit of a shorter explanation in the snpEff docs.\nHere‚Äôs an example:\nANN=T|missense_variant|MODERATE|PXMP4|PXMP4|transcript|NM_007238.4|protein_coding|4/4|c.610G&gt;A|p.Val204Ile|733/5724|610/639|204/212||\nCheck the docs above for an explanation. A big advantage over bcftools here is that snpEff also outputs an html formatted report, and a summary text file giving results by gene. Querying and summarizing a big VCF file yourself can be a big lift.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>Comparing and Annotating Variants</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html",
    "href": "03_1_genomeAssemblyKmers.html",
    "title": "7¬† Genome Assembly - first steps",
    "section": "",
    "text": "7.1 Learning Objectives\nIn this chapter we‚Äôre going to focus on initial characterization of the data we‚Äôll use to assemble a genome.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#learning-objectives",
    "href": "03_1_genomeAssemblyKmers.html#learning-objectives",
    "title": "7¬† Genome Assembly - first steps",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw data specific to genome assembly.\n\n\nEstimate genome size from unassembled reads.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#introduction",
    "href": "03_1_genomeAssemblyKmers.html#introduction",
    "title": "7¬† Genome Assembly - first steps",
    "section": "7.2 Introduction",
    "text": "7.2 Introduction\nThe field of genome assembly is rapidly changing. The first draft human genome, completed in 2003 (but far from a complete or correct genome!) took 13 years to sequence and assemble and cost $2.7 billion.\nSoftware and data used in genome assembly have advanced rapidly ever since. So all this is to say, we‚Äôll demonstrate the general workflow and concepts here, but you can expect improvements in data, software and reductions in cost to make any specific recommendations obsolete pretty quickly!",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#code",
    "href": "03_1_genomeAssemblyKmers.html#code",
    "title": "7¬† Genome Assembly - first steps",
    "section": "7.3 Code",
    "text": "7.3 Code\nWe‚Äôre going to work through scripts that can be found in this GitHub repository. As usual, they are all written for the Xanadu cluster at UConn, and so you should be able to clone the repository and run the scripts without any modification.\n\n\n\n\n\n\nWarning\n\n\n\nBe aware this project is going to balloon up to around 700GB (though the final products are just a couple GB), so you may want to create a directory in /scratch to work in rather than your home.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#data",
    "href": "03_1_genomeAssemblyKmers.html#data",
    "title": "7¬† Genome Assembly - first steps",
    "section": "7.4 Data",
    "text": "7.4 Data\nIn these chapters, we‚Äôll use publicly available data from the Vertebrate Genomes Project. Specifically, the data we‚Äôll grab for this chapter is for banded killifish, Fundulus diaphanus an inhabitant mostly of freshwaters in northeast North America.\nLinks to the raw data can be found at GenomeArk and VGP‚Äôs final assembly has been deposited in NCBI. Note there are two genome assembly products there. That‚Äôs because VGP did a phased diploid assembly, which we‚Äôll explain more about later.\nIf you visit the GenomeArk page you‚Äôll see three data types:\n\nPacBio HiFi data: There are a few products here. We‚Äôre going to grab the fastq files, which are the actual raw HiFi reads we want to use for assembly. HiFi data is one of the most widely used data types in primary assembly at the moment.\nHi-C data: This is chromatin conformation capture data that is generally used to understand the physical organization of the genome. We‚Äôre going to use it to scaffold our contigs after primary assembly. This is paired-end Illumina data generated with a highly specialized library preparation procedure.\nBionano data: We glossed over Bionano in ISG5302, but it‚Äôs a specialized type of data used for scaffolding. We‚Äôre going to skip this as it is less commonly used currently.\n\nNote that by simply downloading pre-existing data, we are skipping over an important stage in planning a genome. We typically recommend that you try to get an estimate of the genome size so that you can aim for certain target coverages. You can sometimes find this in the literature from flow cytometry studies, but we‚Äôll cover below how you can get this number from sequencing data. It‚Äôs often cheap to collect some Illumina data that can be used for this purpose to guide long read sequencing and scaffolding data collection.\n\n7.4.1 Downloading the data\nIf you haven‚Äôt already, clone the repository:\ngit clone https://github.com/isg-certificate/genome_assembly.git\nThese are pretty simple scripts. In scripts/01_dataDownload/01_hifi.sh we just do the following to grab the two HiFi fastq files:\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/pacbio_hifi/m64330e_221020_171856.bc1022--bc1022.hifi_reads.fastq.gz\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/pacbio_hifi/m64334e_221030_084704.bc1022--bc1022.hifi_reads.fastq.gz\nIn scripts/01_dataDownload/02_hiC.sh we‚Äôre going to download the Illumina fastq files and a couple metadata files.\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/arima/fFunDia1_Banded_Killifish_R1_001.fastq.gz\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/arima/fFunDia1_Banded_Killifish_R2_001.fastq.gz\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/arima/re_bases.txt\nwget -P ${OUTDIR} https://genomeark.s3.amazonaws.com/species/Fundulus_diaphanus/fFunDia1/genomic_data/arima/fFunDia1_hic_stats.yaml\nThe yaml file contains some useful metadata:\nhic:\n  data:\n  - mean length: 150.0\n    name: fFunDia1_Banded_Killifish_R1_001.fastq.gz\n    reads: 616286356\n    yield: 92442953400\n  - mean length: 150.0\n    name: fFunDia1_Banded_Killifish_R2_001.fastq.gz\n    reads: 616286356\n    yield: 92442953400\n  metadata:\n  - enzymes: GATC,GANTC,CTNAG,TTAA\n    name: fFunDia1_Banded_Killifish_R1_001.fastq.gz\n    version: arima 2.0\n  - enzymes: GATC,GANTC,CTNAG,TTAA\n    name: fFunDia1_Banded_Killifish_R2_001.fastq.gz\n    version: arima 2.0\n  total:\n    coverage: 122.03690217821782\n    reads: 1232572712\n    yield: 184885906800\nWe have about 616 million 150bp paired end reads giving an expected 122x coverage of the genome. The ‚Äúenzymes‚Äù are restriction enzymes used in the library preparation.\n\n\n7.4.2 Basic QC\nWe‚Äôre going to do some basic QC on these data to get a basic sense of the data quality we‚Äôre dealing with.\n\n7.4.2.1 HiFi data\nFor the HiFi data, we‚Äôll run NanoPlot. It produces an HTML formatted report. See script scripts/02_rawQC/01_nanoplotHIFI.sh\nIt‚Äôs pretty simple:\nNanoPlot --fastq ${INDIR}/*hifi*fastq.gz -o ${OUTDIR} -p hifi -t 4 \nWe‚Äôre using globs (*) to get nanoplot to produce one report for both HiFi fastq files. To see some text results:\ncat results/02_qc/nanoplot/hifiNanoStats.txt \nGeneral summary:         \nMean read length:                 14,662.8\nMean read quality:                    28.3\nMedian read length:               13,825.0\nMedian read quality:                  35.6\nNumber of reads:               3,290,962.0\nRead length N50:                  14,886.0\nSTDEV read length:                 4,058.0\nTotal bases:              48,254,719,500.0\nNumber, percentage and megabases of reads above quality cutoffs\n&gt;Q5:    3290962 (100.0%) 48254.7Mb\n&gt;Q7:    3290962 (100.0%) 48254.7Mb\n&gt;Q10:   3290962 (100.0%) 48254.7Mb\n&gt;Q12:   3290962 (100.0%) 48254.7Mb\n&gt;Q15:   3290962 (100.0%) 48254.7Mb\nTop 5 highest mean basecall quality scores and their read lengths\n1:  93.0 (7981)\n2:  93.0 (8742)\n3:  93.0 (1273)\n4:  93.0 (7051)\n5:  93.0 (6247)\nTop 5 longest reads and their mean basecall quality score\n1:  49746 (22.3)\n2:  49407 (24.3)\n3:  49367 (21.6)\n4:  48162 (27.2)\n5:  47986 (25.8)\nAs expected for HiFi data, we have a mean read length of around 14kb. Median read quality is phred 35.6 for an error rate of 0.00028. This low error rate is really important as it allows assembly through ‚Äúrepetitive‚Äù sequences as long as the repeats have more differences than the expected error rate. Similarly, it allows the accurate separation of haplotypes in diploid organisms as long as heterozygosity is higher than the error rate (it very often is higher than this error rate).\nOur expected genome size is around 1.3gb, so this amounts to about 37x coverage.\nDon‚Äôt forget to download and open the HTML file in a browser: hifiNanoPlot-report.html The visualizations can help you understand more of the detail of your data (such as the fact that reads &lt; Q20 were filtered out).\n\n\n7.4.2.2 Illumina Hi-C data\nWe‚Äôre going to run good old fastqc on the Hi-C data. See scripts/02_rawQC/02_fastqcHiC.sh.\nfastqc -t 4 -o ${OUTDIR} ${INDIR}/fFunDia1*.gz\nThis will produce 2 HTML reports. We‚Äôll skip multiQC here because, well, two html files isn‚Äôt that much to just look at.\nWe can see these data are all 150bp, so they haven‚Äôt been trimmed before deposit in the public repository. We don‚Äôt see any major quality issues, and miraculously, zero adapter contamination. Because of this, we‚Äôll skip quality trimming.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#genome-size-estimation",
    "href": "03_1_genomeAssemblyKmers.html#genome-size-estimation",
    "title": "7¬† Genome Assembly - first steps",
    "section": "7.5 Genome size estimation",
    "text": "7.5 Genome size estimation\nAn area of genomics that is just plain cool and full of simple, clever ideas, is the analysis of k-mer data. The term k-mer refers simply to sequences of length k. The starting point for all k-mer-based analysis is to simply take a collection of sequences and chop them up into all possible subsequences of length k and count up their frequencies.\nSo for a sequence GTCAGTAGAT, all 5-mers are as follows:\nseq   GTCAGTAGAT\n5mer  GTCAG\n       TCAGT\n        CAGTA\n         AGTAG\n          TAGAT\nBecause DNA is double-stranded and typically sequenced without respect to strand, we usually count canonical k-mers, meaning that a k-mer and its reverse complement are treated as the same (e.g.¬†GTCAG = CTGAC).\nWe can chop up and count raw sequencing (as we will do here), or we can do it for completed genomes. In the third chapter in this module we will estimate the completeness and accuracy of our genome assembly by comparing the k-mers in the raw sequence data to the complete genome.\nOnce you have your k-mer count databases, there are a variety of analyses you can do to model the distribution and make inferences about the underlying data. In this section, we will estimate genome size and heterozygosity.\nThe idea behind estimating genome size from k-mers will be covered in ISG5302, so we won‚Äôt explain much more here, but we have a couple basic issues to deal with.\n\nWhat value should you choose for k? It‚Äôs hard to give great guidelines. It depends on genome size and heterozygosity. Usually something in the range of 19-23 works for estimating genome size. We‚Äôll go with 23 for this. - What type of data do you need? These types of k-mer analyses are only effective with highly accurate data, such as HiFi or Illumina data (or ONT duplex data). If the error rate is too high, too many k-mers will have errors in them and the models will perform poorly. You also need the data to be WGS shotgun data. That is, it should be randomly distributed throughout the genome. We have HiFi data, which will work. We also have Illumina Hi-C data. Hi-C might not really have the randomness we want, but we‚Äôll try it anyway and see what happens.\nWhat depth of coverage do we need? We usually recommend people shoot for 80-150x coverage (of Illumina data). We have 37x coverage of HiFi data, which is a bit low, but we‚Äôll try it anyway. The Hi-C data is 122x coverage. That would be great, but the Hi-C data may have some properties that detrimentally effect the estimates. We‚Äôll see!\n\n\n7.5.1 Getting the k-mer frequency spectrum.\nFor this application, what we want is the k-mer frequency spectrum. You can think of this as a histogram giving how many k-mers have a given frequency. So on the x-axis you have k-mer frequency and on the y-axis a count of k-mers.\n\n\n\n\n\n\n\n\n\nIn this plot, the peak is 36. If you‚Äôve done the ISG5302 section on this, you won‚Äôt be surprised to see that it is just about our expected sequencing coverage for the dataset. This is the data that will be modeled to estimate genome size and heterozygosity.\nTo get this, there are two steps:\n\nShred the sequencing data into k-mers and count up the frequency of each k-mer.\nCount up the frequencies of k-mer frequencies to get the k-mer frequency spectrum.\n\n\n7.5.1.1 Counting k-mers.\nWe‚Äôre going to use meryl for this. There are a lot of k-mer counters out there (including KMC, KAT and Jellyfish).\nK-mer counting of large sequencing datasets is typically a pretty memory-intensive process. This is because there are a ton of k-mers to keep track of! The larger k is, this worse this becomes. If k=1, you only have 4 (A, C, G, T). If k=read length (e.g.¬†150bp for our Hi-C data) then you are essentially counting the frequency of each unique read!\nWe have two scripts for k-mer counting, one for each of our datasets: - scripts/03_genomeSize/01_merylCountHIFI.sh - scripts/03_genomeSize/02_merylCountHiC.sh\nThe program calls are similar:\nmeryl count k=23 memory=250 threads=16 ${INDIR}/*hifi*fastq.gz output ${OUTDIR}/db\nHere we are requesting 250G of memory and 16 cpus (also reflected in the SLURM header).\nThe output will be a directory, ${OUTDIR}/db, containing the meryl k-mer database.\nFor the HiFi data, at 37x coverage, seff output looks like this:\nJob ID: 8884236\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 09:02:48\nCPU Efficiency: 87.10% of 10:23:12 core-walltime\nJob Wall-clock time: 00:38:57\nMemory Utilized: 152.96 GB\nMemory Efficiency: 61.19% of 250.00 GB\nSo we requested excess memory, but that‚Äôs ok. It took around 40 minutes of wall time.\nFor the Hi-C data, with 122x coverage:\nJob ID: 8884245\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 16\nCPU Utilized: 1-03:05:48\nCPU Efficiency: 87.58% of 1-06:56:16 core-walltime\nJob Wall-clock time: 01:56:01\nMemory Utilized: 226.71 GB\nMemory Efficiency: 64.78% of 350.00 GB\nIt used 226G of memory and 2 hours of wall time.\n\n\n7.5.1.2 The k-mer frequency spectrum\nNow that we‚Äôve digested the data into k-mer databases, actually calculating the frequency spectrum is super fast and easy and requires very little resources.\nThe scripts are:\n\nscripts/03_genomeSize/03_merylHistogramHIFI.sh\nscripts/03_genomeSize/04_merylHistogramHiC.sh\n\nThe command line looks like:\nmeryl histogram ${INDIR} | sed 's/\\t/ /' &gt;${OUTDIR}/hifi.histo\nAnd the hifi.histo file is very simple:\n1 1074054143\n2 105077828\n3 27325822\n4 10787262\n5 5526611\n6 3432311\n7 2509450\n8 2124587\n9 1995496\n10 2061575\n...\nColumn 1 is a frequency and column 2 is the number of k-mers that have that frequency. The sed command changes the tab separated table to a space because that‚Äôs what the next program we‚Äôre going to use wants.\nNote that by far the most common k-mer frequencies are 1 and 2. Those are k-mers with sequencing errors in them! Because sequencing errors are rare and random, they most often create singleton or doubleton k-mers.\n\n\n\n7.5.2 GenomeScope 2.0\nNow we‚Äôve got our k-mer frequencies, we‚Äôre going to run GenomeScope 2.0. GenomeScope is available as an R package, but it‚Äôs pretty light weight and the authors provide a nice web server for it, so we‚Äôll just use that.\nDownload the .histo files locally, visit the link and add the histo file to run the analysis. Don‚Äôt forget we used k=23. We get a nice figure:\n\n\n\nGenomeScope 2.0 HiFi output\n\n\nAnd text output:\nGenomeScope version 2.0\ninput file = user_uploads/17ZdeuYze9itb3IG8sym\noutput directory = user_data/17ZdeuYze9itb3IG8sym\np = 2\nk = 23\n\nproperty                      min               max               \nHomozygous (aa)               99.6948%          99.7166%          \nHeterozygous (ab)             0.283432%         0.305203%         \nGenome Haploid Length         1,262,324,162 bp  1,264,199,891 bp  \nGenome Repeat Length          432,312,843 bp    432,955,230 bp    \nGenome Unique Length          830,011,320 bp    831,244,660 bp    \nModel Fit                     64.8325%          95.2763%          \nRead Error Rate               0.135773%         0.135773%\nThis is telling us that GenomeScope thinks the genome is 1.262GB long and has a heterozygosity of 0.00283. This is not a terrible estimate for the genome length. We‚Äôll get closer to 1.3GB in the assembly. We might get a more accurate estimate if we had higher coverage data.\nSo what does the Hi-C data say?\n\n\n\nGenomeScope 2.0 HiFi output\n\n\nAnd text output:\nGenomeScope version 2.0\ninput file = user_uploads/q7IbQMxgKNTVYQBqgmUR\noutput directory = user_data/q7IbQMxgKNTVYQBqgmUR\np = 2\nk = 23\n\nproperty                      min               max               \nHomozygous (aa)               99.354%           99.381%           \nHeterozygous (ab)             0.618952%         0.646049%         \nGenome Haploid Length         1,220,750,197 bp  1,224,552,304 bp  \nGenome Repeat Length          439,125,189 bp    440,492,873 bp    \nGenome Unique Length          781,625,009 bp    784,059,431 bp    \nModel Fit                     65.7103%          98.1208%          \nRead Error Rate               0.448916%         0.448916%      \nWe‚Äôre still getting a similar underestimate of genome size, and for some reason an increased estimate of heterozygosity.\nThese estimates, while not quite right, would be sufficient for planning sequencing coverage. The actual genome is 3-6% larger, so if you had aimed for 30x coverage based on this estimate, you would have wound up with 28.3-29.1x coverage.\nThe estimate of heterozygosity is considerably noisier, changing by 2x. This isn‚Äôt really a problem though, we can still see that it is much higher than our expected sequencing error rate (0.00028). We‚Äôll expect to see 4.2 errors per 15kb sequencing read, but an average of 42.5-92.8 heterozygous sites. This means we should expect our assembler to separately assemble haplotypes for a lot of the genome.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "03_1_genomeAssemblyKmers.html#conclusion",
    "href": "03_1_genomeAssemblyKmers.html#conclusion",
    "title": "7¬† Genome Assembly - first steps",
    "section": "7.6 Conclusion",
    "text": "7.6 Conclusion\nWe‚Äôve got a pretty good picture of our data here and what to expect from our genome. In the next chapter we‚Äôll go over assembly and scaffolding. In the third chapter in the module we‚Äôll cover assembly QC.",
    "crumbs": [
      "Genome Assembly",
      "<span class='chapter-number'>7</span>¬† <span class='chapter-title'>Genome Assembly - first steps</span>"
    ]
  },
  {
    "objectID": "01_0_IntroToGit.html",
    "href": "01_0_IntroToGit.html",
    "title": "Introduction to Git",
    "section": "",
    "text": "Learning Objectives\nIn this module we will learn the basics of the version control software git, and how to set up a remote code repository on the web service GitHub.\nThese chapters will be somewhat sparse, as you will be referred to existing training material in HuskyCT.\nMany (perhaps most?) people find Git to be a bit confusing at times. You should prepare for that possibility, and know that you may need to refer to documentation or seek help.",
    "crumbs": [
      "Introduction to Git"
    ]
  },
  {
    "objectID": "01_0_IntroToGit.html#learning-objectives",
    "href": "01_0_IntroToGit.html#learning-objectives",
    "title": "Introduction to Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nCreate a code repository with git.\n\n\nSet up a documented remote copy of the repository on github.\n\n\nBranch the repository to edit, and then merge changes.\n\n\nWork collaboratively.",
    "crumbs": [
      "Introduction to Git"
    ]
  },
  {
    "objectID": "02_0_variantDetection.html",
    "href": "02_0_variantDetection.html",
    "title": "Variant Detection and Genotyping",
    "section": "",
    "text": "Learning Objectives\nIn this module we will work through a basic variant detection and genotyping workflow. We‚Äôll cover quality control at various stages of the analysis, and use a few different variant callers so that we can compare results. We‚Äôll also cover basic variant effect annotation. The basic scripts we‚Äôre going to work through can be found at this github repository.",
    "crumbs": [
      "Variant Detection and Genotyping"
    ]
  },
  {
    "objectID": "02_0_variantDetection.html#learning-objectives",
    "href": "02_0_variantDetection.html#learning-objectives",
    "title": "Variant Detection and Genotyping",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw and mapped data specific to variant detection.\n\n\nIdentify variants and genotype samples against a reference genome.\n\n\nManipulate the VCF format\n\n\nApply strategies for filtering problematic variants.\n\n\nAssess variant call-set quality.\n\n\nConduct basic variant effect annotation.",
    "crumbs": [
      "Variant Detection and Genotyping"
    ]
  },
  {
    "objectID": "03_0_genomeAssembly.html",
    "href": "03_0_genomeAssembly.html",
    "title": "Genome Assembly",
    "section": "",
    "text": "Learning Objectives\nIn this module we‚Äôll go over the essentials of a genome assembly workflow. We‚Äôll cover QC steps (pretty sparse here) and do some initial estimation of some features of the genome using k-mers. We will then assemble a haplotype phased genome using PacBio HiFi data and Hi-C data from Arima. We‚Äôll scaffold the assembly using the Hi-C data and do some QC steps on the assembly.",
    "crumbs": [
      "Genome Assembly"
    ]
  },
  {
    "objectID": "03_0_genomeAssembly.html#learning-objectives",
    "href": "03_0_genomeAssembly.html#learning-objectives",
    "title": "Genome Assembly",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw data specific to genome assembly.\n\n\nEstimate genome size from unassembled reads.\n\n\nAssemble a genome sequence.\n\n\nScaffold a genome sequence using Hi-C data.\n\n\nEstimate the completeness, correctness, and contiguity of an assembled genome.",
    "crumbs": [
      "Genome Assembly"
    ]
  },
  {
    "objectID": "04_0_scRNAseq.html",
    "href": "04_0_scRNAseq.html",
    "title": "Single-cell RNAseq",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\n\n\nLearning Objectives:\n\n\nFIRST OBJECTIVE",
    "crumbs": [
      "Single-cell RNAseq"
    ]
  },
  {
    "objectID": "05_0_workflows.html",
    "href": "05_0_workflows.html",
    "title": "Pipeline development with Nextflow",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\n\n\nLearning Objectives:\n\n\nFIRST OBJECTIVE",
    "crumbs": [
      "Pipeline development with Nextflow"
    ]
  }
]