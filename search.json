[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ISG 5312 - Genomic Data Analysis in Practice II",
    "section": "",
    "text": "Preface\nThis course is the second in a two part series on practical analysis of genomic data. The first course heavily emphasized Linux and HPC skills, the statistical computing language R, and the basics of high throughput sequencing data. It used a model workflow, bulk RNA-seq analysis, as a framework for developing basic competencies.\nThis course will broaden the focus to include higher level bioinformatics skills, such as using Git for version controlling code, an introduction to workflow languages through Nextflow, and introduces several more workflows (variant detection, genome assembly, and single-cell RNA-seq). The course comprises five modules, one for each of these topics.\nIn this semester students will also do independent projects where they pick a workflow and reanalyze a relevant public dataset.\nThere are five modules:\n\nIntroduction to version control with Git/GitHub.\nVariant Detection.\nGenome Assembly.\nscRNAseq\nPipeline development with Nextflow",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html",
    "href": "01_1_git-ingStarted.html",
    "title": "1¬† First steps with Git",
    "section": "",
    "text": "1.1 Learning Objectives\nIn the first chapter of this module, you will learn the very basics of Git and learn how to set up a remote repository on GitHub.\nThere are GUI tools for using Git, but we‚Äôre going to focus on the command-line here (except when we get to GitHub).\nNote that explanation will be sparse here, as you will be expected to do some assigned reading and watch some videos.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#learning-objectives",
    "href": "01_1_git-ingStarted.html#learning-objectives",
    "title": "1¬† First steps with Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nCreate a code repository with git.\n\n\nSet up a documented remote copy of the repository on GitHub.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#installing-git",
    "href": "01_1_git-ingStarted.html#installing-git",
    "title": "1¬† First steps with Git",
    "section": "1.2 Installing Git",
    "text": "1.2 Installing Git\nYou‚Äôre going to want git to be installed on your local machine. There are instructions here\nIf you‚Äôre using a Mac, you‚Äôve probably already got it. If not, you can install Xcode developer tools, which will include it.\nIf you‚Äôre using Windows (and thus probably also WSL2) you may want it installed for both systems. See the Linux and Windows instructions here.\nNote that on Xanadu, the base installation of Git is waaay out of date. In most cases things will still work fine. Do git --version to see what version is currently running (üíÄ). You can module load git/2.30.0, however, if you run into problems.\nIf you did the chapter on customizing your shell in ISG5311, you can add module load git/2.30.0 to your .bashrc file in your home directory so that the module will be loaded every time you log in.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#a-brief-pause",
    "href": "01_1_git-ingStarted.html#a-brief-pause",
    "title": "1¬† First steps with Git",
    "section": "1.3 A brief pause‚Ä¶",
    "text": "1.3 A brief pause‚Ä¶\nBefore going further here, please do the readings and watch the videos in HuskyCT.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#configuring-git",
    "href": "01_1_git-ingStarted.html#configuring-git",
    "title": "1¬† First steps with Git",
    "section": "1.4 Configuring Git",
    "text": "1.4 Configuring Git\nBefore we do much with Git, we want to make sure that your instances are configured correctly. Anywhere you‚Äôve got it installed, you want to set it up to say who you are. Run these commands everywhere you might use it (Mac, Windows, WSL, and don‚Äôt forget Xanadu!), but obviously, please edit them first.\ngit config --global user.name \"Bob Loblaw\"\ngit config --global user.mail \"Bob.Loblaw@BobLoblawsLawBlog.com\"\nThe config subcommand edits git‚Äôs configuration file. --global applies settings to all the current user‚Äôs repositories on this computer (you can change these settings for individual repositories). You can see your configuration with\ngit config -l\n\n1.4.1 Git‚Äôs text editor\nGit is meant to hide in the background. You don‚Äôt use it to write or edit code, but to track it as it develops. Nevertheless, there are a times when Git will demand that you explain something, or reconcile conflicting edits. When that happens it will drop you into a text editor.\n\n1.4.1.1 vim\nBy default that text editor is vi, a powerful and widely used command line editor that comes with, shall we say, a steep learning curve.\nYou can change this. First let‚Äôs quickly cover the absolute barest of details for vi (more likely an updated version vim), as you will sooner or later run into it.\nLet‚Äôs create a dummy file with some text.\necho {a..z} | sed 's/ /\\n/g' &gt;letters.txt\nWe can open it with vi\nvi letters.txt\nAfter opening, you are in Normal mode. You can navigate around, but not edit.\nIf you press i you will go into Insert mode. Now you can edit. Press escape to go back to normal mode. Beware there are several other modes.\nQuitting vi is famously annoying to people who don‚Äôt know how to use it (or who have learned and forgotten many times).\nFirst, go to normal mode by pressing escape. After this:\n\nIf you have made no edits and wish to quit, you can type : then q then enter (:q then enter).\nIf you have made edits you wish to discard, you can type :q! then enter.\nIf you have made edits you wish to save you can type :w then enter THEN :q OR just :wq then enter.\n\n\nIf you want the bragging rights (and efficiency) that come with being proficient at using a powerful command-line editor, there are lots of ways to learn. One is through this game, vim adventures.\n\n\n1.4.1.2 Changing the text editor (if you want)\nIf you thought, ‚Äúvim is fine‚Äù, then fantastic, you can keep using it. If you thought, ‚Äúyuck‚Äù, and want to change the editor, you can use git config.\nIf you wanted to use nano for instance, you could do:\ngit config --global core.editor nano\nYou can also use VS Code (only on your local machine). First, you have to enable launching VS Code from the command line (see here)\nThen run\ngit config --global core.editor \"code --wait\"\nThe flag --wait will cause the terminal to wait until VS Code is closed before moving on.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#git-basics",
    "href": "01_1_git-ingStarted.html#git-basics",
    "title": "1¬† First steps with Git",
    "section": "1.5 Git Basics",
    "text": "1.5 Git Basics\nOk, hopefully git is all set up now. We can run through the basics. There are two ways you might get started with a Git repository.\n\nYou create a new one in a brand new directory as you would with a new project.\nYou can clone a copy of an existing repository to alter or contribute to it. Perhaps one from GitHub (or GitLab, or BitBucket).\n\n\n1.5.1 Starting a new repository:\nTo start a git repository:\nmkdir newproject\ncd newproject\ngit init\nWhich writes out:\nhint: Using 'master' as the name for the initial branch. This default branch name\nhint: is subject to change. To configure the initial branch name to use in all\nhint: of your new repositories, which will suppress this warning, call:\nhint: \nhint:   git config --global init.defaultBranch &lt;name&gt;\nhint: \nhint: Names commonly chosen instead of 'master' are 'main', 'trunk' and\nhint: 'development'. The just-created branch can be renamed via this command:\nhint: \nhint:   git branch -m &lt;name&gt;\nInitialized empty Git repository in /home/FCAM/blah/blah/blah/newproject/.git/\nYou can see (with ls -a) that a hidden directory newproject/.git/ has been created. This .git directory is where Git will store the whole version control history. You should never muck around in .git. If for some reason you wanted to destroy the whole version history, you could delete .git and do git init to start anew.\n\n\n1.5.2 Cloning a repository\nWe saw this last semester with our RNAseq example, but you can clone a git repository like this:\ngit clone https://github.com/isg-certificate/rnaseq.git\ncd rnaseq\nWith stderr output:\nCloning into 'rnaseq'...\nremote: Enumerating objects: 85, done.\nremote: Counting objects: 100% (85/85), done.\nremote: Compressing objects: 100% (62/62), done.\nremote: Total 85 (delta 35), reused 66 (delta 19), pack-reused 0 (from 0)\nReceiving objects: 100% (85/85), 20.91 KiB | 509.00 KiB/s, done.\nResolving deltas: 100% (35/35), done.\nIf we do ls -la:\ndrwxr-xr-x 5 nreid cbc 2560 Jan  6 17:05 .\ndrwxr-xr-x 4 nreid cbc 1024 Jan  6 17:05 ..\ndrwxr-xr-x 8 nreid cbc 5632 Jan  6 17:05 .git\n-rw-r--r-- 1 nreid cbc   68 Jan  6 17:05 .gitignore\ndrwxr-xr-x 2 nreid cbc  512 Jan  6 17:05 metadata\n-rw-r--r-- 1 nreid cbc    8 Jan  6 17:05 README.md\ndrwxr-xr-x 7 nreid cbc 2560 Jan  6 17:05 scripts\nYou can see we grab all the contents of the repository, but also we have a .git directory.\n\n\n1.5.3 Adding files to the repository\nAdding a file (or a change to a file) to the repository happens in three steps.\n\nYou create (or alter) a file.\nYou add the file to the staging area.\nYou commit the file.\n\nLet‚Äôs consider our fresh, clean repository newproject. Let‚Äôs assume we‚Äôre going to start a new bioinformatics project. cd into newproject and type git status. You should see this:\n# On branch master\n#\n# Initial commit\n#\nnothing to commit (create/copy files and use \"git add\" to track)\ngit status gives us a summary of any changes that have been made to the repository since it was last ‚Äúcommitted‚Äù (more in a moment).\nLet‚Äôs create a script:\n# this syntax allows you to print a multi-line file and redirect it. \ncat &lt;&lt;EOF &gt;hw.sh\n#!/bin/bash\n\necho \"Hello World!\"\nEOF\n\nbash hw.sh\nNow git status again:\n# On branch master\n#\n# Initial commit\n#\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hw.sh\nnothing added to commit but untracked files present (use \"git add\" to track)\nGit sees that we have a new file that is not being tracked as part of the repository. We can have lots of files that are not tracked as part of the repository. In fact, when doing bioinformatics, we probably want to keep all our results (and maybe even the data) in this working directory, but we do not want to keep them as part of the\nWe can add this file to the staging area with git add hw.sh. We could also add everything in the repository with git add --all.\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#\n#   new file:   hw.sh\n#\nNow git is tracking the file and its current contents have been staged, but it has NOT been committed to the repository. Remember, there are three conceptual spaces in Git, the working directory (where we actually work) the staging area (where we add changes temporarily) and the repository (where we more or less permanently record updates to the project).\nNow that we‚Äôve got our file in the staging area, we can commit it to the repository.\ngit commit -m \"first commit\"\nThat produces the message:\n[master (root-commit) fe2dafd] first commit added hw.sh\n 1 file changed, 3 insertions(+)\n create mode 100644 hw.sh \nNow we‚Äôve committed the first file to the repository.\nThere is lots more to learn, including branching, merging, inspecting the history‚Ä¶ we‚Äôll get to that in the next chapter. For the last part of this chapter we‚Äôll introduce GitHub,",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#remote-repositories",
    "href": "01_1_git-ingStarted.html#remote-repositories",
    "title": "1¬† First steps with Git",
    "section": "1.6 Remote repositories",
    "text": "1.6 Remote repositories\nGit is a distributed version control system, so it naturally allows us (and collaborators) to maintain and interact with remote copies of our repositories.\nAssuming you cloned rnaseq above, if you enter the working directory and type\ngit remote -v\nYou will see:\norigin  https://github.com/isg-certificate/rnaseq.git (fetch)\norigin  https://github.com/isg-certificate/rnaseq.git (push)\nThis indicates the cloned repository is using a repository at the URL https://github.com/isg-certificate/rnaseq.git as a remote. rnaseq.git is a Git repository in the same sense that the .git directory in newproject is a Git repository. The remote is named origin. This is arbitrary and the default choice in git. You can name remotes anything you want. (fetch) and (push) mean that pulling down changes from the remote and pushing them up uses the same URL. The repository is hosted at GitHub (more in the next section), which controls access, so you won‚Äôt have permission to push any changes you might make.\nIf you cd to our brand new directory newproject and do git remote -v You should see no output.\n\n1.6.1 Adding a remote repository\nMost people host remote repositories, well, remotely, but to make things clear (hopefully) we‚Äôll create a remote repository for newproject locally.\ncd into the directory containing newproject (not newproject itself) and do\ngit init --bare remotenewproject.git\nIf you do ls you should now see (at least) newproject  remotenewproject.git.\nremotenewproject.git is a git repository, like .git in newproject. There is no working directory (or working tree, as git sometimes calls it). You cannot directly edit the files in this remote (without changing some settings first). This is a result of the option --bare. You want this because you want the remote to serve as a common access point or a backup of your code.\nNow cd back into newproject. You can add remotenewproject.git like this:\ngit remote add origin ../remotenewproject.git\nThe git remote add is adding a remote (arbitrarily) named origin at the location ../remotenewproject.git.\nIf you do git remote -v you‚Äôll see\norigin  ../remotenewproject.git (fetch)\norigin  ../remotenewproject.git (push)\nThe remote repository currently does not have any of our local commits in it. We will cover branches in the next chapter, but to push our local commits to the remote, we need to set an ‚Äúupstream‚Äù branch in the remote repository to push to.\nWe can see that our current branch does not have an upstream remote branch to push to by doing:\ngit branch -vv\n* master bf15b43 initial commit\nWe can set the upstream branch and push in one command:\ngit push -u origin master\nCounting objects: 3, done.\nWriting objects: 100% (3/3), 226 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo ../remotenewproject.git\n * [new branch]      master -&gt; master\nBranch master set up to track remote branch master from origin.\n-u origin says we‚Äôre setting up an upstream branch in origin master is the (arbitrary) name of the local and remote branch we‚Äôre pushing.\nDo git branch -vv to see we have now set up a remote master branch at origin:\n* master bf15b43 [origin/master] initial commit\nIn the future we can just do git push to push changes to the remote.\n\n\n1.6.2 Pushing and pulling from remotes.\nThere are many cases, such as collaboration, or a single person using multiple workstations, where you will have a single central remote repository and clones in multiple locations. You may update the code in one place, push it to the remote, and then need to pull it down in another place. This can get complicated (more in the next chapter) but we‚Äôll do a simple version of that now.\nFirst, clone a new copy of the remote:\ngit clone remotenewproject.git newproject_copy\nCloning into 'newproject_copy'...\ndone.\nNow ls and see: newproject  newproject_copy  remotenewproject.git.\nWe have the original directory we created, the remote repository, and our new clone. cd into newproject_copy and create a new file:\ncat &lt;&lt;EOF &gt;hm.sh\n#!/bin/bash\n\necho \"Hola Mundo!\"\nEOF\n\nbash hm.sh\nNow: git status\n# On branch master\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hm.sh\nnothing added to commit but untracked files present (use \"git add\" to track)\nDo git add hm.sh and then git commit -m \"added hola mundo script\"\nNow git status:\n# On branch master\n# Your branch is ahead of 'origin/master' by 1 commit.\n#   (use \"git push\" to publish your local commits)\n#\nnothing to commit, working directory clean\nOur branch is 1 commit ahead of the remote.\nNow git push to send the local repository changes to the remote:\nCounting objects: 4, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 312 bytes | 0 bytes/s, done.\nTotal 3 (delta 0), reused 0 (delta 0)\nTo /path/to/remotenewproject.git\n   bf15b43..7f84097  master -&gt; master\nNow go back to our original newproject directory and git status. You‚Äôll see it does not know that the remote repository has been updated. To pull down any changes do git pull\nremote: Counting objects: 4, done.\nremote: Compressing objects: 100% (2/2), done.\nremote: Total 3 (delta 0), reused 0 (delta 0)\nUnpacking objects: 100% (3/3), done.\nFrom ../remotenewproject\n   bf15b43..7f84097  master     -&gt; origin/master\nUpdating bf15b43..7f84097\nFast-forward\n hm.sh | 3 +++\n 1 file changed, 3 insertions(+)\n create mode 100644 hm.sh\nls should show we now have our new file.\nTo see our commit history try git log\ncommit 7f84097d7c3528e43023c66b5ddfc7d302567002\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Wed Dec 31 23:59:59 1999 -0500\n\n    added hola mundo script\n\ncommit bf15b4396cb1c7379c03bbca9222ef46f2384359\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Wed Dec 31 11:59:59 1999 -0500\n\n    initial commit\nThis process can get complicated, and conflicts can arise. We‚Äôll deal with that in the next chapter.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#github",
    "href": "01_1_git-ingStarted.html#github",
    "title": "1¬† First steps with Git",
    "section": "1.7 GitHub",
    "text": "1.7 GitHub\nGitHub is a web service (among others, such as GitLab and Bitbucket) that can host Git repositories. GitHub is a natural place to store a remote repository. It allows you to control who can access or contribute to the repository, and is a great way to share (and find) open code.\nWe are going to use GitHub in this course in several exercises and the final project. You may wish to keep your final project from this semester in a public repository as a sort of portfolio project.\n\n\n\n\n\n\nGitHub Copilot and privacy\n\n\n\n\n\nBe aware that GitHub has an AI assistant, ‚ÄúCopilot‚Äù which is trained at least in part on code stored in GitHub. It is unclear which code is used for training, but most likely individual account repositories are used (both public and private).\n\n\n\nBefore moving forward, please create an account.\nAfter you‚Äôve created an account, you will need to set up SSH key authentication so that Git can access your remote repositories hosted at GitHub. GitHub no longer allows password authentication with command-line Git. You may have some experience with SSH keys already if you set them up for access to Xanadu. GitHub‚Äôs documentation for this is here.\nYou should set up ssh key access from both your local machine and Xanadu. If you have trouble with this, please reach out for help.\n\n1.7.1 Setting up GitHub as a remote repository\nThe easiest way to approach setting up GitHub as a remote with a new project is to start a new empty repository on GitHub (from your home page) and then clone it locally.\nIf you‚Äôve already got a repository started, you can more or less follow the same approach we did above. First go to your GitHub profile and click on the Repositories tab (e.g.¬†user isg-certificate would go to the page https://github.com/isg-certificate?tab=repositories) and click the green New button.\nEnter the name newproject, select ‚Äúpublic‚Äù or ‚Äúprivate‚Äù as you prefer and then click Create repository.\nGitHub will then tell you how to push an existing repository (although it will use main as the default main branch name, whereas in this material we have been using master).\nBefore you do this, however, let‚Äôs get rid of our extra clone and local remote:\n# force remove git repos\nrm -rf newproject_copy\nrm -rf newproject.git\n\n# remove current remote\ncd newproject\ngit remote remove origin\nNow we can add a new remote (again named origin). cd into newproject and:\ngit remote add origin git@github.com:isg-certificate/newproject.git\ngit branch -M master\ngit push -u origin master\nIf you go to the repository page on GitHub you should see your files hm.sh and hw.sh have been pushed there.\n\n\n1.7.2 README.md\nIt‚Äôs always advisable to include documentation. GitHub encourages this through the inclusion of a (GitHub flavored) markdown formatted document titled README.md. GitHub will format this document into HTML and include its contents on the splash page for the GitHub hosted repository.\nLet‚Äôs create a file locally in newproject now and push it to GitHub:\necho \"This is a dummy project to demo Git/GitHub\" &gt;README.md\ngit add README.md\ngit commit -m \"added README.md\"\ngit push\nNow visit the GitHub repository page and you should see the contents of README.md.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_1_git-ingStarted.html#summing-up",
    "href": "01_1_git-ingStarted.html#summing-up",
    "title": "1¬† First steps with Git",
    "section": "1.8 Summing up",
    "text": "1.8 Summing up\nGit has many commands (type git to see them all). We have used the following:\n\ngit init to initialize a new repository.\ngit add to add a file to the staging area.\ngit commit to commit a staged snapshot of the working directory to the repository.\ngit status to summarize the current status of new, alterered, or staged files.\ngit remote to list and modify remote repositories.\ngit push to push changes to a remote repository.\ngit pull to pull changes from a remote repository. ## A few tips for thinking about Git/GitHub\nIn Bioinformatics, Git is used for code, documentation and sometimes figures or written reports. Don‚Äôt track analysis results unless they are very small and you think its important to version control them. fastqs, bams, etc should not be committed. Git is not meant for dealing with large data.\nGenerally think about Git as forward looking. If you want to delete a file, don‚Äôt think about deleting it from the history of the repository, think about deleting now and leaving it out in the future. If you want to restore a file (that was deleted or altered in a regrettable way) think about it as grabbing that file from a past commit and bringing it forward to replace your unwanted changes rather than ‚Äúundoing‚Äù them. We‚Äôll cover this in the next chapter.\nThere is a ton of stuff you can do with GitHub beyond hosting remote repositories (including hosting static web pages like this one!). We won‚Äôt get too far into it in this course.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>First steps with Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html",
    "href": "01_2_moreGit.html",
    "title": "2¬† More Git",
    "section": "",
    "text": "2.1 Learning Objectives\nIn the second chapter, we‚Äôre going to cover a few more topics.\nAgain, this will all be pretty short on explanation, make sure to refer to readings and videos in HuskyCT to understand better how things work.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#learning-objectives",
    "href": "01_2_moreGit.html#learning-objectives",
    "title": "2¬† More Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nBranch the repository to edit, and then merge changes.\n\n\nWork collaboratively.\n\n\n\n\n\nRestoring past versions.\nTelling git to ignore files.\nBranching and merging.\nManaging remote branches.\nForking and pull requests on Github.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#restoring-past-versions",
    "href": "01_2_moreGit.html#restoring-past-versions",
    "title": "2¬† More Git",
    "section": "2.2 Restoring past versions",
    "text": "2.2 Restoring past versions\nConsider our newproject directory. It contains two files (and the hidden .git directory):\ntree\n.\n‚îú‚îÄ‚îÄ hm.sh\n‚îî‚îÄ‚îÄ hw.sh\n\n0 directories, 2 files\nThey should both be committed at this point.\nWhat happens if we make some modifications and we want to get rid of them? Like say, we overwrite the entire file.\necho 'oops!' &gt;hw.sh\ngit status will show us the file has been modified:\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   hw.sh\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n\n2.2.1 When the changes are unstaged\nSo we‚Äôve made an edit, but haven‚Äôt staged it. We can grab the most recent committed version with:\ngit restore hw.sh\nNow cat hw.sh\n#!/bin/bash\n\necho \"Hello World!\"\nReveals the file has been restored.\n\n\n2.2.2 When the changes are staged\nIf we have already staged the file however, this won‚Äôt work directly, as it will simply check the file out from the staging area, not the most recent commit.\nIf you make the same mistake again, echo 'oops!' &gt;hw.sh, but this time stage the change git add hw.sh, you will see with git status:\nOn branch master\nYour branch is up to date with 'origin/master'.\n\nChanges to be committed:\n  (use \"git restore --staged &lt;file&gt;...\" to unstage)\n    modified:   hw.sh\nPer the message, you can unstage with\ngit restore --staged hw.sh\nAnd then go back to the last commit like before with git restore hw.sh.\n\n\n2.2.3 When the changes have been committed\nIf we make an edit and commit it:\necho 'echo \"Good day, world!\"' &gt;&gt;hw.sh\ngit add hw.sh\ngit commit -m \"good day\"\nWe‚Äôve now added a change to our repository. If we want to go back to a previous version (i.e without the ‚Äúgood day world‚Äù echo), we first need to identify the commit containing the file version we want. We can run git log to see a list of commits.\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84 (HEAD -&gt; master)\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 1999 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 1998 -0500\n\n    added hola mundo script\n\ncommit 39fb28833d64c1176fc4fa8dc010d0dbdf8ff374\nAuthor: Noah Reid &lt;noah@uconn.edu&gt;\nDate:   Fri Jan 10 10:43:07 1997 -0500\n\n    first commit\nCommits are uniquely identified by those long strings of letters and numbers. They are hashes of the snapshot (you can read more elsewhere!).\nTo get a version of the file from one of these commits:\ngit restore --source=ed8b357e hw.sh\nWe only need to provide enough of the hash that it is unique among commits. Now git status\nOn branch master\nYour branch is ahead of 'origin/master' by 1 commit.\n  (use \"git push\" to publish your local commits)\n\nChanges not staged for commit:\n  (use \"git add &lt;file&gt;...\" to update what will be committed)\n  (use \"git restore &lt;file&gt;...\" to discard changes in working directory)\n    modified:   hw.sh\n\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nWe have restored our file, but not staged or committed it. We need to git add and then git commit. Now git log:\ncommit 3140e0d685f677c4ac2cc2123a8b53ef68eeee4a (HEAD -&gt; master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:40:11 2025 -0500\n\n    restoring hw.sh\n\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 2025 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 2025 -0500\n\n    added hola mundo script\n\ncommit 39fb28833d64c1176fc4fa8dc010d0dbdf8ff374\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:43:07 2025 -0500\n\n    first commit\nWe can see we have added a new commit. We didn‚Äôt go backwards in our history so much as reached back to a previous file version and pulled it forward into a new change.\n\n\n2.2.4 Examining differences between file versions\nWhat if we‚Äôre not sure which commit to restore from? We can use git diff to examine differences between files:\nWe can compare file versions with git diff.\ngit diff f8b8dcc hw.sh\nThis tells git to compare the file hw.sh between the current working space and the commit hash beginning with f8b8dcc. The output:\ndiff --git a/hw.sh b/hw.sh\nindex 22b4300..cd16289 100644\n--- a/hw.sh\n+++ b/hw.sh\n@@ -1,4 +1,3 @@\n #!/bin/bash\n \n echo \"Hello World!\"\n-echo \"Good day, world!\"\nThere‚Äôs a lot here (nicely colored in the terminal), but you can see, essentially, that the line echo \"Good day, world!\" has been removed (signified by -) from our current version, relative to the commit we‚Äôre examining.\nIf you‚Äôre working on a local machine, you can use something else to view the diff results. For Visual Studio Code, you can configure the difftool command like this:\ngit config --global diff.tool vscode\ngit config --global difftool.vscode.cmd 'code --wait --diff $LOCAL $REMOTE'\nAs mentioned in the previous chapter, code will have to be in your PATH (linked instructions are there).\nNow\ngit difftool ed8b357e46329f4 hw.sh\nAnd after a prompt you should get a nicely formatted window with side-by-side files that makes it easy to see differences.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#branching-and-merging",
    "href": "01_2_moreGit.html#branching-and-merging",
    "title": "2¬† More Git",
    "section": "2.3 Branching and merging",
    "text": "2.3 Branching and merging\nThis is one of the pillars of git and can get a little confusing, especially when remotes are brought into the equation.\nWhen working with an established code repository, you may have an idea you want to test out, but you are unsure of whether it will be any good, or it may take some time to implement and you don‚Äôt want to unsettle the existing code.\n\n2.3.1 Branching\nIn these cases you can create a new branch (again, see materials in huskyCT for more conceptual details). A branch is an independent line of commits. Repositories can have dozens of branches. Check the GitHub repository for the NF-Core pipeline rnaseq. Click the button with the branching icon that says master.\nAfter you have worked out your idea, if you decide it should become a part of the repository, you can merge it back into the main branch.\nWith our newproject repository, we can see existing branches like this:\ngit branch -vv\n* master 3140e0d [origin/master: ahead 2] restoring hw.sh\nThere‚Äôs only one branch currently. The * means this branch is currently what we see in the working directory. master is the name of the branch (the arbitrary default). 3140e0d is the beginning of the commit hash. [origin/master: ahead 2] indicates this branch is tracking a branch master at our remote origin and that it is two commits ahead of origin/master as of the last time we communicated with the remote. restoring hw.sh is the commit message from the last commit.\nLet‚Äôs make a new branch.\ngit branch goodbye\nNow git branch -vv\n  goodbye 3140e0d restoring hw.sh\n* master  3140e0d [origin/master: ahead 2] restoring hw.sh\nWe have a new branch goodbye. We didn‚Äôt switch to that branch, however (* is still on master), and importantly this branch is not on our remote (or any remote).\nTo switch to the branch:\ngit checkout goodbye\nNow git branch -vv\n* goodbye 3140e0d restoring hw.sh\n  master  3140e0d [origin/master: ahead 2] restoring hw.sh\nLet‚Äôs add some new files:\ncat &lt;&lt;EOF &gt;gw.sh\n#!/bin/bash\n\necho \"Goodbye World!\"\nEOF\n\ncat &lt;&lt;EOF &gt;am.sh\n#!/bin/bash\n\necho \"Adios Mundo!\"\nEOF\nAdd git add --all and commit git commit -m \"goodbye\"\nNow we‚Äôve added two brand new files that should only be present on branch goodbye.\nSee ls\nam.sh  gw.sh  hm.sh  hw.sh\nNow switch branches git checkout master and ls\nhm.sh  hw.sh\nNow back to goodbye with git checkout goodbye.\nLet‚Äôs look at our history another way git log --decorate. The --decorate option will write the names of any references. References very simply point at commits. While conceptually a branch is much like the branch of a tree, when you create a branch, git creates one of these pointers with the name of your branch and attaches it to a commit. The last few commits with references added:\ngit log --decorate\ncommit 45f76a809cc3b03c53d7fe1e91ab23944438f5a2 (HEAD, goodbye)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 14:44:40 2001 -0500\n\n    goodbye\n\ncommit 3140e0d685f677c4ac2cc2123a8b53ef68eeee4a (master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:40:11 2000 -0500\n\n    restoring hw.sh\n\ncommit f8b8dcc6e75b325d4976f27d10c74f7154452d84\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 11:23:27 1999 -0500\n\n    good day\n\ncommit ed8b357e46329f423101296fbba6b85b988972fc (origin/master)\nAuthor: Noah Reid &lt;noah.reid@uconn.edu&gt;\nDate:   Fri Jan 10 10:44:00 1998 -0500\n\n    added hola mundo script\nWe have references HEAD, goodbye, master and origin/master.\nHEAD is a special pointer telling us which commit is currently checked out in our working directory (uncommitted changes notwithstanding). goodbye is the reference for our current branch. origin/master is a remote tracking branch (more shortly) telling us which commit the remote was one when we last checked in with it.\nWe can put HEAD on a commit without a branch, but this isn‚Äôt a great state of affairs:\ngit checkout f8b8dcc6e75b325d4976f27d10c74f7154452d84\nNote: checking out 'f8b8dcc6e75b325d4976f27d10c74f7154452d84'.\n\nYou are in 'detached HEAD' state. You can look around, make experimental\nchanges and commit them, and you can discard any commits you make in this\nstate without impacting any branches by performing another checkout.\n\nIf you want to create a new branch to retain commits you create, you may\ndo so (now or later) by using -b with the checkout command again. Example:\n\n  git checkout -b new_branch_name\n\nHEAD is now at f8b8dcc... good day\ndetached HEAD state means the HEAD pointer is on a commit but has no branch. As the message says, you can create a new branch from this point, or just git checkout goodbye to get back to your original commit.\nSee git log --decorate to validate that HEAD is all by its lonesome.\nmaster is currently directly upstream of goodbye, but branches can diverge down independent paths. Lets make a couple changes to master. First git checkout master.\necho 'echo Hiya World!' &gt;&gt;hw.sh\n\ncat &lt;&lt;EOF &gt;bl.sh\n#!/bin/bash\n\necho 'Bonjour le Monde!'\nEOF\nWe‚Äôve added a file and altered one. Now git add --all and git commit -m \"more hi\"\nIf you do git log --decorate now, you‚Äôll see the pointer for goodbye has disappeared. It‚Äôs not gone, but it‚Äôs not part of the commit history for master at this point. The changes we made to goodbye and master are on diverging branches.\n\n\n2.3.2 Merging\nNow at this point we may be happy with changes we‚Äôve made to both branches and want to incorporate them. For this we use git merge. If we assume master is going to be our main branch, we probably want to incorporate our changes there.\nSo git checkout master to ensure we‚Äôre there, and git merge goodbye. This should drop you into whatever text editor you‚Äôve specified (if it‚Äôs vim, see last chapter) to write a message about the merge.\nGit is smart enough that it can figure out our changes don‚Äôt conflict with each other at the level of text, so it brings them all together. This doesn‚Äôt rule out a merge breaking functionality at some higher level.\nWhen changes cannot be trivially merged, git will ask you to edit the conflicting files.\nIf you‚Äôve merged a branch and you‚Äôre done with it, you can delete it (not the commit history, just the pointer).\ngit branch -d goodbye",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#branching-and-remotes",
    "href": "01_2_moreGit.html#branching-and-remotes",
    "title": "2¬† More Git",
    "section": "2.4 Branching and Remotes",
    "text": "2.4 Branching and Remotes\nWith remotes, things get a little more complicated. In the above example, we created a branch goodbye, made edits and then deleted the branch pointer without ever pushing the changes to the remote repository. goodbye never existed on the remote. If we push the changes now, the commit history will be recorded on the remote, but there hasn‚Äôt been and will not be a goodbye branch there.\nIf we want a branch to be represented on the remote (because we want collaborators to see it or work on it or we want to back it up), we need to add it there. When we do that, two things happen.\n\nWe create the branch on the remote (and push our commit history there).\nWe create another branch locally, called a remote tracking branch.\n\nLet‚Äôs see how this goes.\nCreate a new branch and check it out.\ngit branch congratulations\ngit checkout congratulations\nMake an edit.\ncat &lt;&lt;EOF &gt;cw.sh\n#!/bin/bash\n\necho 'Congratulations World!'\nEOF\nNow git add and git commit. If we simply try to push the branch, git will push our changes on master, but not those on congratulations. Try it and check out your remote (on GitHub, ideally, at this point).\ngit push\nTo create the branch on the remote and the remote tracking branch locally:\ngit push -u origin congratulations\nWith output:\nCounting objects: 4, done.\nDelta compression using up to 8 threads.\nCompressing objects: 100% (2/2), done.\nWriting objects: 100% (3/3), 306 bytes | 0 bytes/s, done.\nTotal 3 (delta 1), reused 0 (delta 0)\nremote: Resolving deltas: 100% (1/1), completed with 1 local object.\nremote: \nremote: Create a pull request for 'congratulations' on GitHub by visiting:\nremote:      https://github.com/isg-certificate/newproject/pull/new/congratulations\nremote: \nTo git@github.com:isg-certificate/newproject.git\n * [new branch]      congratulations -&gt; congratulations\nBranch congratulations set up to track remote branch congratulations from origin.\nThe remote tracking branch keeps track of where in our commit history the remote was the last time we communicated with the remote. Above in the git log --decorate output we saw the branch (or reference, or pointer) origin/master indicating the commit associated with the remote tracking branch. We can see remote tracking branches with the -a flag in git branch\ngit branch -a -vv\n* congratulations                c6f675f [origin/congratulations] congratulations\n  master                         1c735ce [origin/master] Merge branch 'goodbye'\n  remotes/origin/congratulations c6f675f congratulations\n  remotes/origin/master          1c735ce Merge branch 'goodbye'\nNow that we‚Äôre tracking a branch that exists on our remote origin, it‚Äôs possible for the remote tracking branch and our local congratulations branch to diverge just like master and goodbye did above.\nThis can happen if work happens on the branches in two different places without pushing and pulling changes to and from the remote (such as when you work at two workstations, or a collaborator contributes). If the remote branch gets ahead of your tracking branch, git will not allow you to push new changes from your version of the branch before you get the changes from the remote and merge them.\nYou can update the tracking branch with git fetch and then you can do git merge to merge the tracking and local branches (fixing any conflicts as necessary) or you can do git pull, which updates the remote tracking branch and merges in one command (which will still require you to fix conflicts).",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#forking-and-pull-requests-on-github",
    "href": "01_2_moreGit.html#forking-and-pull-requests-on-github",
    "title": "2¬† More Git",
    "section": "2.5 Forking and pull requests on GitHub",
    "text": "2.5 Forking and pull requests on GitHub\nWe aren‚Äôt going to demo forking and pull requests here, because the key parts of that occur on the GitHub web site. But to put this in context: a single git repository can have branches that diverge and then merge again. When working with remotes, the complexity increases, especially when you have collaborators. Branches can exist in local repositories, but not the remote, and then be added to the remote (creating remote tracking branches in the local repository), and all these different branches can diverge and then be merged in various sequences.\nForking adds another layer of complexity. On GitHub you can fork someone else‚Äôs public repository. This is a bit like cloning it, except you are creating a mostly independent copy of it under your own account. You might create forks for a couple reasons:\n\nYou want to mess around with the repository and possibly adapt it to your own needs.\nYou found something you don‚Äôt like about the repository and you want to improve it, potentially sending your improvements back to the original developers for incorporation.\n\nIn both these cases, you probably don‚Äôt have permission to modify the original repository, or maybe your modifications would not be appropriate to its original purpose. You are likely not a close collaborator or co-worker if you are forking a repository, otherwise you would probably be able to simply clone the original and create branches to incorporate your work.\nOn your fork, you can do whatever you want. It‚Äôs your copy. You can‚Äôt accidentally push bad changes to the origina.\nIf you want to alter the original, however, you can create a pull request. This is a GitHub feature that allows you to notify the repository‚Äôs owner that you‚Äôve made a change you think should be incorporated into the original. The pull request creates a discussion thread that allows the owner to see what you‚Äôve proposed and talk about the changes, possibly asking you to tweak them.\nIn the end, the owner can merge the changes proposed from your fork in the original repository. Instead of doing this here, we‚Äôll have an assignment where students fork each other‚Äôs repositories and make pull requests.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#assorted-git-topics",
    "href": "01_2_moreGit.html#assorted-git-topics",
    "title": "2¬† More Git",
    "section": "2.6 Assorted Git Topics",
    "text": "2.6 Assorted Git Topics",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#visualizing-the-commit-history",
    "href": "01_2_moreGit.html#visualizing-the-commit-history",
    "title": "2¬† More Git",
    "section": "2.7 Visualizing the commit history",
    "text": "2.7 Visualizing the commit history\nThis has all been fairly abstract. If you think it might be helpful to be able to quickly visualize the history of a repository and the changes made to files, check out Sourcetree. You don‚Äôt have to connect it to your GitHub account, but you can if you want. To have a look at how it works, try cloning NF-Core‚Äôs rnaseq repository. This is a big repository with a lot of collaborators. If you want, you can make some arbitrary commits, branches, etc and see how they show up in the visualization of the history. Even though the GitHub repository is still the remote, you won‚Äôt have permission to push any changes.\n\n2.7.1 Ignoring files\nYou may want to git add --all sometimes when you have many changes to incorporate into a repository. However, it‚Äôs often the case in data analysis that as you develop your set of scripts and run them that you generate files you don‚Äôt want to commit, such as results and log files. You may also want to organize your project so that your data (or a symlink to your data) are inside the project directory to keep everything self-contained, and large raw data sets definitely don‚Äôt belong in a git repository.\nFor cases like these, you can create a file in the root of your working directory, .gitignore and write out file names and patterns there that you want git to ignore.\nCheck out the .gitignore file for this repository containing scripts for a bulk RNA-seq/differential expression tutorial.\nIt‚Äôs got a lot in it because these scripts all write results together, but briefly:\n-**/results matches any directory or file named results anywhere (and thus also ignores the contents of the directory) -*.fa matches any file ending in .fa\nThe documentation contains more details on pattern matching in .gitignore.\nYou can commit the .gitignore file to keep it as part of the repository.\n\n\n2.7.2 Removing and renaming\nYou can rm and mv just like you normally do, but it makes more work with Git. If you remove with git rm and rename (or move) with git mv Git will handle it better and stage the changes for you.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "01_2_moreGit.html#getting-help",
    "href": "01_2_moreGit.html#getting-help",
    "title": "2¬† More Git",
    "section": "2.8 Getting help",
    "text": "2.8 Getting help\nPlease reach out through the teams channel if you need help. The Pro Git book is really nicely written as well. Re-reading sections of it as you get your feet under you will be helpful.\nGetting help with Git is also an ideal use case for LLMs like chatGPT. If you‚Äôve run into an issue that you don‚Äôt understand, an LLM can give you suggestions (with an explanation) about how to fix it that you can implement and then immediately check for correctness. It‚Äôs much less risky than relying on an LLM for something conceptually more complicated and harder to validate the output for, like choosing a parameter in a statistical model that is appropriate for your data.\n\n2.8.1 Git is kind of hard\nWhile the basic ideas behind the Git workflow are not that challenging, when getting more involved in collaborations, working with remotes, and actually navigating the complexity of the command-line tools, things can get difficult. Depending on how and how much you use git, you should expect there to be a somewhat long learning curve. Don‚Äôt get discouraged, it‚Äôs a useful and widespread tool and great to have some experience with.",
    "crumbs": [
      "Introduction to Git",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>More Git</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html",
    "href": "02_1_qc.html",
    "title": "3¬† Variant calling - First Steps",
    "section": "",
    "text": "3.1 Learning Objectives\nIn this chapter, we‚Äôre going to introduce a basic outline of a variant detection workflow, and then work through the initial QC and mapping stages of the analysis. In the following chapters we‚Äôll cover actually calling variants and genotyping, filtering variants and evaluating quality, comparing variant call sets, and functional annotation.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#learning-objectives",
    "href": "02_1_qc.html#learning-objectives",
    "title": "3¬† Variant calling - First Steps",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw and mapped data specific to variant detection.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#the-basic-workflow",
    "href": "02_1_qc.html#the-basic-workflow",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.2 The basic workflow",
    "text": "3.2 The basic workflow\nOur workflow is going to be focused on calling short variants (SNPs/indels) for multiple samples against a single linear reference genome using short-read data. We‚Äôre not going to cover multiple whole genome alignment pan-genome graphs, or structural variant detection here, though those are really fun and interesting topics in bioinformatics.\nA typical reference-based variant detection workflow consists of the following steps:\n\nOrganizing your data resources and setting up your project structure (always the first step!).\nDoing basic QC on your input sequence data (and perhaps also your reference genome).\nIndexing your reference genome and aligning your sequence data to it.\nDoing QC on the alignments.\nCalling variants and genotyping.\nFiltering and evaluating the variant callset.\n\nIn this module, we will additionally look at ways to compare multiple variant callsets and functionally annotate them given a structural annotation of the genome.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#our-example-data",
    "href": "02_1_qc.html#our-example-data",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.3 Our example data",
    "text": "3.3 Our example data\nFor this module, we‚Äôre going to use data from the Genome-in-a-Bottle project which is created and housed in the US National Institute of Standards and Technology, a part of the Department of Commerce. NIST is devoted to the science of measurement and the development and use of standards. The GIAB data is created as part of an initiative to create benchmarking data to test methods of measuring genetic variation in humans.\nGIAB has a set of human samples they subject to every sequencing technology imaginable. They publish the data, along with products such as alignments and variant call sets for others to use. We‚Äôre going to use one set of three samples, a trio (mother, father, son) of Ashkenazi ancestry. For each sample, we‚Äôre going to download Illumina 2x250bp paired end sequence at about 50x coverage. In our scripts in this chapter, to make things run quickly, we‚Äôre going to cut the dataset down to look at only 5mb of chromosome 20. To make the QC steps interesting, this region overlaps a bit of messy sequence adjacent to the centromere.\nLinks to the data (and all the other sequencing data for these and other GIAB samples) can be found here.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#getting-set-up",
    "href": "02_1_qc.html#getting-set-up",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.4 Getting set up",
    "text": "3.4 Getting set up\nAssuming you‚Äôre working on UConn‚Äôs Xanadu (or Mantis) cluster, you should clone this github repository. It contains all the scripts that will be covered here. Each of them can be run in sequence to reproduce the results you‚Äôll see in this chapter. Please follow along, submitting them as you go. In the exercises for this module, you will be asked to modify and re-run the scripts and answer questions about what you find.\ngit clone git@github.com:isg-certificate/variants.git\nYou‚Äôll see initially that there is only a README.md file and a scripts directory containing scripts for the analysis. The scripts will build up more subdirectories as we go.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#getting-the-data",
    "href": "02_1_qc.html#getting-the-data",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.5 Getting the data",
    "text": "3.5 Getting the data\nIn the directory scripts/01_downloadData we have two scripts for downloading data. cd there now.\n\n3.5.1 The read data\nLet‚Äôs look at 01_getSequences.sh.\nTo make these examples run quickly, we‚Äôre going to grab only a subset of data from a small region of the genome. As you know at this point, fastq files straight off the sequencer are not ordered with respect to location in the genome, so doing this with truly raw data would be a challenge. GIAB provides genome-aligned BAM files, however, so we can do this relatively easily.\nLoad up our software modules:\nmodule load samtools/1.12 \nmodule load bedtools/2.29.0\nWe worked with samtools in ISG5311, so we won‚Äôt cover it again here. bedtools is new however. It‚Äôs a toolkit that allows the creation and manipulation of data in genomic windows. We‚Äôre going to elide the details in this section and cover it more depth shortly.\nTo grab the data (for one sample):\nSON='https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/data/AshkenazimTrio/HG002_NA24385_son/NIST_Illumina_2x250bps/novoalign_bams/HG002.GRCh38.2x250.bam'\n\nsamtools view -uh $SON chr20:29400000-34400000 | \\\nsamtools sort -n - | \\\nbedtools bamtofastq -i /dev/stdin/ -fq $OUTDIR/son.1.fq -fq2 $OUTDIR/son.2.fq\nWe first set a variable pointing to the bam file for the son. Then we use samtools to extract region chr20:29400000-34400000 directly from the file on the server. This is a pretty nice feature of samtools. As along as the .bai index is also on the server, this approach will work great.\nNext we pipe the data to samtools sort -n to sort the reads by name instead of position.\nFinally, we‚Äôre sending the name-sorted output to bedtools bamtofastq, which will split the paired-end reads into separate files, reconstituting something like our raw data file, with a bunch of caveats. We could equally well have used samtools fastq to accomplish this.\nWe do this for all three samples (we could have done it in parallel, or used an array job as well). The last step of the script gzip compresses all the fastq files. So if we check out output directory:\n$ ll ../../data/\ntotal 765M\n-rw-r--r-- 1 nreid cbc 114M Feb  2 10:56 dad.1.fq.gz\n-rw-r--r-- 1 nreid cbc 125M Feb  2 10:56 dad.2.fq.gz\n-rw-r--r-- 1 nreid cbc 123M Feb  2 10:55 mom.1.fq.gz\n-rw-r--r-- 1 nreid cbc 134M Feb  2 10:55 mom.2.fq.gz\n-rw-r--r-- 1 nreid cbc 131M Feb  2 10:53 son.1.fq.gz\n-rw-r--r-- 1 nreid cbc 141M Feb  2 10:53 son.2.fq.gz\nYou should always check the .err. and .out files to make sure you script ran successfully. In this case you‚Äôre going to see a lot of warnings produced by bedtools that look like this:\n*****WARNING: Query D00360:94:H2YT5BCXX:1:1102:3248:71588 is marked as paired, but its mate does not occur next to it in your BAM file.  Skipping. \nCan you guess why these warnings occur?\n\n\n3.5.2 The reference genome\nNow let‚Äôs look at 02_getGenome.sh. This is very straightforward:\n# this is modified version of GRCh38 that corrects some errors. GIAB uses this for their benchmarking work. \n    # \"with masked false duplications and contaminations, as well as decoy sequences from CHM13, which we are now using for GIAB analyses\"\n    #  https://genomebiology.biomedcentral.com/articles/10.1186/s13059-023-02863-7\nwget https://ftp-trace.ncbi.nlm.nih.gov/ReferenceSamples/giab/release/references/GRCh38/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.gz\ngunzip *gz\nThe human genome is 3.1 gigabases. That‚Äôs decently large (though it‚Äôs no axolotl). You could run something like QUAST on it to get some basic stats. When it‚Äôs your first time working with a given genome, it‚Äôs good to get to know it a bit. We won‚Äôt do that here though, we‚Äôll cover evaluating genome quality in the next module.\nThere are lots of human reference genome versions out there. GRCh38 was the state of the art until the telomere-to-telomere (CHM13) genome was released. Within GRCh38 there are multiple versions as well. We‚Äôre using GRCh38, and this very particular version, because GIAB has made some corrections to it (they do not alter the coordinate system, so GRCh38 annotations should work fine with it) and because it‚Äôs the one they use in their analyses. T2T is more complete and correct, however. Problems with a multiplicity of confusing genome versions are typically only a problem in model systems, and most of all in human.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#initial-qc",
    "href": "02_1_qc.html#initial-qc",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.6 Initial QC",
    "text": "3.6 Initial QC\nWe‚Äôve finished downloading our data, so let‚Äôs move to the directory scripts/02_qc.\n\n3.6.1 fastqc/multiqc on raw data\nWe have seen fastqc and multiqc used with RNA-seq data, so we won‚Äôt belabor this. But we‚Äôre going to run them on all our fastq files. Look at the script 01_fastqcRaw.sh:\n# run fastqc. \"*fq\" tells it to run on the illumina fastq files in directory \"data/\"\nfastqc -t 6 -o $OUTDIR ../../data/*1.fq.gz\nfastqc -t 6 -o $OUTDIR ../../data/*2.fq.gz\n\n# run on fastqc output\nmultiqc -f -o $OUTDIR/multiqc $OUTDIR\nCheck your outputs! There are some notable things here:\n\nThe sequence quality drops considerably at the end of these 250bp PE reads. It‚Äôs normal for sequence quality to drop. We‚Äôre going to trim a bit so it will increase.\nWe have a super-weird per-sequence GC content distribution! The bimodal shape is because we have deliberately pulled down reads from a region that is part run-of-the-mill genome sequence, and part really messy sequence next to the centromere. These two subregions of our 5mb window apparently have substantially different GC content.\nWe have a little bit of adapter contamination. It‚Äôs not so much that you would be super worried, but we‚Äôre going to trim it out anyway.\n\n\n\n3.6.2 Trimmomatic\nWe‚Äôve seen Trimmomatic before, but here‚Äôs the call, from a SLURM array script 02_trimmomatic.sh:\n# adapters to trim out\nADAPTERS=/isg/shared/apps/Trimmomatic/0.39/adapters/TruSeq3-PE-2.fa\n\n# sample bash array\nSAMPLELIST=(son mom dad)\n\n# run trimmomatic\n\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\njava -jar $Trimmomatic PE -threads 4 \\\n        ${INDIR}/${SAMPLE}.1.fq.gz \\\n        ${INDIR}/${SAMPLE}.2.fq.gz \\\n        ${TRIMDIR}/${SAMPLE}_trim.1.fq.gz ${TRIMDIR}/${SAMPLE}_trim_orphans.1.fq.gz \\\n        ${TRIMDIR}/${SAMPLE}_trim.2.fq.gz ${TRIMDIR}/${SAMPLE}_trim_orphans.2.fq.gz \\\n        ILLUMINACLIP:\"${ADAPTERS}\":2:30:10 \\\n        SLIDINGWINDOW:4:15 MINLEN:45\nShould be pretty standard at this point. In the .err files we should see trimming didn‚Äôt have a huge impact, e.g.:\nInput Read Pairs: 719326 Both Surviving: 708775 (98.53%) Forward Only Surviving: 9828 (1.37%) Reverse Only Surviving: 692 (0.10%) Dropped: 31 (0.00%)\nIf we look at our output directory:\nll ../../results/02_qc/trimmed_fastq/\ntotal 706M\n-rw-r--r-- 1 nreid cbc 108M Feb  2 11:19 dad_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 111M Feb  2 11:19 dad_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.3M Feb  2 11:19 dad_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 117K Feb  2 11:19 dad_trim_orphans.2.fq.gz\n-rw-r--r-- 1 nreid cbc 116M Feb  2 11:20 mom_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 120M Feb  2 11:20 mom_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.5M Feb  2 11:20 mom_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 122K Feb  2 11:20 mom_trim_orphans.2.fq.gz\n-rw-r--r-- 1 nreid cbc 123M Feb  2 11:20 son_trim.1.fq.gz\n-rw-r--r-- 1 nreid cbc 126M Feb  2 11:20 son_trim.2.fq.gz\n-rw-r--r-- 1 nreid cbc 1.5M Feb  2 11:20 son_trim_orphans.1.fq.gz\n-rw-r--r-- 1 nreid cbc 131K Feb  2 11:20 son_trim_orphans.2.fq.gz\nNote that trimmomatic does produce ‚Äúorphans‚Äù files with singletons whose mate pairs did not survive trimming. We‚Äôre going to ignore those going forward.\n\n\n3.6.3 fastqc/multiqc on trimmed data\nThe script 03_fastqcTrimmed.sh is essentially the same as above, but pointed at the trimmmed fastq files. We see improved overall sequence quality, and our minor residual adapter contamination is gone. We still very much have the bimodal GC distribution though!",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#alignment",
    "href": "02_1_qc.html#alignment",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.7 Alignment",
    "text": "3.7 Alignment\nIn ISG5311 we used a spliced aligner, HISAT2 to align our RNA-seq reads to a reference genome. Here we‚Äôre going to use bwa-mem2, an extremely popular alignment program for aligning DNA. It is perfectly happy to do ‚Äúsplit-read‚Äù alignments, where chunks of a read are aligned non-contiguously. These are in principle quite similar to spliced alignments, but instead of having a large splice in the CIGAR string of the SAM record (e.g.¬†21M1552N80M with 552N indicating a 552bp intron), if bwa-mem2 determines a read should be split-aligned, it will output multiple alignment records, one for each aligned chunk. The first will be the primary alignment, and the rest will be flagged as secondary in the FLAG column of the SAM file.\nOur alignment scripts are located in scripts/03_Alignment. cd over there right now.\n\n3.7.1 Indexing\nAs always, when using a short-read aligner, the first step is to index the reference genome. We do this with the script 01_bwaIndex.sh. It‚Äôs very straightforward:\nbwa-mem2 index \\\n   -p $INDEXDIR/GRCh38 \\\n   $GENOME\nWe assign a prefix GRCh38 to be used for our index files, and that‚Äôs what we use to invoke the index later. The index files can be found in the results directory:\n$ ll ../../results/03_Alignment/bwa_index/\ntotal 16G\n-rw-r--r-- 1 nreid cbc 5.8G Feb  2 11:46 GRCh38.0123\n-rw-r--r-- 1 nreid cbc  18K Feb  2 11:44 GRCh38.amb\n-rw-r--r-- 1 nreid cbc 9.9K Feb  2 11:44 GRCh38.ann\n-rw-r--r-- 1 nreid cbc 9.4G Feb  2 12:34 GRCh38.bwt.2bit.64\n-rw-r--r-- 1 nreid cbc 740M Feb  2 11:44 GRCh38.pac\nNote that indexing can use significant amounts of memory (depending on the size of the genome) and take some time. For this job the SLURM seff output looks like this:\nJob ID: 8839080\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 8\nCPU Utilized: 00:50:12\nCPU Efficiency: 12.45% of 06:43:12 core-walltime\nJob Wall-clock time: 00:50:24\nMemory Utilized: 69.31 GB\nMemory Efficiency: 86.64% of 80.00 GB\nSo this human genome required around 70G of memory and 50 minutes to run. In the script it was allowed 8 cpus, but only used one of them, and apparently has no options to specify number of cpus/threads. So we over-requested resources on this. Not a big in this case.\n\n\n3.7.2 Alignment\nNow that we‚Äôve got our data all QC‚Äôed and our genome indexed, we‚Äôre ready to align. At this stage the workflow starts to diverge a little bit more from the RNA-seq of last semester. We have a couple new points to cover here.\nLet‚Äôs look at our script 02_bwaAlign.sh. It is another array script.\n\n3.7.2.1 Read group ID\nThe beginning is pretty standard stuff. The first weird thing we notice is the creation of the variable RG.\n# sample ID list\nSAMPLELIST=(son dad mom)\n\n# extract one sample ID\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\n# create read group string\nRG=$(echo \\@RG\\\\tID:$SAMPLE\\\\tSM:$SAMPLE)\nIn that line we populate the variable with, for the son, @RG\\tID:son\\tSM:son. This will ultimately be parsed as @RG     ID:son  SM:son.\nWe‚Äôre going to provide one of these strings to our aligner for each sample. It‚Äôs known as the read group string. This allows every read in a file to be tagged with some metadata. Different groups of reads can be kept in the same SAM/BAM file and keep a distinguishing identifier. There are a few bits of metadata that can be attached, most importantly for our use-case here is the sample tag SM, which is son in this case.\nRead group IDs must be attached for variant callers to correctly process the reads downstream. If you fail to attach them during the alignment step, you can add them later (e.g.¬†using PICARD), but you‚Äôll create a whole new BAM file and have to delete the old one. Very inefficient.\nThe line @RG     ID:son  SM:son will be added the the SAM header, and then for each alignment record (line in the SAM/BAM file), a tag is added: RG:Z:son. The ID tag is a short identifier for the whole read group.\n\n\n3.7.2.2 The alignment pipe\nNow we arrive at the actual alignment, which should be mostly familiar:\nbwa-mem2 mem -t 7 -R ${RG} ${INDEX} ${SAMPDIR}/${SAMPLE}_trim.1.fq.gz $SAMPDIR/${SAMPLE}_trim.2.fq.gz | \\\n    samblaster | \\\n    samtools view -S -h -u - | \\\n    samtools sort -T ${OUTDIR}/${SAMPLE}.temp -O BAM &gt;$OUTDIR/${SAMPLE}.bam \nWe provide our reads, the index, the read group string and sort and compress the output, just as we did with RNA-seq reads.\n\n\n3.7.2.3 Marking duplicates\nThere is one new step in there: samblaster. We are using this program to mark duplicate read pairs. In variant calling (and many other applications) we want to assume that each fragment of sequenced DNA derives from an independent original biological molecule. When we sequence reads covering a heterozygous site, our variant calling model assumes the alleles are sampled in proportion to their true frequency in the pool of DNA extracted from the organism.\nMany library preparation protocols, however, involve the use of the polymerase chain reaction, which creates copies of molecules. These copies can distort the apparent frequencies of alleles. It‚Äôs also possible for there to be ‚Äúoptical‚Äù duplicates, which are artifacts of the sequencer, not the library preparation.\nsamblaster identifies putative duplicates and marks them in the FLAG column of their SAM records. Downstream tools will (typically by default) ignore these duplicate reads.\nRead pairs are identified as duplicates when both members of the pair share the same alignment as another pair.\n\n\n\n3.7.3 Alignment indexing\nFinally, we‚Äôre going to align the sorted, compressed BAM file:\nsamtools index ${OUTDIR}/${SAMPLE}.bam\nThis index allows fast access to reads from any part of the reference genome.\nAnd Finally we‚Äôve got this in our results directory:\nll ../../results/03_Alignment/bwa_align/\ntotal 612M\n-rw-r--r-- 1 nreid cbc 190M Feb  2 12:49 dad.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:50 dad.bam.bai\n-rw-r--r-- 1 nreid cbc 202M Feb  2 12:49 mom.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:49 mom.bam.bai\n-rw-r--r-- 1 nreid cbc 216M Feb  2 12:49 son.bam\n-rw-r--r-- 1 nreid cbc 1.6M Feb  2 12:49 son.bam.bai",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_1_qc.html#alignment-qc",
    "href": "02_1_qc.html#alignment-qc",
    "title": "3¬† Variant calling - First Steps",
    "section": "3.8 Alignment QC",
    "text": "3.8 Alignment QC\nAt this point it‚Äôs good practice to learn a bit about your alignment files. There are a few scripts in scripts/04_alignQC.\n\n3.8.1 samtools stats\nA straightforward first step is to summarize some aspects of the alignment. We can do that with samtools stats. It‚Äôs easy to run and can be summarized across samples using MultiQC. The script 01_samstats.sh does this. It also includes some bash code for pulling out the ‚Äúsummary numbers‚Äù table from the output for each sample and putting it into a table. That looks like this:\n\n\n\n\n\n\n\n\n\nMetric\ndad\nmom\nson\n\n\n\n\nRaw total sequences\n1308084\n1417550\n1505512\n\n\nFiltered sequences\n0\n0\n0\n\n\nSequences\n1308084\n1417550\n1505512\n\n\nIs sorted\n1\n1\n1\n\n\n1st fragments\n654042\n708775\n752756\n\n\nLast fragments\n654042\n708775\n752756\n\n\nReads mapped\n1307653\n1417146\n1505108\n\n\nReads mapped and paired\n1307222\n1416742\n1504704\n\n\nReads unmapped\n431\n404\n404\n\n\nReads properly paired\n1281272\n1392498\n1470964\n\n\nReads paired\n1308084\n1417550\n1505512\n\n\nReads duplicated\n19212\n39831\n39867\n\n\nReads MQ0\n18822\n19495\n22938\n\n\nReads QC failed\n0\n0\n0\n\n\nNon-primary alignments\n0\n0\n0\n\n\nSupplementary alignments\n11446\n10429\n14171\n\n\nTotal length\n304366705\n332326447\n353260305\n\n\nTotal first fragment length\n156799604\n170401850\n180976062\n\n\nTotal last fragment length\n147567101\n161924597\n172284243\n\n\nBases mapped\n304320288\n332284625\n353218188\n\n\nBases mapped (cigar)\n303309452\n331333643\n352041382\n\n\nBases trimmed\n0\n0\n0\n\n\nBases duplicated\n4487160\n9392133\n9387993\n\n\nMismatches\n2968410\n2783598\n3624412\n\n\nError rate\n9.79e-03\n8.40e-03\n1.03e-02\n\n\nAverage length\n232\n234\n234\n\n\nAverage first fragment length\n240\n240\n240\n\n\nAverage last fragment length\n226\n228\n229\n\n\nMaximum length\n250\n250\n250\n\n\nMaximum first fragment length\n250\n250\n250\n\n\nMaximum last fragment length\n250\n250\n250\n\n\nAverage quality\n36.7\n36.8\n36.8\n\n\nInsert size average\n404.2\n409.4\n419.2\n\n\nInsert size standard deviation\n117.1\n88.5\n187.1\n\n\nInward oriented pairs\n633663\n690440\n730889\n\n\nOutward oriented pairs\n14307\n11466\n14481\n\n\nPairs with other orientation\n659\n760\n754\n\n\nPairs on different chromosomes\n4982\n5705\n6228\n\n\nPercentage of properly paired reads (%)\n98.0\n98.2\n97.7\n\n\n\nThat‚Äôs a lot of data, especially when you‚Äôve got many samples, so it‚Äôs often a good idea to read this information into R and make some plots that can help you understand what‚Äôs going.\nThere are a few bits that are important to pay attention to:\n\nThe mapping rate ‚Äúreads mapped‚Äù / ‚ÄúRaw total sequences‚Äù: This tells you how many of your reads actually mapped to the reference genome! In this case the rate should be extremely high because we only grabbed mapped reads in the first place. Unmapped reads may be attributable to GIAB‚Äôs alignment having been produced by a different aligner.\nReads MQ0: These are reads that have been mapped, but assigned a mapping quality of 0, indicating the mapper has no confidence the alignment location is correct.\nReads duplicated: This is the number of duplicate reads mentioned above. Ideally this will be very low, but there is no ‚Äúcorrect‚Äù percentage. Observed duplication rates depend heavily on library prep, input DNA, and sequencing depth.\nPercentage of properly paired reads: This is the number of read pairs with the expected relative orientation and that are not too far apart / on separate reference sequences. This number tends to be larger when divergence between sample and reference is high and there is structural variation.\nError rate: this is the number of mismatches in mapped reads divided by the number of bases mapped. This encompasses both sequencing errors and true variation. When you have lots of samples you can get a sense of what this value should be in your study, and outliers with high error rates can indicate problematic samples.\n\n\n\n3.8.2 Looking at coverage\nAnother really important thing to do at this stage is to look at the distribution of depth of coverage across your genome. As you are learning in ISG5302, strong deviations from expected coverage can point to genomic regions with alignment issues. In those regions, variant calls are likely to be highly unreliable. You will want to know about those regions so that you can filter them out later (or even during variant calling) or at least treat them with a lot of caution.\nThere are lots of ways you can do this. We are going to write our own little pipe to calculate per-base coverage summed across all three samples in 1kb windows (see mosdepth for a fast alternative. This will be a helpful introduction to bedtools as well.\nTo give a brief overview: We will first use bedtools to create a BED file defining the genomic windows over which we want to calculate coverage. Then, in a single pipe, we‚Äôre going to merge our three BAM files into a single stream, filter it a little bit, and pass it to samtools depth, which will output the depth of coverage at each base in the genome. We will pipe that stream to awk to reformat it into BED format, and then pipe that stream to bedtools map, also passing in our 1kb window file. This tool will allow us to summarize (mean and median) the per-base coverage in 1kb windows. We use 1kb windows because the resulting file is a lot smaller and easier to manage (3.1 million lines) than the raw per-base coverage (3.1 billion lines).\nTo create the window file we do the following:\n# create faidx genome index file\nGENOME=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\nFAI=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta.fai\nsamtools faidx ${GENOME}\n\n# make a \"genome\" file, required by bedtools makewindows command, set variable for location\nGFILE=${OUTDIR}/GRCh38.genome\ncut -f 1-2 $FAI &gt; $GFILE\n\n# make 1kb window bed file, set variable for location\nWIN1KB=${OUTDIR}/GRCh38_1kb.bed\nbedtools makewindows -g ${GFILE} -w 1000 &gt;${WIN1KB}\nbedtools makewindows wants a ‚Äúgenome‚Äù file, which is just a table with each sequence in the reference and its length, in order to create the window BED file.\nbedtools makewindows is then pretty straightforward -w is the window size. If you want the windows to be overlapping you can specify a ‚Äúslide‚Äù smaller than the window size with -s.\nTo refresh your memory about BED files: BED has only 3 required columns: the sequence identifier, the start position and the end position. The trick here is that whereas GTF is 1-based and fully closed, BED is 0-based and half-open. This means that in a GTF file, an interval of 1-1000 refers to the first 1000 bases, including the start and end point of the interval. In BED format, however, the first base in a sequence is numbered 0, and the end base in an interval is not included as part of the interval, making the comparable BED interval 0-1000. See this blog for a discussion about intervals in genomics.\nOk, so after creating our window file, we can now run our pipe:\nbamtools merge -list ${OUTDIR}/bam.list | \\\nbamtools filter -in - -mapQuality \"&gt;30\" -isDuplicate false -isProperPair true | \\\nsamtools depth -a /dev/stdin | \\\nawk '{OFS=\"\\t\"}{print $1,$2-1,$2,$3}' | \\\nbedtools map \\\n-a ${WIN1KB} \\\n-b stdin \\\n-c 4 -o mean,median,count \\\n-g ${GFILE} | \\\nbgzip &gt;${OUTDIR}/coverage_1kb.bed.gz\nWe use bamtools here to merge and filter our 3 samples (provided in a list). bamtools is older software at this point and no longer being updated, so this pipeline could use a refresh with samtools merge, but it will work for now.\nWe pipe the output to samtools depth -a. -a simply means output all sites, even if they are zero.\nThe output looks like this:\nchr20   29858643        83\nchr20   29858644        82\nchr20   29858645        83\nchr20   29858646        82\nchr20   29858647        84\nThe awk line reformats samtools depth output to BED format, putting the depth column in column 4:\nchr20   29858642   29858643        83\nchr20   29858643   29858644        82\nchr20   29858644   29858645        83\nchr20   29858645   29858646        82\nchr20   29858646   29858647        84\nFinally, we‚Äôve got bedtools map. This tool takes in intervals in BED format with argument -a, and maps the intervals provided using argument -b. You can do all kinds of operations on the mapped intervals. Here we are telling bedtools map to grab column 4 of argument -b (-c 4) and calculate the mean, median, and total number of records (-o mean,median,count) for column 4 in each -b interval overlapping a given interval from -a. The output looks like this:\nchr20   34387000        34388000        155.231 156     1000\nchr20   34388000        34389000        174.651 179     1000\nchr20   34389000        34390000        203.441 202     1000\nchr20   34390000        34391000        133.606 136     1000\nchr20   34391000        34392000        234.953 236     1000\nWhat can we do with this table? Let‚Äôs make some plots in R:\nFirst load up the tidyverse:\n\nlibrary(tidyverse)\n\nRead in the table and filter down to just our target region:\n\ncov &lt;- read.table(\"variants/results/04_alignQC/coverage/coverage_1kb.bed.gz\", header=FALSE) %&gt;%\n  filter(V1==\"chr20\", V2 &gt;= 29400000 & V3 &lt;= 34400000)\ncolnames(cov) &lt;- c(\"chromosome\", \"start\", \"end\", \"mean\", \"median\", \"count\")\n\n# some stats read in as character data, fix that:\ncov[, 4] &lt;- as.numeric(cov[,4])\ncov[, 5] &lt;- as.numeric(cov[,5])\n\nFirst, let‚Äôs ask what the median (of medians, of course) coverage is across windows:\n\nmedian(cov$median)\n\n[1] 185\n\n\nNow let‚Äôs plot coverage over the region:\n\nggplot(cov, aes(x=start, y=mean)) + \n  geom_point(size=0.2)\n\n\n\n\n\n\n\n\nWell, we‚Äôve got some serious outliers here. We expect in the neighborhood of 50x coverage for each sample, so around 150x for all three. This plot goes up past 6000x coverage for a few points. That extreme of a coverage spike is guarantee there is mismapping in that spot.\nLet‚Äôs truncate the y-axis to get a closer look at the rest of the points:\n\nggplot(cov, aes(x=start, y=mean)) + \n  geom_point(size=0.2) + \n  ylim(0,500)\n\n\n\n\n\n\n\n\nAt a smaller scale we can see a little better the problematic coverage in left 30-40% of this region. This region is centromeric (or at least centromere-adjacent), and there are problems with the assembly because of the repetitiveness and low complexity of the sequence.\nYou can see spots where the coverage drops to zero. Some of those are actually gaps, others may be locations where no reads map with MAPQ &gt; 30. You can also see other spikes in coverage. You can explore this region of genome with the UCSC genome browser.\nLater on we‚Äôre going to want to either exclude some of these regions from variant calling, or filter them out afterward.\nThere aren‚Äôt really great ways of setting coverage thresholds for filtering. We usually just eyeball a graph and set some thresholds. Let‚Äôs say 90 and 260.\n\nggplot(cov, aes(x=mean)) + \n  geom_histogram(binwidth=10) +\n  xlim(0,500) + \n  geom_vline(xintercept=c(90, 260), color=\"red\", linetype=\"dashed\", size=1)\n\n\n\n\n\n\n\n\nHow many windows would we lose out of our 5000?\n\nsum(cov$mean &lt; 90 | cov$mean &gt; 260)\n\n[1] 645\n\n\nOrdinarily that would seem like a lot, but we‚Äôve deliberately chosen a problematic genomic region here.\nLet‚Äôs replot:\n\ncov &lt;- mutate(cov, \n              exclude = cov$mean &lt; 90 | cov$mean &gt; 260)\n\nggplot(cov, aes(x=start, y=mean, color=exclude)) + \n  geom_point(size=0.2) + \n  ylim(0,500)\n\n\n\n\n\n\n\n\nIt‚Äôs pretty clear we will be able to exclude some of the most aberrant regions with these thresholds. We could either use these thresholds to filter individual variants after the fact, or use them to identify regions to filter out either during or after variant calling by creating a bed file that merged together windows to include or exclude. We will cover that in the next chapter.\nJust for fun, we‚Äôve got a script 03_bedtoolsNuc.sh that uses bedtools to calculate the base content in windows. It‚Äôs a simple script, so we won‚Äôt review it here, but have a look and run it. We can load up the output:\n\n# read in the table and filter down the output to focal region\nnuc &lt;- read.table(\"variants/results/04_alignQC/bedtoolsnuc/nuc.bed.gz\", header=FALSE) %&gt;%\n    filter(V1==\"chr20\", V2 &gt;= 29400000 & V3 &lt;= 34400000)\ncolnames(nuc) &lt;- c(\"chromosome\", \"start\", \"end\", \"ATpct\", \"GCpct\", \"A\", \"C\", \"G\", \"T\", \"N\", \"other\", \"length\")\n\nLet‚Äôs look at the GC percentage:\n\nggplot(nuc, aes(x=start, y=GCpct)) + \n  geom_point(size=0.2)\n\n\n\n\n\n\n\n\nWe certainly have some wacky things going on with the GC content, especially in the left-most part of the plot and they are associated with aberrations in coverage. The spots where GC content plunges to zero are gaps in the assembly (you will see this if you plot the number of N‚Äôs in each window‚Äìcolumn 10).\nWe will leave off here for now. In the next chapter we‚Äôll tackle variant calling itself.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Variant calling - First Steps</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html",
    "href": "02_2_variantCalling.html",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "4.1 Learning Objectives\nIn this section we‚Äôre going to take our qc‚Äôed and aligned data and call variants using a couple different methods. We‚Äôll demonstrate freebayes and bcftools, which do multi-sample variant calling, and GATK which can do basic multi-sample variant calling, but we‚Äôll walk through the step-wise procedure for calling variants on multiple samples because it‚Äôs a great way of doing variant calling when your set of samples could grow over time. We‚Äôll also see a basic split-apply-combine approach to variant calling. We‚Äôll also introduce the VCF format and some tools we can use to manipulate it.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#learning-objectives",
    "href": "02_2_variantCalling.html#learning-objectives",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "Learning Objectives:\n\n\nIdentify variants and genotype samples against a reference genome.\n\n\nManipulate the VCF format.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#multi-sample-variant-calling",
    "href": "02_2_variantCalling.html#multi-sample-variant-calling",
    "title": "4¬† Variant Calling",
    "section": "4.2 Multi-sample variant calling",
    "text": "4.2 Multi-sample variant calling\nWhen you have more than one sample (typical when variant calling) you want to combine the samples together so they can be called simultaneously rather than calling variants on a per-sample basis and then combining them. This has a couple benefits:\n\nCombining the data increases statistical power. When alleles are shared by more than one individual, pooling the data adds more evidence that they are real (rare alleles always remain a relative challenge).\nCalling variants across samples unifies representation of complex variants. Some variants have more than one possible representation and calling samples together ensures representation is consistent across samples. Representation issues arise most often with complex variants involving multiple indels or mixtures of SNPs and indels and/or when you have multiple haplotypes. If you called variants on each sample separately with the intent to combine them later, you could wind up with the same alleles being represented differently, which could be a hindrance.\nHomozygous reference genotypes (at variable sites) are accurately called The output of variant calling algorithms typically only outputs variant records at sites with alternate alleles (ones that differ from the reference genome) and does not distinguish sites with no data (or not enough evidence to make a call) from sites where there is good evidence for a genotype that is homozygous for the reference allele. If you do single sample calling and combine later, your site x genotype matrix will be splattered with missing data and you won‚Äôt really know if those missing genotypes are ref/ref genotypes or actually missing data and it won‚Äôt be safe to assume either way.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#freebayes",
    "href": "02_2_variantCalling.html#freebayes",
    "title": "4¬† Variant Calling",
    "section": "4.3 freebayes",
    "text": "4.3 freebayes\nFreebayes is a popular variant caller. It uses a Bayesian model that accounts for many different features of the mapped data in trying to distinguish errors from true variation. It has a notably simple in application in that it does not require pre-processing or filters to be applied to the read data and produces output in a single step. As the authors note in the github readme, freebayes adheres to the Unix philosophy of creating modular tools that can read from stdin and write to stdout and be chained together by users into pipelines that fit their specific needs. This is a really nice feature that makes it easy to work with.\n\n4.3.1 freebayes and short haplotypes\nA major distinguishing feature of freebayes is that it outputs short haplotype variants. Working with haplotypes improves the sensitivity and specificity of variant calling, but as output, they can be useful or annoying, depending on your downstream application.\nConsider the codon TTT, which codes for phenyalanine. Now imagine you have a reference sequence with a protein containing TTT at some position and you have sequenced a diploid sample that is heterozygous TTT/TAA. If your variant caller outputs short haplotypes, you will see the genotype TT/AA and easily recognize that one is the reference sequence and the other produces a premature stop codon. If, like most others, your variant caller outputs two heterozygous SNPs T/A and T/A and no phasing information, you will have no way of knowing (without going back to the data) if you have the haplotypes TT/AA, producing phenylalanine/stop codon or TA/AT, producing leucine/tyrosine. Note that this is a general problem when we lack phasing information.\nAn example where these haplotypes can be a headache is in population genetics, where you might wish to calculate a statistic like pi, the mean pairwise divergence between a collection of sequences (phase is unimportant here). The short haplotypes introduce complexity into the calculations that simple SNPs avoid. .\nWe will look more at haplotypes and how to manage them later in this chapter in the section on variant normalization.\n\n\n4.3.2 Running freebayes\nAfter all that introduction, running freebayes is pretty simple. We will continue in the variants GitHub repository assuming you have completed chapter 2. Go to the directory scripts/05_variantCalling and look at script 01_freebayes.sh.\nWe‚Äôre going to provide freebayes with a list of bam files (a useful feature when you‚Äôve got tons of samples), so we‚Äôll create that first:\n# make a list of bam files\nls ${INDIR}/*.bam &gt;${INDIR}/bam_list.txt\nThen we can run the program, using a variable $GEN to hold the genome location and clean up the command line:\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# run freebayes\nfreebayes \\\n-f ${GEN} \\\n--bam-list ${INDIR}/bam_list.txt |\nbgzip -c &gt;${OUTDIR}/freebayes.vcf.gz\n\ntabix -p vcf ${OUTDIR}/freebayes.vcf.gz\n\n4.3.2.1 bgzip and tabix\nBy default freebayes will write results to the standard output in uncompressed format. We‚Äôre going to pipe that to bgzip to compress it and then index the compressed file with tabix. These are both part of the htslib project, which also includes samtools and bcftools (more on bcftools in a moment).\nbgzip is a variant of gzip: ‚Äúblock gzip‚Äù. It does a slightly modified gzip compression, but can still be read by gzip, gunzip, zgrep etc. The modification allows for easy indexing of position sorted tabular genomic data for fast access to data from any genomic region (just as with sorted, indexed bam files).\nOnce the file is bgzipped, we index with tabix. tabix can be used to index any bgzipped position sorted tabular file, and once indexed, retrieve data from the file. It has a few preset modes for BED, GFF, and VCF, but you can specify which columns contain the positional information so that any tabular format can be effectively indexed. Here we specify -p vcf because our output is in VCF format.\n\n\n4.3.2.2 A few other freebayes options\nIt‚Äôs worth skimming the github readme for freebayes. It has lots of information and isn‚Äôt hard to read. We‚Äôll highlight a few more useful options here.\n\nCalling variants in a specific region: You can limit freebayes to calling variants in a single genomic interval with the following flag -r chr20:29400000-34400000. It probably would have been wise to do that in our case, as we only really have usable data from that region (mismapped reads notwithstanding). You can also provide a BED file containing multiple target regions with -t targets.bed. We could have limited freebayes to calling variants only in windows within our coverage threshold this way. This would have sped things up a little, particularly as it would not have needed to churn through all the useless data in that region with 6000x coverage.\nSpecifying populations: freebayes puts a prior probability distribution on genotype frequencies that assumes that all samples in a run are drawn from a single population. If this is incorrect (it often is) this prior may increase the error rate for genotype calls (most likely by favoring heterozygote calls too much). If you have high coverage (as we do in the test data) this probably doesn‚Äôt matter too much, as the data will overwhelm the prior, but if your coverage is on the low end, it can cause real problems (particularly for very low pass sequencing). You can specify which samples belong to which populations with --populations FILE (see docs for details). You can turn off this prior altogether with -w.\nSpecifying genetic diversity: freebayes also puts a prior on the expected genetic diversity. By default this is 0.001. This is appropriate for human data, but won‚Äôt be correct for everything. Fundulus heteroclitus populations range in diversity from 0.008 to 0.02. Again, with high coverage data this prior will have less influence that with low. You can change it with -T &lt;expected diversity&gt;. You can turn off both this and the population prior with -k, a good idea if you have low coverage.\nSpecifying ploidy: To set default ploidy, or to provide ploidy (or copy number) by region, and even by sample, look at the options --ploidy and --cnv-map.\n\n\n\n4.3.2.3 Resource usage\nfreebayes is pretty lightweight. It can only use 1 cpu. The amount of memory required depends on the number of samples and the depth of coverage. For our 3 samples at ~50x coverage, we get the following seff output:\n$ seff 8846713\n\nJob ID: 8846713\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 01:03:01\nCPU Efficiency: 14.30% of 07:20:32 core-walltime\nJob Wall-clock time: 01:02:56\nMemory Utilized: 718.59 MB\nMemory Efficiency: 7.02% of 10.00 GB\nFor this run we considerably over-requested CPUs as we can only use 1. Memory is harder to predict and will increase with more samples. It took freebayes about an hour to process all of our data.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#bcftools",
    "href": "02_2_variantCalling.html#bcftools",
    "title": "4¬† Variant Calling",
    "section": "4.4 bcftools",
    "text": "4.4 bcftools\nbcftools is part of the htslib project mentioned above. It includes lots of tools for calling variants and for manipulating them downstream. Its variant calling pipeline is much, much faster than freebayes (or GATK, below), but it tends to produce slightly worse calls, mostly at loci where there is complex indel/snp variation. This comes down to the fact that it does not do haplotypic inference as freebayes does or local assembly as GATK does. Depending on your application, the speed and convenience gains may greatly outweigh the computational costs. It also has the benefit of being part of a large open-source project that is continuously developed and supported. Like freebayes, bcftools is very much developed around the Unix ethos of simple tools built for piping.\n\n4.4.1 Running bcftools\nCalling variants with bcftools actually involves two steps. First, a summary of the read pileup across samples is generated. This digests the raw mapped BAM file into a tabular format that tracks potential alternate alleles and their evidence in each sample. That ‚Äúpileup‚Äù file is then passed to a variant caller that evaluates the evidence and outputs variant calls and genotypes in VCF format.\nLet‚Äôs have a look at the script 02_bcftools.sh.\n# make a list of bam files\nls ${INDIR}/*.bam &gt;${INDIR}/bam_list.txt\n\n# set reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# call variants\nbcftools mpileup -f ${GEN} -b ${INDIR}/bam_list.txt -q 20 -Q 30 | bcftools call -m -v -Oz -o ${OUTDIR}/bcftools.vcf.gz \n\n# index vcf\ntabix -p vcf ${INDIR}/bcftools.vcf.gz\nAs above, we provide a list of bam files and the reference genome. We can do the full variant calling analysis by piping the two steps together bcftools mpileup which produces the summary of the evidence and bcftools call, which does the variant calling and genotyping.\nIn mpileup we have also provided input filters: -q 20 ignores all bases with base quality &lt; 20 and -Q 30 ignores all reads with mapping quality &lt; 30.\nIn call we provide some options: -m with uses the recommended ‚Äúmulti-allelic‚Äù caller, -v which outputs only variable sites, -Oz which outputs bgzip compressed variant calls and -o ${OUTDIR}/bcftools.vcf.gz to specify an output file name. We could also have let it write to stdout and redirected to a file (or piped the output to something else!).\n\n\n4.4.2 bcftools options\nWe‚Äôll keep this quick. bcftools has similar options to those mentioned above for freebayes. They can be found in the usage for mpileup and call. Note that there is no way to turn off the population priors, but you can provide a --group-samples file as with freebayes and put each sample in its own ‚Äúpopulation‚Äù.\n\n\n4.4.3 Resource usage\nbcftools is very lightweight. It finished in 1/10th of the time that freebayes did and used 1/4 of the memory.\nCores: 1\nCPU Utilized: 00:06:17\nCPU Efficiency: 99.74% of 00:06:18 core-walltime\nJob Wall-clock time: 00:06:18\nMemory Utilized: 164.06 MB\nMemory Efficiency: 3.20% of 5.00 GB",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#parallelizing-variant-calling",
    "href": "02_2_variantCalling.html#parallelizing-variant-calling",
    "title": "4¬† Variant Calling",
    "section": "4.5 Parallelizing variant calling",
    "text": "4.5 Parallelizing variant calling\nWe won‚Äôt do this here, but note that it is relatively straightforward to parallelize variant calling. The steps are:\n\nSpecify a set of windows within which to call variants.\nCall variants in each window separately (perhaps using GNU parallel or an array job).\nCombine the windowed variant calls, removing duplicate variants that overlap window edges.\n\nfreebayes distributes a script freebayes_parallel. You can find it here /isg/shared/apps/freebayes/1.3.4/freebayes-1.3.4/scripts/freebayes-parallel on Xanadu, or on github.\nIt‚Äôs a simple bash script. The meat of it is:\n\ncommand=(\"freebayes\" \"$@\")\n\n(\ncat \"$regionsfile\" | parallel -k -j \"$ncpus\" \"${command[@]}\" --region {}\n) | \nvcffirstheader |\nvcfstreamsort -w 1000 | \nvcfuniq \ncat $regionsfile pipes the genomic windows, however you define them, to parallel. The parallel option -k means process the input in order (and output it that way).-j gives a number of jobs to run simultaneously (one for each cpu in this case). Then you have the freebayes command line \"${command[@]}\" with --region {} appended to the end, so freebayes runs on just that region. All of this is wrapped in (), which typically groups commands into a single stdout stream. It‚Äôs not doing anything useful here, but it‚Äôs in the original script.\nThe output here is a single stream of VCF files\nThe next three commands process the output stream of variant calls. They are part of the vcflib suite. vcffirstheader retains the header from just the first VCF output (because these are parallel invocations of freebayes, each output VCF will have a header). vcfstreamsort sorts variants in a small window (1000 sites) to account for any weirdness due to duplicated variant calls at region edges. vcfuniq then removes any such duplicated variants.\nThis is handy for cases where the job can be usefully accelerated within a single node. Instead of using parallel, you could also break this out into an array job and spread the work over 100 (max number of jobs on Xanadu) simultaneous array tasks for the first step, and then in a second job combine the results.\nIf your independent project involves variant calling, you should really consider some version of this to speed things up. You can do it for any of the variant callers.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#two-step-variant-calling-with-gatk",
    "href": "02_2_variantCalling.html#two-step-variant-calling-with-gatk",
    "title": "4¬† Variant Calling",
    "section": "4.6 Two-step variant calling with gatk",
    "text": "4.6 Two-step variant calling with gatk\nThe last variant caller we‚Äôll demonstrate is the HaplotypeCaller module from the package GATK.\nGATK is a very large suite of tools centered mostly on variant calling. It‚Äôs developed at The Broad Institute, a large biomedical research institute focused on genomics, and affiliated with Harvard and M.I.T. GATK is one of the most widely used packages for variant calling and performs very well in tests. It has extensive documentation, and has published ‚Äúbest-practice‚Äù recommendations for how to use their software. Unfortunately, the best practices are often very complex 1, and sometimes require resources that are unavailable in model systems (e.g.¬†‚Äúgold-standard‚Äù variant call sets). In recent years the documentation has become out of date and hard to parse, with lots of broken links.\nWhile HaplotypeCaller can be run in one step just like freebayes, a cool feature of GATK is its ability to do stepwise joint calling of multiple samples. The advantage of this procedure arises mainly if you have a study where you are likely going to have sampling that increases over time. In this procedure, the heaviest computation is done first, and can be done independently for each sample. If you save the products of that computation, then when you get new samples, you only need to do the initial step for the new samples before combining all samples together. This can save lots of time over standard joint calling in which all the computation needs to be redone every time a new sample is obtained.\n\n4.6.1 Running the workflow\nThere are three key steps:\n\nRun HaplotypeCaller in GVCF mode on each sample. A GVCF is a modification of VCF format (which we haven‚Äôt covered yet). This is the heavy compute. It can be parallelized across samples and genomic regions if necessary.\nRun GenomicsDBImport to create a database of your samples (this runs quickly).\nRun GenotypeGVCFs to create joint genotype calls (this also runs quickly).\n\nLet‚Äôs look at the scripts.\n\n4.6.1.1 Creating the sequence dictionary\nWe need one helper file that we don‚Äôt already have, a sequence dictionary for the reference genome. We‚Äôre going to run 03_createDict.sh to create that. It uses Picard, a toolkit for manipulating genomes, alignment files, and variant calls.\n# load required software\nmodule load picard/2.23.9\n\n# input/output\nINDIR=../../results/03_Alignment/bwa_align/\n\nOUTDIR=../../results/05_variantCalling/gatk\nmkdir -p $OUTDIR\n\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# create required .dict file\njava -jar $PICARD CreateSequenceDictionary R=$GEN\nWe have seen Java programs run this way before, but let‚Äôs reinforce what‚Äôs happening. When we load the module, an environment variable is created that points at a java ‚Äújar‚Äù file.\n$ module load picard/2.23.9\n$ echo $PICARD\n/isg/shared/apps/picard/2.23.9/picard.jar\nWe start the program with java -jar $PICARD. We can modify the memory usage using command line options as we‚Äôll see in a later script.\n\n\n4.6.1.2 Generating the GVCF files\nIn our next step, we‚Äôll generate one GVCF file for each sample, using an array job. Look at the script 04_makeGVCFs.sh\nSAMPLELIST=(son dad mom)\nSAMPLE=${SAMPLELIST[$SLURM_ARRAY_TASK_ID]}\n\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\n# run haplotype caller on one sample\ngatk HaplotypeCaller \\\n     -R ${GEN} \\\n     -I ${INDIR}/${SAMPLE}.bam \\\n     -ERC GVCF \\\n     --output ${OUTDIR}/${SAMPLE}.g.vcf\nThe flag -ERC GVCF directs GATK to produce the GVCF file. ERC stands for ‚Äúemit reference confidence‚Äù. So these files are actually tracking whether or not there is good evidence for a homozygous reference genotype in the sample.\nAfter we‚Äôve run this we should see this in the results directory:\n$ ll ../../results/05_variantCalling/gatk/\n-rw-r--r-- 1 nreid cbc  29M Feb  5 16:50 dad.g.vcf\n-rw-r--r-- 1 nreid cbc  91K Feb  5 16:50 dad.g.vcf.idx\n-rw-r--r-- 1 nreid cbc  26M Feb  5 16:51 mom.g.vcf\n-rw-r--r-- 1 nreid cbc  82K Feb  5 16:51 mom.g.vcf.idx\n-rw-r--r-- 1 nreid cbc  25M Feb  5 16:55 son.g.vcf\n-rw-r--r-- 1 nreid cbc  80K Feb  5 16:55 son.g.vcf.idx\nThis step uses the most resources. For one of the three array tasks:\n$ seff 8847173_0\nJob ID: 8847174\nArray Job ID: 8847173_0\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 02:05:44\nCPU Efficiency: 23.89% of 08:46:17 core-walltime\nJob Wall-clock time: 01:15:11\nMemory Utilized: 13.30 GB\nMemory Efficiency: 66.51% of 20.00 GB\nSo in fact we used something like 3hrs 45min of wall time to run these three tasks. This probably would have been a bit faster if we had specified that it should only run on our focal 5mb region instead of crawling across the whole genome. In any case, it took more resources that freebayes and vastly more resources than bcftools.\n\n\n4.6.1.3 Creating the database\nIn the next step we‚Äôre going to create our database. This step can be done on at most one reference sequence at a time. So you must specify a region (in this case all of chr20) when you do this step, and you must do this step once for each sequence in your reference genome. So in some sense GATK encourages parallelism at this stage, though this step and the next are quick enough that it doesn‚Äôt seem to that important in practice. Have a look at the script 05_DBimport.sh.\n# make an \"arguments\" file to provide all samples\nfind ${INDIR} -name \"*g.vcf\" &gt;${INDIR}/args.txt\nsed -i 's/^/-V /' ${INDIR}/args.txt\n\n#IMPORTANT: The -Xmx value the tool is run with should be less than the total amount of physical memory available by at least a few GB, as the native TileDB library requires additional memory on top of the Java memory. Failure to leave enough memory for the native code can result in confusing error messages!\ngatk --java-options \"-Xmx10g -Xms4g\" GenomicsDBImport \\\n  --genomicsdb-workspace-path ${OUTDIR} \\\n  --overwrite-existing-genomicsdb-workspace true \\\n  -L chr20 \\\n  --arguments_file ${INDIR}/args.txt\nWe need to tell it which samples to use. You can specify them on the command line with -V sample1.g.vcf, but that gets tedious. So we‚Äôre going to provide a file with a list of arguments to append to the command line ‚Äúargs.txt‚Äù. It contains a -V flag for each GVCF file.\nWhen we run GenomicsDBImport we specify some java options: --java-options \"-Xmx10g -Xms4g\". Those give the maximum and minimum memory boundaries for this execution of the program. See the comment line saying we need to request more memory than the java option max from SLURM.\nIn this case we‚Äôre providing a workspace path, but also telling the program to overwrite any existing data there (helpful when you‚Äôre testing the code out and running it over and over again).\nWhen we finally run this, it goes quickly, taking only 13 minutes:\n$ seff 8847371\nJob ID: 8847371\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 00:00:57\nCPU Efficiency: 7.14% of 00:13:18 core-walltime\nJob Wall-clock time: 00:01:54\nMemory Utilized: 2.08 GB\nMemory Efficiency: 13.86% of 15.00 GB\n\n\n4.6.1.4 Generating the VCF file\nFinally we can run the script that actually generates our VCF output, 06_genotypeGVCFs.sh. Let‚Äôs have a look at it:\n# set a variable for the reference genome location\nGEN=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n\ngatk GenotypeGVCFs \\\n    -R ${GEN} \\\n    -V gendb://../../results/05_variantCalling/gatk/db \\\n    -O ${OUTDIR}/gatk.vcf \n\nbgzip ${OUTDIR}/gatk.vcf \ntabix -p vcf ${OUTDIR}/gatk.vcf.gz\nAt this point it‚Äôs very straightforward. Provide the reference genome, the location of the database and an output file name. Because the database is already region-restricted, we don‚Äôt need to specify a region.\nAfter we output the VCF file, we bgzip it and tabix index it.\nAgain, this step does not require much in the way of resources. It took less than 1 minute to process chr20.\n$ seff 8847490\nJob ID: 8847490\nCluster: xanadu\nUser/Group: nreid/cbc\nState: COMPLETED (exit code 0)\nNodes: 1\nCores per node: 7\nCPU Utilized: 00:00:49\nCPU Efficiency: 15.91% of 00:05:08 core-walltime\nJob Wall-clock time: 00:00:44\nMemory Utilized: 1.13 GB\nMemory Efficiency: 7.51% of 15.00 GB\n\n\n\n4.6.2 Options in the GATK approach\nIt is possible to split up the GVCF calling into subregions to speed it up, though it gets a little complex parallelizing across individuals and regions, requiring some work to organize.\nGATK also uses a population prior, but we don‚Äôt know how to turn it off or specify population groupings! You can change the expected genetic diversity, however, which by default is again set at 0.001 (this value is ubiquitous because it is approximately the value in humans).",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#what-about-aiml",
    "href": "02_2_variantCalling.html#what-about-aiml",
    "title": "4¬† Variant Calling",
    "section": "4.7 What about AI/ML!?!",
    "text": "4.7 What about AI/ML!?!\nThere is another big variant caller we are not covering in this course, but it is a pretty cool one: Google‚Äôs DeepVariant.\nDeepVariant uses machine learning rather than a probabilistic model to distinguish sequencing and mapping errors from true variation and genotype samples. In tests, it works well. The trick is that ML models like those used in DeepVariant need to be trained on a set of true variants. Ideally those true variants need to share similar characteristics with those in the data that will ultimately be analyzed.\nIn model systems (human, mouse, Drosophila) there are often resources available to train models, and the trained models may already be available (the one distributed with DeepVariant is trained on human data). Training is not computationally trivial and relies heavily on the quality of the input.\nIn non-model systems, good training data may not be available, or you may not have the time or expertise to do your own training. So the question becomes, will a variant detection model trained on another species do well on mine? The answer to that is‚Ä¶ maybe, or maybe not. It really depends on what features of the data the model has learned to associate with true variants and artifacts, and whether those are consistent with your species. For genetically diverse species, such as Fundulus heteroclitus, it‚Äôs highly likely that DeepVariant would do very poorly when applying a human-trained model. In humans, tight clusters of variants often signal false positives. In killifish, there is so much genetic diversity that all variants will look crowded together by comparison. This could conceivably cause problems.\nSee this blog post from google about this very question, suggesting that a human trained model didn‚Äôt do so well when run on mosquitoes, but species-specific training greatly improved the situation.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#the-vcf-format",
    "href": "02_2_variantCalling.html#the-vcf-format",
    "title": "4¬† Variant Calling",
    "section": "4.8 The VCF format",
    "text": "4.8 The VCF format\nOk, we‚Äôve finally gotten our VCF files. Let‚Äôs go over what a VCF file actually is. VCF (or Variant Call Format) is the dominant file format used to store variant calls generated from high-throughput sequencing data. VCF format is a bit of a bear because it packs in so much information. After we cover it here, we‚Äôll look at tools for extracting useful information from it.\nAt this point, the format will probably seem a little familiar, as it is a tabular file that begins with a (sometimes) extensive header. It even has one field (INFO) that serves as a garbage bin of semi-colon separated tags (just like the attributes field in GTF/GFF!).\nYou can find a formal specification here. But we‚Äôll also cover the basics in this section.\nWe‚Äôre going to use bcftools to extract bits of the VCF to view them. We saw above how we can use it for variant calling, but it‚Äôs got tons of other functionality (much like samtools!) so it‚Äôs worth skimming through the documentation to see what it can do beyond what you see here.\nWe‚Äôre going to look at the freebayes VCF because it produces the most annotations for each variant.\n\n4.8.1 The header\nVCF begins with a header. bcftools view prints a compressed VCF file to stdout, and -h prints only the header.\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\n##fileformat=VCFv4.2\n##FILTER=&lt;ID=PASS,Description=\"All filters passed\"&gt;\n##fileDate=20250205\n##source=freeBayes v1.3.4\n##reference=../../genome/GRCh38_GIABv3_no_alt_analysis_set_maskedGRC_decoys_MAP2K3_KMT2C_KCNJ18.fasta\n##contig=&lt;ID=chr1,length=248956422&gt;\n##contig=&lt;ID=chr2,length=242193529&gt;\n##contig=&lt;ID=chr3,length=198295559&gt;\n##contig=&lt;ID=chr4,length=190214555&gt;\n##contig=&lt;ID=chr5,length=181538259&gt;\n\n\nThe biggest part of the header is a sequence dictionary, listing all reference sequences and their lengths.\nLet‚Äôs look at the last few lines of the header:\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | tail -n 20 \n\n##INFO=&lt;ID=LEN,Number=A,Type=Integer,Description=\"allele length\"&gt;\n##INFO=&lt;ID=MQM,Number=A,Type=Float,Description=\"Mean mapping quality of observed alternate alleles\"&gt;\n##INFO=&lt;ID=MQMR,Number=1,Type=Float,Description=\"Mean mapping quality of observed reference alleles\"&gt;\n##INFO=&lt;ID=PAIRED,Number=A,Type=Float,Description=\"Proportion of observed alternate alleles which are supported by properly paired read fragments\"&gt;\n##INFO=&lt;ID=PAIREDR,Number=1,Type=Float,Description=\"Proportion of observed reference alleles which are supported by properly paired read fragments\"&gt;\n##INFO=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum depth in gVCF output block.\"&gt;\n##INFO=&lt;ID=END,Number=1,Type=Integer,Description=\"Last position (inclusive) in gVCF output record.\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=GQ,Number=1,Type=Float,Description=\"Genotype Quality, the Phred-scaled marginal (or unconditional) probability of the called genotype\"&gt;\n##FORMAT=&lt;ID=GL,Number=G,Type=Float,Description=\"Genotype Likelihood, log10-scaled likelihoods of the data given the called genotype for each possible genotype generated from the reference and alternate alleles given the sample ploidy\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n##FORMAT=&lt;ID=AD,Number=R,Type=Integer,Description=\"Number of observation for each allele\"&gt;\n##FORMAT=&lt;ID=RO,Number=1,Type=Integer,Description=\"Reference allele observation count\"&gt;\n##FORMAT=&lt;ID=QR,Number=1,Type=Integer,Description=\"Sum of quality of the reference observations\"&gt;\n##FORMAT=&lt;ID=AO,Number=A,Type=Integer,Description=\"Alternate allele observation count\"&gt;\n##FORMAT=&lt;ID=QA,Number=A,Type=Integer,Description=\"Sum of quality of the alternate observations\"&gt;\n##FORMAT=&lt;ID=MIN_DP,Number=1,Type=Integer,Description=\"Minimum depth in gVCF output block.\"&gt;\n##bcftools_viewVersion=1.19+htslib-1.19.1\n##bcftools_viewCommand=view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz; Date=Fri Feb  7 11:31:22 2025\n#CHROM  POS ID  REF ALT QUAL    FILTER  INFO    FORMAT  son dad mom\n\n\nThey record how we produced the VCF we‚Äôre viewing now (unfortunately freebayes did not add its command line call to the header).\nNote lines beginning: INFO=&lt;ID=... and FORMAT=&lt;ID=.... These give the definitions of tags found in the INFO and FORMAT fields of the tabular data. If you want to find out what the tag DP means, for example, you can do:\n\nbcftools view -h variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | grep \"=DP,\"\n\n##INFO=&lt;ID=DP,Number=1,Type=Integer,Description=\"Total read depth at the locus\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n\n\nThe final line is the header line for the tabular data that follows. The fields are:\n\nCHROM: The reference sequence of the variant record, e.g.¬†chr20 or possibly an NCBI accession.\nPOS: The left most position of the variant. In case of an insertion, will be the base to the left of an insertion. 1-indexed (not zero).\nID: Any database identifiers that have been attached to the variant (such as dbSNP IDs. see next chapter). Empty by default.\nREF: The reference allele. Cannot be empty, as in the case of an insertion. Will be th base to the left.\nALT: One or more alternate alleles. Comma-separated. Cannot be empty. In case of a deletion will be the base to the left of the deletion.\nQUAL: Phred-scaled variant quality determined by variant caller. You should know what these are by now!\nFILTER: Can be populated with values like PASS or FAIL or LowQual to indicate variants that pass some filtering procedure. Usually empty by default.\nINFO: A semicolon separated list of annotations of the variant. Almost always contains basic information like the total depth at the locus, the counts of each observed allele and some other useful stuff. We‚Äôll see how to extract it into an easier format later.\nFORMAT: Gives the format of the following fields, which contain genotypes.\ngenotypes for sample X: Every field from 10 onward contains genotype information for a sample along with (typically) some annotations, as defined in the FORMAT field.\n\nEach column from 10 onwards represents one sample‚Äôs vector of genotypes.\n\n\n4.8.2 The tabular data\nNow let‚Äôs look at the tabular data:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head\n\nchr20   33100435    .   T   C   7253.87 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=218;CIGAR=1X;DP=218;DPB=218;DPRA=0;EPP=3.6478;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=91.0153;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8104;QR=0;RO=0;RPL=116;RPP=4.96263;RPPR=0;RPR=102;RUN=1;SAF=119;SAP=6.99465;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0  1/1:80:0,80:0:0:80:2938:-264.561,-24.0824,0 1/1:65:0,65:0:0:65:2454:-221.045,-19.567,0\nchr20   33100454    .   C   T   7306.07 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=219;CIGAR=1X;DP=219;DPB=219;DPRA=0;EPP=3.49615;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=87.0387;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8160;QR=0;RO=0;RPL=108;RPP=3.09954;RPPR=0;RPR=111;RUN=1;SAF=120;SAP=7.38299;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 1/1:76:0,76:0:0:76:2851:-256.745,-22.8783,0 1/1:84:0,84:0:0:84:3111:-280.139,-25.2865,0 1/1:59:0,59:0:0:59:2198:-198.023,-17.7608,0\nchr20   33100524    .   C   T   7087.29 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=222;CIGAR=1X;DP=222;DPB=222;DPRA=0;EPP=3.04943;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=84.1199;PAIRED=0.995495;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7917;QR=0;RO=0;RPL=102;RPP=6.17948;RPPR=0;RPR=120;RUN=1;SAF=118;SAP=4.92746;SAR=104;SRF=0;SRP=0;SRR=0;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 1/1:80:0,80:0:0:80:3005:-270.607,-24.0824,0 1/1:88:0,88:0:0:88:3005:-270.594,-26.4906,0 1/1:54:0,54:0:0:54:1907:-171.854,-16.2556,0\nchr20   33100922    .   TCCAT   TCAT    6892.35 .   AB=0;ABP=0;AC=6;AF=1;AN=6;AO=225;CIGAR=1M1D3M;DP=226;DPB=180.8;DPRA=0;EPP=6.49431;EPPR=0;GTI=0;LEN=1;MEANALT=1.33333;MQM=59.7733;MQMR=0;NS=3;NUMALT=1;ODDS=89.5189;PAIRED=0.986667;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7707;QR=0;RO=0;RPL=108;RPP=3.79203;RPPR=0;RPR=117;RUN=1;SAF=124;SAP=8.11567;SAR=101;SRF=0;SRP=0;SRR=0;TYPE=del  GT:DP:AD:RO:QR:AO:QA:GL 1/1:96:0,95:0:0:95:3234:-291.03,-28.5979,0  1/1:61:0,61:0:0:61:2117:-190.736,-18.3628,0 1/1:69:0,69:0:0:69:2356:-212.226,-20.7711,0\nchr20   33101062    .   G   C   6054.04 .   AB=0.515152;ABP=3.1419;AC=5;AF=0.833333;AN=6;AO=195;CIGAR=1X;DP=228;DPB=228;DPRA=0;EPP=12.3755;EPPR=8.34028;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=69.7788;PAIRED=1;PAIREDR=0.969697;PAO=0;PQA=0;PQR=0;PRO=0;QA=7079;QR=1205;RO=33;RPL=115;RPP=16.6516;RPPR=8.34028;RPR=80;RUN=1;SAF=94;SAP=3.55595;SAR=101;SRF=21;SRP=8.34028;SRR=12;TYPE=snp GT:DP:AD:RO:QR:AO:QA:GL 1/1:87:0,87:0:0:87:3248:-292.458,-26.1896,0 0/1:66:32,34:32:1184:34:1175:-86.148,0,-86.9768 1/1:75:1,74:1:21:74:2656:-237.119,-20.4773,0\nchr20   33101256    .   C   T   1054.39 .   AB=0.58209;ABP=6.93191;AC=1;AF=0.166667;AN=6;AO=39;CIGAR=1X;DP=220;DPB=220;DPRA=0.875817;EPP=5.73856;EPPR=5.37479;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=63.486;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1470;QR=6671;RO=180;RPL=21;RPP=3.51141;RPPR=4.21667;RPR=18;RUN=1;SAF=21;SAP=3.51141;SAR=18;SRF=88;SRP=3.20332;SRR=92;TYPE=snp    GT:DP:AD:RO:QR:AO:QA:GL 0/0:84:83,0:83:3020:0:0:0,-24.9855,-271.782 0/0:69:69,0:69:2583:0:0:0,-20.7711,-232.64  0/1:67:28,39:28:1068:39:1470:-112.389,0,-76.2508\nchr20   33101562    .   CTTTTCTTTTGA    CTTCTTTTGA  529.868 .   AB=0.40678;ABP=7.46366;AC=1;AF=0.166667;AN=6;AO=24;CIGAR=1M2D9M;DP=225;DPB=221;DPRA=0.710843;EPP=3.37221;EPPR=4.11819;GTI=0;LEN=2;MEANALT=2;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=81.6124;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=875;QR=7339;RO=196;RPL=10;RPP=4.45795;RPPR=26.4533;RPR=14;RUN=1;SAF=17;SAP=12.0581;SAR=7;SRF=128;SRP=42.8945;SRR=68;TYPE=del    GT:DP:AD:RO:QR:AO:QA:GL 0/0:76:75,0:75:2831:0:0:0,-22.5772,-254.814 0/1:59:34,24:34:1257:24:875:-61.466,0,-95.8355  0/0:90:87,0:87:3251:0:0:0,-26.1896,-292.616\nchr20   33101824    .   G   A   725.819 .   AB=0.461538;ABP=3.84548;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=219;DPB=219;DPRA=0.844156;EPP=26.4622;EPPR=3.42611;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=67.3506;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1156;QR=7082;RO=188;RPL=20;RPP=10.2485;RPPR=3.0103;RPR=10;RUN=1;SAF=6;SAP=26.4622;SAR=24;SRF=69;SRP=31.8863;SRR=119;TYPE=snp   GT:DP:AD:RO:QR:AO:QA:GL 0/0:91:90,0:90:3378:0:0:0,-27.0927,-304.011 0/0:63:63,0:63:2374:0:0:0,-18.9649,-213.856 0/1:65:35,30:35:1330:30:1156:-84.7577,0,-100.405\nchr20   33102044    .   T   G   0   .   AB=0;ABP=0;AC=0;AF=0;AN=6;AO=4;CIGAR=1X;DP=193;DPB=193;DPRA=0.919118;EPP=11.6962;EPPR=4.97275;GTI=0;LEN=1;MEANALT=1.5;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=61.3932;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=72;QR=6826;RO=187;RPL=0;RPP=11.6962;RPPR=7.20229;RPR=4;RUN=1;SAF=0;SAP=11.6962;SAR=4;SRF=101;SRP=5.62303;SRR=86;TYPE=snp  GT:DP:AD:RO:QR:AO:QA:GL 0/0:72:71,1:71:2576:1:13:0,-20.3742,-230.715    0/0:53:48,3:48:1794:3:59:0,-10.1644,-156.119    0/0:68:68,0:68:2456:0:0:0,-20.47,-221.219\nchr20   33102134    .   T   C,G 7.50367e-15 .   AB=0,0;ABP=0,0;AC=0,0;AF=0,0;AN=6;AO=2,3;CIGAR=1X,1X;DP=171;DPB=171;DPRA=0.571429,0.655405;EPP=7.35324,9.52472;EPPR=4.31842;GTI=0;LEN=1,1;MEANALT=2,1.5;MQM=60,60;MQMR=60;NS=3;NUMALT=2;ODDS=46.6774;PAIRED=1,1;PAIREDR=1;PAO=0,0;PQA=0,0;PQR=0;PRO=0;QA=55,37;QR=5947;RO=166;RPL=1,0;RPP=3.0103,9.52472;RPPR=4.31842;RPR=1,3;RUN=1,1;SAF=1,0;SAP=3.0103,9.52472;SAR=1,3;SRF=86;SRP=3.48122;SRR=80;TYPE=snp,snp GT:DP:AD:RO:QR:AO:QA:GL 0/0:74:74,0,0:74:2603:0,0:0,0:0,-22.2762,-234.443,-22.2762,-234.443,-234.443    0/0:38:34,2,2:34:1226:2,2:55,25:0,-5.81527,-105.463,-8.66201,-104.431,-108.169  0/0:59:58,0,1:58:2118:0,1:0,12:0,-17.4597,-190.705,-16.5608,-189.93,-189.629\n\n\nIn this case we specify -H to suppress the header and -r chr20:33100000-33200000 to identify a particular region.\nWe can see there‚Äôs a lot of information there. Let‚Äôs take it a bit at a time.\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 1-7\n\nchr20   33100435    .   T   C   7253.87 .\nchr20   33100454    .   C   T   7306.07 .\nchr20   33100524    .   C   T   7087.29 .\nchr20   33100922    .   TCCAT   TCAT    6892.35 .\nchr20   33101062    .   G   C   6054.04 .\nchr20   33101256    .   C   T   1054.39 .\nchr20   33101562    .   CTTTTCTTTTGA    CTTCTTTTGA  529.868 .\nchr20   33101824    .   G   A   725.819 .\nchr20   33102044    .   T   G   0   .\nchr20   33102134    .   T   C,G 7.50367e-15 .\n\n\nMuch cleaner! We can see a diversity of variant calls here. We have 7 biallelic SNPs, one multi-allelic SNP and two longer variants. The last two variants have very bad quality scores (0 and 7.50367e-15) respectively. Those are an example of freebayes opting for extreme high sensitivity, and we will definitely filter those out later. The other variant calls have rather extreme high quality scores. Improbably high, really. The probability of error for QUAL=7253.87 is 10-725. Take a moment to ask yourself whether we should ever believe any statistical output with that degree of confidence.\nAnyway‚Ä¶ these high quality variants are most likely real. The SNPs are straightforward. The haplotypes represent deletions, but they don‚Äôt seem to be represented in a very parsimonious way. The first one might better have the alleles TC and T indicating a single-base deletion. We‚Äôll see how we can standardize representation later, but we can in fact see that‚Äôs exactly how GATK does it:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/gatk/gatk.vcf.gz | head | cut -f 1-7\n\nchr20   33100435    .   T   C   8075.9  .\nchr20   33100454    .   C   T   8110.9  .\nchr20   33100524    .   C   T   7343.9  .\nchr20   33100922    .   TC  T   8409.86 .\nchr20   33101062    .   G   C   6336.13 .\nchr20   33101256    .   C   T   1110.13 .\nchr20   33101562    .   CTT C   935.09  .\nchr20   33101824    .   G   A   997.13  .\nchr20   33102548    .   G   A   4207.13 .\nchr20   33103215    .   G   A   856.13  .\n\n\nNow let‚Äôs look at the INFO field:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 8\n\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=218;CIGAR=1X;DP=218;DPB=218;DPRA=0;EPP=3.6478;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=91.0153;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8104;QR=0;RO=0;RPL=116;RPP=4.96263;RPPR=0;RPR=102;RUN=1;SAF=119;SAP=6.99465;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=219;CIGAR=1X;DP=219;DPB=219;DPRA=0;EPP=3.49615;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=87.0387;PAIRED=1;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=8160;QR=0;RO=0;RPL=108;RPP=3.09954;RPPR=0;RPR=111;RUN=1;SAF=120;SAP=7.38299;SAR=99;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=222;CIGAR=1X;DP=222;DPB=222;DPRA=0;EPP=3.04943;EPPR=0;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=0;NS=3;NUMALT=1;ODDS=84.1199;PAIRED=0.995495;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7917;QR=0;RO=0;RPL=102;RPP=6.17948;RPPR=0;RPR=120;RUN=1;SAF=118;SAP=4.92746;SAR=104;SRF=0;SRP=0;SRR=0;TYPE=snp\nAB=0;ABP=0;AC=6;AF=1;AN=6;AO=225;CIGAR=1M1D3M;DP=226;DPB=180.8;DPRA=0;EPP=6.49431;EPPR=0;GTI=0;LEN=1;MEANALT=1.33333;MQM=59.7733;MQMR=0;NS=3;NUMALT=1;ODDS=89.5189;PAIRED=0.986667;PAIREDR=0;PAO=0;PQA=0;PQR=0;PRO=0;QA=7707;QR=0;RO=0;RPL=108;RPP=3.79203;RPPR=0;RPR=117;RUN=1;SAF=124;SAP=8.11567;SAR=101;SRF=0;SRP=0;SRR=0;TYPE=del\nAB=0.515152;ABP=3.1419;AC=5;AF=0.833333;AN=6;AO=195;CIGAR=1X;DP=228;DPB=228;DPRA=0;EPP=12.3755;EPPR=8.34028;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=69.7788;PAIRED=1;PAIREDR=0.969697;PAO=0;PQA=0;PQR=0;PRO=0;QA=7079;QR=1205;RO=33;RPL=115;RPP=16.6516;RPPR=8.34028;RPR=80;RUN=1;SAF=94;SAP=3.55595;SAR=101;SRF=21;SRP=8.34028;SRR=12;TYPE=snp\nAB=0.58209;ABP=6.93191;AC=1;AF=0.166667;AN=6;AO=39;CIGAR=1X;DP=220;DPB=220;DPRA=0.875817;EPP=5.73856;EPPR=5.37479;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=63.486;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1470;QR=6671;RO=180;RPL=21;RPP=3.51141;RPPR=4.21667;RPR=18;RUN=1;SAF=21;SAP=3.51141;SAR=18;SRF=88;SRP=3.20332;SRR=92;TYPE=snp\nAB=0.40678;ABP=7.46366;AC=1;AF=0.166667;AN=6;AO=24;CIGAR=1M2D9M;DP=225;DPB=221;DPRA=0.710843;EPP=3.37221;EPPR=4.11819;GTI=0;LEN=2;MEANALT=2;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=81.6124;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=875;QR=7339;RO=196;RPL=10;RPP=4.45795;RPPR=26.4533;RPR=14;RUN=1;SAF=17;SAP=12.0581;SAR=7;SRF=128;SRP=42.8945;SRR=68;TYPE=del\nAB=0.461538;ABP=3.84548;AC=1;AF=0.166667;AN=6;AO=30;CIGAR=1X;DP=219;DPB=219;DPRA=0.844156;EPP=26.4622;EPPR=3.42611;GTI=0;LEN=1;MEANALT=1;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=67.3506;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=1156;QR=7082;RO=188;RPL=20;RPP=10.2485;RPPR=3.0103;RPR=10;RUN=1;SAF=6;SAP=26.4622;SAR=24;SRF=69;SRP=31.8863;SRR=119;TYPE=snp\nAB=0;ABP=0;AC=0;AF=0;AN=6;AO=4;CIGAR=1X;DP=193;DPB=193;DPRA=0.919118;EPP=11.6962;EPPR=4.97275;GTI=0;LEN=1;MEANALT=1.5;MQM=60;MQMR=60;NS=3;NUMALT=1;ODDS=61.3932;PAIRED=1;PAIREDR=1;PAO=0;PQA=0;PQR=0;PRO=0;QA=72;QR=6826;RO=187;RPL=0;RPP=11.6962;RPPR=7.20229;RPR=4;RUN=1;SAF=0;SAP=11.6962;SAR=4;SRF=101;SRP=5.62303;SRR=86;TYPE=snp\nAB=0,0;ABP=0,0;AC=0,0;AF=0,0;AN=6;AO=2,3;CIGAR=1X,1X;DP=171;DPB=171;DPRA=0.571429,0.655405;EPP=7.35324,9.52472;EPPR=4.31842;GTI=0;LEN=1,1;MEANALT=2,1.5;MQM=60,60;MQMR=60;NS=3;NUMALT=2;ODDS=46.6774;PAIRED=1,1;PAIREDR=1;PAO=0,0;PQA=0,0;PQR=0;PRO=0;QA=55,37;QR=5947;RO=166;RPL=1,0;RPP=3.0103,9.52472;RPPR=4.31842;RPR=1,3;RUN=1,1;SAF=1,0;SAP=3.0103,9.52472;SAR=1,3;SRF=86;SRP=3.48122;SRR=80;TYPE=snp,snp\n\n\nThere‚Äôs a ton of INFO. Many of these are statistics you could use to filter on. The tags are all defined in the header, as we mentioned above. A couple we look at a lot are DP, giving the total depth, AF giving the alternate allele frequency, and AO, giving the number of reads supporting the alternate allele.\nNow let‚Äôs look at the format and genotype fields:\n\nbcftools view -H -r chr20:33100000-33200000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | head | cut -f 9-\n\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0  1/1:80:0,80:0:0:80:2938:-264.561,-24.0824,0 1/1:65:0,65:0:0:65:2454:-221.045,-19.567,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:76:0,76:0:0:76:2851:-256.745,-22.8783,0 1/1:84:0,84:0:0:84:3111:-280.139,-25.2865,0 1/1:59:0,59:0:0:59:2198:-198.023,-17.7608,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:80:0,80:0:0:80:3005:-270.607,-24.0824,0 1/1:88:0,88:0:0:88:3005:-270.594,-26.4906,0 1/1:54:0,54:0:0:54:1907:-171.854,-16.2556,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:96:0,95:0:0:95:3234:-291.03,-28.5979,0  1/1:61:0,61:0:0:61:2117:-190.736,-18.3628,0 1/1:69:0,69:0:0:69:2356:-212.226,-20.7711,0\nGT:DP:AD:RO:QR:AO:QA:GL 1/1:87:0,87:0:0:87:3248:-292.458,-26.1896,0 0/1:66:32,34:32:1184:34:1175:-86.148,0,-86.9768 1/1:75:1,74:1:21:74:2656:-237.119,-20.4773,0\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:84:83,0:83:3020:0:0:0,-24.9855,-271.782 0/0:69:69,0:69:2583:0:0:0,-20.7711,-232.64  0/1:67:28,39:28:1068:39:1470:-112.389,0,-76.2508\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:76:75,0:75:2831:0:0:0,-22.5772,-254.814 0/1:59:34,24:34:1257:24:875:-61.466,0,-95.8355  0/0:90:87,0:87:3251:0:0:0,-26.1896,-292.616\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:91:90,0:90:3378:0:0:0,-27.0927,-304.011 0/0:63:63,0:63:2374:0:0:0,-18.9649,-213.856 0/1:65:35,30:35:1330:30:1156:-84.7577,0,-100.405\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:72:71,1:71:2576:1:13:0,-20.3742,-230.715    0/0:53:48,3:48:1794:3:59:0,-10.1644,-156.119    0/0:68:68,0:68:2456:0:0:0,-20.47,-221.219\nGT:DP:AD:RO:QR:AO:QA:GL 0/0:74:74,0,0:74:2603:0,0:0,0:0,-22.2762,-234.443,-22.2762,-234.443,-234.443    0/0:38:34,2,2:34:1226:2,2:55,25:0,-5.81527,-105.463,-8.66201,-104.431,-108.169  0/0:59:58,0,1:58:2118:0,1:0,12:0,-17.4597,-190.705,-16.5608,-189.93,-189.629\n\n\nThe FORMAT field is a colon separated list of tags indicating what each genotype contains. These are also defined in the header.\nConsider this format field GT:DP:AD:RO:QR:AO:QA:GL and this genotype field 1/1:73:0,73:0:0:73:2712:-244.24,-21.9752,0. It works out like this:\n\n\n\nGT\nDP\nAD\nRO\nQR\nAO\nQA\nGL\n\n\n\n\n1/1\n73\n0,73\n0\n0\n73\n2712\n-244.24,-21.9752,0\n\n\n\nGT is the big one! Alleles are numbered from 0 as REF, ALT1, ALT2‚Ä¶ALTN. A genotype 0/1 indicates a REF/ALT heterozygote. The allele separator / indicates an unphased genotype. If the separator were instead | (0|1) that would indicate that all consecutive genotypes from that sample also using the separator were phase-known. For example, at position 1 you have 0|1, position 10 you have 1|0 and position 20 you have 0|3. You know that the two haplotypes spanning positions 1-20 are 010 and 103.\nUnphased genotypes have alleles sorted numerically, so a REF/ALT heterozygote will always be 0/1 and not 1/0. Obviously the ordering matters for phased genotypes.\nMissing genotypes are encoded differently by different programs. They may simply be . or they may be ./.. For some programs missing genotypes will also have the rest of the annotations, but also missing, e.g ./.:.:.:.:.:.:.:..\nDP gives the total depth for the sample. AD is the depth for each possible allele in the genotype. We don‚Äôt often scrutinize these except in cases where we are interested in particular variants and their genotypes, or we are curious about some behavior of the variant caller that doesn‚Äôt match our expectations for the data. RO and AO give counts of REF and ALT allele observations, while QR and QAgive the sums of the phred base qualities for REF and ALT alleles.\nFor most callers there will be some version of a comma-separated genotype likelihood vector, in this case GL, which is the log10 scaled likelihood (i.e.¬†the probability of the read data) for each possible genotype given the alleles. The values are scaled so that they are relative to the highest likelihood genotype (which makes that value 0).\n\n\n4.8.3 How do we dig in to a VCF file?\nWe‚Äôll look at a few very basic things with linux/bash here, but we‚Äôll save the bulk for the next chapter. These tricks are useful when you want a quick glance at what‚Äôs going on in a VCF file. You should only really use them on smallish regions as exploratory measures when you get results back.\nLet‚Äôs start by asking how many variants we‚Äôve got we‚Äôve got in our target region.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz | wc -l\n\n   57423\n\n\nWhat do the QUAL scores look like? We‚Äôll try four categories: &lt; 10, &gt;= 10 & &lt; 30, &gt;= 30 & &lt; 100, &gt;= 100.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nawk '\n  {if($6 &lt; 10){w+=1}}\n  {if($6 &gt;= 10 && $6 &lt; 30){x+=1}}\n  {if($6 &gt;= 30 && $6 &lt; 100){y+=1}}\n  {if($6 &gt;= 100){z+=1}}\n  END {print w,x,y,z}'\n\n39872 408 883 16260\n\n\nThe vast majority are pretty very low quality. At least in this dataset, setting a QUAL threshold anywhere between 10 and 100 won‚Äôt have a big impact on the number of variants retained. GATK and bcftools won‚Äôt output so many garbage variants. Extremely high sensitivity is just how freebayes rolls.\nWe can also quickly pull out INFO field tags if we want. Let‚Äôs grab the overall depth tag DP and summarize it along the lines of our coverage thresholds we considered previously.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nggrep -oP \"(?&lt;=DP=)[0-9]+\" |\nawk '\n  {if($1 &lt; 90){w+=1}}\n  {if($1 &gt;= 90 && $1 &lt; 260){x+=1}}\n  {if($1 &gt;= 260 && $1 &lt; 1000){y+=1}}\n  {if($1 &gt;= 1000){z+=1}}\n  END {print w,x,y,z}'\n\n9748 38025 7154 2496\n\n\nNote the ggrep is because this is being compiled on a mac, which natively has BSD versions of grep with slightly different options ggrep is the GNU version of grep that is installed as plain old grep on linux systems.\nThe regex \"(?&lt;=DP=)[0-9]+\" contains a zero-length assertion, it pulls out strings preceded by DP=.\nThere isn‚Äôt really strong concordance between the QUAL numbers and the DP numbers. Not all bad variant records fall outside the depth thresholds.\nLet‚Äôs pull out one more tag that freebayes produces, the TYPE tag.\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\nggrep -oP \"(?&lt;=TYPE=)[a-zA-Z]+\" |\nsort | uniq -c | sort -g\n\n 318 mnp\n1144 ins\n1416 del\n3075 complex\n51470 snp\n\n\nWe can see the vast majority of variant records are categorized as SNPs, though we also have insertions, deletions, complex variants, and multi-nucleotide polymorphisms (typically short haployptes of just SNPs).\nNow let‚Äôs look at the distribution of genotypes in just one sample:\n\nbcftools view -H -r chr20:29400000-34400000 variants/results/05_variantCalling/freebayes/freebayes.vcf.gz |\ncut -f 10 |\nsed 's/:.*//' |\nsort | uniq -c | sort -g\n\n   1 0/5\n   1 0/7\n   1 4/6\n   3 0/4\n   3 1/4\n   3 2/3\n   8 2/2\n   9 1/3\n  21 0/3\n 141 .\n 158 0/2\n 427 1/2\n2781 1/1\n13722 0/1\n40144 0/0\n\n\nAs we might expect, most genotypes are 0/0 in this sample, with the next most common being 0/1. We can see there are a handful of loci that have many alleles. These are probably mostly false positives. After all, with only 3 diploids, it‚Äôs impossible to have more than 6 alleles at a site, and if sites with such allelic diversity existed, it would be next to impossible to sample 6 different alleles with only three individuals anyway.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#conclusions",
    "href": "02_2_variantCalling.html#conclusions",
    "title": "4¬† Variant Calling",
    "section": "4.9 Conclusions",
    "text": "4.9 Conclusions\nWe‚Äôve now seen several pieces of software for calling variants from mapped data. We‚Äôve covered the VCF format and how to dig into a little bit. In the next chapter we will look at tools for manipulating and extracting information from VCF files, summarizing them, filtering them and reformatting.",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "02_2_variantCalling.html#footnotes",
    "href": "02_2_variantCalling.html#footnotes",
    "title": "4¬† Variant Calling",
    "section": "",
    "text": "For preprocessing, they recommend that fastq files be converted to unmapped bam so that sequences can be annotated (e.g.¬†with adapter contamination), then converted back to fastq, then aligned, then finally for the aligned sequences to be merged with the unmapped bam to reintroduce the annotations made on the unmapped sequence. Whew.‚Ü©Ô∏é",
    "crumbs": [
      "Variant Detection and Genotyping",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Variant Calling</span>"
    ]
  },
  {
    "objectID": "01_0_IntroToGit.html",
    "href": "01_0_IntroToGit.html",
    "title": "Introduction to Git",
    "section": "",
    "text": "Learning Objectives\nIn this module we will learn the basics of the version control software git, and how to set up a remote code repository on the web service GitHub.\nThese chapters will be somewhat sparse, as you will be referred to existing training material in HuskyCT.\nMany (perhaps most?) people find Git to be a bit confusing at times. You should prepare for that possibility, and know that you may need to refer to documentation or seek help.",
    "crumbs": [
      "Introduction to Git"
    ]
  },
  {
    "objectID": "01_0_IntroToGit.html#learning-objectives",
    "href": "01_0_IntroToGit.html#learning-objectives",
    "title": "Introduction to Git",
    "section": "",
    "text": "Learning Objectives:\n\n\nCreate a code repository with git.\n\n\nSet up a documented remote copy of the repository on github.\n\n\nBranch the repository to edit, and then merge changes.\n\n\nWork collaboratively.",
    "crumbs": [
      "Introduction to Git"
    ]
  },
  {
    "objectID": "02_0_variantDetection.html",
    "href": "02_0_variantDetection.html",
    "title": "Variant Detection and Genotyping",
    "section": "",
    "text": "Learning Objectives\nIn this module we will work through a basic variant detection and genotyping workflow. We‚Äôll cover quality control at various stages of the analysis, and use a few different variant callers so that we can compare results. We‚Äôll also cover basic variant effect annotation. The basic scripts we‚Äôre going to work through can be found at this github repository.",
    "crumbs": [
      "Variant Detection and Genotyping"
    ]
  },
  {
    "objectID": "02_0_variantDetection.html#learning-objectives",
    "href": "02_0_variantDetection.html#learning-objectives",
    "title": "Variant Detection and Genotyping",
    "section": "",
    "text": "Learning Objectives:\n\n\nConduct quality control analysis of raw and mapped data specific to variant detection.\n\n\nIdentify variants and genotype samples against a reference genome.\n\n\nManipulate the VCF format\n\n\nApply strategies for filtering problematic variants.\n\n\nAssess variant call-set quality.\n\n\nConduct basic variant effect annotation.",
    "crumbs": [
      "Variant Detection and Genotyping"
    ]
  },
  {
    "objectID": "03_0_genomeAssembly.html",
    "href": "03_0_genomeAssembly.html",
    "title": "Genome Assembly",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\n\n\nLearning Objectives:\n\n\nFIRST OBJECTIVE",
    "crumbs": [
      "Genome Assembly"
    ]
  },
  {
    "objectID": "04_0_scRNAseq.html",
    "href": "04_0_scRNAseq.html",
    "title": "Single-cell RNAseq",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\n\n\nLearning Objectives:\n\n\nFIRST OBJECTIVE",
    "crumbs": [
      "Single-cell RNAseq"
    ]
  },
  {
    "objectID": "05_0_workflows.html",
    "href": "05_0_workflows.html",
    "title": "Pipeline development with Nextflow",
    "section": "",
    "text": "Learning Objectives\n\n\n\n\n\n\nLearning Objectives:\n\n\nFIRST OBJECTIVE",
    "crumbs": [
      "Pipeline development with Nextflow"
    ]
  }
]